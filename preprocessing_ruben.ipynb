{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the data\n",
    "### setting comprehensible col names and right types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting arch\n",
      "  Downloading arch-7.2.0-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.22.3 in c:\\users\\nicki\\anaconda3\\lib\\site-packages (from arch) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.8 in c:\\users\\nicki\\anaconda3\\lib\\site-packages (from arch) (1.11.4)\n",
      "Requirement already satisfied: pandas>=1.4 in c:\\users\\nicki\\anaconda3\\lib\\site-packages (from arch) (2.1.4)\n",
      "Requirement already satisfied: statsmodels>=0.12 in c:\\users\\nicki\\anaconda3\\lib\\site-packages (from arch) (0.14.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nicki\\anaconda3\\lib\\site-packages (from pandas>=1.4->arch) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nicki\\anaconda3\\lib\\site-packages (from pandas>=1.4->arch) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\nicki\\anaconda3\\lib\\site-packages (from pandas>=1.4->arch) (2023.3)\n",
      "Requirement already satisfied: patsy>=0.5.2 in c:\\users\\nicki\\anaconda3\\lib\\site-packages (from statsmodels>=0.12->arch) (0.5.3)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\nicki\\anaconda3\\lib\\site-packages (from statsmodels>=0.12->arch) (23.1)\n",
      "Requirement already satisfied: six in c:\\users\\nicki\\anaconda3\\lib\\site-packages (from patsy>=0.5.2->statsmodels>=0.12->arch) (1.16.0)\n",
      "Downloading arch-7.2.0-cp311-cp311-win_amd64.whl (927 kB)\n",
      "   ---------------------------------------- 0.0/927.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 10.2/927.1 kB ? eta -:--:--\n",
      "   ----------- ---------------------------- 276.5/927.1 kB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 927.1/927.1 kB 9.8 MB/s eta 0:00:00\n",
      "Installing collected packages: arch\n",
      "Successfully installed arch-7.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install arch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting statsforecast\n",
      "  Downloading statsforecast-2.0.0-cp311-cp311-win_amd64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\nicki\\anaconda3\\lib\\site-packages (from statsforecast) (2.2.1)\n",
      "Collecting coreforecast>=0.0.12 (from statsforecast)\n",
      "  Downloading coreforecast-0.0.15-cp311-cp311-win_amd64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: numba>=0.55.0 in c:\\users\\nicki\\anaconda3\\lib\\site-packages (from statsforecast) (0.59.0)\n",
      "Requirement already satisfied: numpy>=1.21.6 in c:\\users\\nicki\\anaconda3\\lib\\site-packages (from statsforecast) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.3.5 in c:\\users\\nicki\\anaconda3\\lib\\site-packages (from statsforecast) (2.1.4)\n",
      "Requirement already satisfied: scipy>=1.7.3 in c:\\users\\nicki\\anaconda3\\lib\\site-packages (from statsforecast) (1.11.4)\n",
      "Requirement already satisfied: statsmodels>=0.13.2 in c:\\users\\nicki\\anaconda3\\lib\\site-packages (from statsforecast) (0.14.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nicki\\anaconda3\\lib\\site-packages (from statsforecast) (4.65.0)\n",
      "Collecting fugue>=0.8.1 (from statsforecast)\n",
      "  Downloading fugue-0.9.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting utilsforecast>=0.1.4 (from statsforecast)\n",
      "  Downloading utilsforecast-0.2.10-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting threadpoolctl>=3 (from statsforecast)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting triad>=0.9.7 (from fugue>=0.8.1->statsforecast)\n",
      "  Downloading triad-0.9.8-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting adagio>=0.2.4 (from fugue>=0.8.1->statsforecast)\n",
      "  Downloading adagio-0.2.6-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in c:\\users\\nicki\\anaconda3\\lib\\site-packages (from numba>=0.55.0->statsforecast) (0.42.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nicki\\anaconda3\\lib\\site-packages (from pandas>=1.3.5->statsforecast) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nicki\\anaconda3\\lib\\site-packages (from pandas>=1.3.5->statsforecast) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\nicki\\anaconda3\\lib\\site-packages (from pandas>=1.3.5->statsforecast) (2023.3)\n",
      "Requirement already satisfied: patsy>=0.5.2 in c:\\users\\nicki\\anaconda3\\lib\\site-packages (from statsmodels>=0.13.2->statsforecast) (0.5.3)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\nicki\\anaconda3\\lib\\site-packages (from statsmodels>=0.13.2->statsforecast) (23.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\nicki\\anaconda3\\lib\\site-packages (from tqdm->statsforecast) (0.4.6)\n",
      "Requirement already satisfied: six in c:\\users\\nicki\\anaconda3\\lib\\site-packages (from patsy>=0.5.2->statsmodels>=0.13.2->statsforecast) (1.16.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.1 in c:\\users\\nicki\\anaconda3\\lib\\site-packages (from triad>=0.9.7->fugue>=0.8.1->statsforecast) (14.0.2)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in c:\\users\\nicki\\anaconda3\\lib\\site-packages (from triad>=0.9.7->fugue>=0.8.1->statsforecast) (2023.10.0)\n",
      "Collecting fs (from triad>=0.9.7->fugue>=0.8.1->statsforecast)\n",
      "  Downloading fs-2.4.16-py2.py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: appdirs~=1.4.3 in c:\\users\\nicki\\anaconda3\\lib\\site-packages (from fs->triad>=0.9.7->fugue>=0.8.1->statsforecast) (1.4.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\nicki\\anaconda3\\lib\\site-packages (from fs->triad>=0.9.7->fugue>=0.8.1->statsforecast) (68.2.2)\n",
      "Downloading statsforecast-2.0.0-cp311-cp311-win_amd64.whl (251 kB)\n",
      "   ---------------------------------------- 0.0/251.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 251.4/251.4 kB 7.5 MB/s eta 0:00:00\n",
      "Downloading coreforecast-0.0.15-cp311-cp311-win_amd64.whl (189 kB)\n",
      "   ---------------------------------------- 0.0/189.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 189.7/189.7 kB 11.2 MB/s eta 0:00:00\n",
      "Downloading fugue-0.9.1-py3-none-any.whl (278 kB)\n",
      "   ---------------------------------------- 0.0/278.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 278.2/278.2 kB 16.7 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading utilsforecast-0.2.10-py3-none-any.whl (41 kB)\n",
      "   ---------------------------------------- 0.0/41.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 41.7/41.7 kB ? eta 0:00:00\n",
      "Downloading adagio-0.2.6-py3-none-any.whl (19 kB)\n",
      "Downloading triad-0.9.8-py3-none-any.whl (62 kB)\n",
      "   ---------------------------------------- 0.0/62.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 62.3/62.3 kB ? eta 0:00:00\n",
      "Downloading fs-2.4.16-py2.py3-none-any.whl (135 kB)\n",
      "   ---------------------------------------- 0.0/135.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 135.3/135.3 kB 7.8 MB/s eta 0:00:00\n",
      "Installing collected packages: threadpoolctl, fs, coreforecast, utilsforecast, triad, adagio, fugue, statsforecast\n",
      "  Attempting uninstall: threadpoolctl\n",
      "    Found existing installation: threadpoolctl 2.2.0\n",
      "    Uninstalling threadpoolctl-2.2.0:\n",
      "      Successfully uninstalled threadpoolctl-2.2.0\n",
      "Successfully installed adagio-0.2.6 coreforecast-0.0.15 fs-2.4.16 fugue-0.9.1 statsforecast-2.0.0 threadpoolctl-3.5.0 triad-0.9.8 utilsforecast-0.2.10\n"
     ]
    }
   ],
   "source": [
    "!pip install statsforecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the necessary packages\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "from arch import arch_model\n",
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import AutoARIMA\n",
    "from statsmodels.stats.diagnostic import het_arch\n",
    "import pandas as pd\n",
    "from pandas.errors import PerformanceWarning\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from prophet import Prophet\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import pmdarima as pm\n",
    "from pmdarima.arima import ndiffs, nsdiffs\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "\n",
    "# For legibility, we mute some warnings\n",
    "import warnings\n",
    "\n",
    "# Ignore FutureWarning for deprecated 'T' frequency in Prophet\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"'T' is deprecated\")\n",
    "\n",
    "# Ignore PerformanceWarning from pandas\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mix of different sources, mostly ESO\n",
    "balancing_df = pd.read_csv(\"balancing_data.csv\")\n",
    "# Demand data only for GB\n",
    "GB_demand_df = pd.read_csv(\"demand_load_data.csv\")\n",
    "# Generation data only for GB\n",
    "GB_generation_df = pd.read_csv(\"generation_data.csv\")\n",
    "# the price dataframe only concerns EPEX (only prices from there)\n",
    "EPEX_price_df = pd.read_csv(\"price_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final columns:\n",
      "['GMT Time' 'System_Price' 'NIV_Outturn' 'BM_Bid_Acceptances'\n",
      " 'BM_Offer_Acceptances' 'BSAD_Turn_Up' 'BSAD_Turn_Down' 'BSAD_Total'\n",
      " 'EPEX_Intraday_Volume']\n"
     ]
    }
   ],
   "source": [
    "def rename_balancing_columns(df):\n",
    "    # Define a dictionary for concise renaming\n",
    "    rename_map = {\n",
    "        'GMT Time': 'GMT Time',\n",
    "        'System Price (ESO Outturn) - GB (£/MWh)': 'System_Price',\n",
    "        'NIV Outturn (+ve long) - GB (MW)': 'NIV_Outturn',\n",
    "        'BM Bid Acceptances (total) - GB (MW)': 'BM_Bid_Acceptances',\n",
    "        'BM Offer Acceptances (total) - GB (MW)': 'BM_Offer_Acceptances',\n",
    "        'Total BSAD Volume - Turn Up - GB (MW)': 'BSAD_Turn_Up',\n",
    "        'Total BSAD Volume - Turn Down - GB (MW)': 'BSAD_Turn_Down',\n",
    "        'Total BSAD Volume - Total - GB (MW)': 'BSAD_Total',\n",
    "        'Intraday Volume (EPEX Outturn, APX, MID) - GB (MWh)': 'EPEX_Intraday_Volume'\n",
    "    }\n",
    "    \n",
    "    # Apply the renaming map\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    # Force all the non datetime columns to numeric\n",
    "    for column in df.columns:\n",
    "        if column != 'GMT Time':  # Skip the 'GMT Time' column\n",
    "            df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "    return df\n",
    "\n",
    "# Apply the function to rename columns in balancing_df\n",
    "balancing_df = rename_balancing_columns(balancing_df)\n",
    "\n",
    "print(\"Final columns:\")\n",
    "print(balancing_df.columns.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final columns:\n",
      "['GMT Time' 'Loss_of_Load_Prob' 'Total_Load' 'Demand_Outturn']\n"
     ]
    }
   ],
   "source": [
    "def rename_demand_columns(df):\n",
    "    \"\"\"\n",
    "    Rename columns for easier reference and convert non-datetime columns to numeric.\n",
    "    \"\"\"\n",
    "    # Define a dictionary for concise renaming\n",
    "    rename_map = {\n",
    "        'GMT Time': 'GMT Time',\n",
    "        'Loss of Load Probability - Latest - GB ()': 'Loss_of_Load_Prob',\n",
    "        'Actual Total Load - GB (MW)': 'Total_Load',\n",
    "        'Demand Outturn (ITSDO) - GB (MW)': 'Demand_Outturn'\n",
    "    }\n",
    "    \n",
    "    # Apply the renaming map\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    # Force all the non-datetime columns to numeric\n",
    "    for column in df.columns:\n",
    "        if column != 'GMT Time':  # Skip the 'GMT Time' column\n",
    "            df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "    return df\n",
    "\n",
    "# Apply the renaming and filling functions\n",
    "GB_demand_df = rename_demand_columns(GB_demand_df)\n",
    "\n",
    "\n",
    "print(\"Final columns:\")\n",
    "print(GB_demand_df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final columns:\n",
      "['GMT Time' 'Biomass' 'Fossil_Gas' 'Fossil_Hard_Coal' 'Fossil_Oil'\n",
      " 'Hydro_Pumped_Storage' 'Hydro_Run-of-River_and_Poundage' 'Nuclear'\n",
      " 'Solar' 'Wind_Onshore' 'Wind_Offshore']\n"
     ]
    }
   ],
   "source": [
    "def rename_columns_generation(df):\n",
    "    # Define a function to clean each column name\n",
    "    def clean_column_name(col):\n",
    "        # Extract the generation type using regex\n",
    "        match = re.search(r'Actual Aggregated Generation By Type - (.+?) - GB', col)\n",
    "        if match:\n",
    "            # Replace spaces with underscores for readability\n",
    "            return match.group(1).replace(\" \", \"_\")\n",
    "        return col  # Return the column as is if no match is found\n",
    "\n",
    "    # Rename columns using the clean_column_name function\n",
    "    df.columns = [clean_column_name(col) for col in df.columns]\n",
    "    for column in df.columns:\n",
    "        if column != 'GMT Time':  # Skip the 'GMT Time' column\n",
    "            df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the function to rename columns in generation_df\n",
    "GB_generation_df = rename_columns_generation(GB_generation_df)\n",
    "\n",
    "\n",
    "print(\"Final columns:\")\n",
    "print(GB_generation_df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final columns:\n",
      "['GMT Time' 'Day_Ahead_Price' 'Intraday_Price']\n"
     ]
    }
   ],
   "source": [
    "def rename_epex_columns(df):\n",
    "    # Define a dictionary for manual renaming based on your desired column names\n",
    "    rename_map = {\n",
    "        'GMT Time': 'GMT Time',\n",
    "        'Day Ahead Price (EPEX half-hourly, local) - GB (LC/MWh)': 'Day_Ahead_Price',\n",
    "        'Intraday Price (EPEX Outturn, APX, MID) - GB (£/MWh)': 'Intraday_Price'\n",
    "    }\n",
    "\n",
    "    # Rename columns using the dictionary\n",
    "    df = df.rename(columns=rename_map)\n",
    "    for column in df.columns:\n",
    "        if column != 'GMT Time':  # Skip the 'GMT Time' column\n",
    "            df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the function to rename columns in EPEX_price_df\n",
    "EPEX_price_df = rename_epex_columns(EPEX_price_df)\n",
    "\n",
    "\n",
    "print(\"Final columns:\")\n",
    "print(EPEX_price_df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged columns:\n",
      "['Datetime' 'System_Price' 'NIV_Outturn' 'BM_Bid_Acceptances'\n",
      " 'BM_Offer_Acceptances' 'BSAD_Turn_Up' 'BSAD_Turn_Down' 'BSAD_Total'\n",
      " 'EPEX_Intraday_Volume' 'Loss_of_Load_Prob' 'Total_Load' 'Demand_Outturn'\n",
      " 'Biomass' 'Fossil_Gas' 'Fossil_Hard_Coal' 'Fossil_Oil'\n",
      " 'Hydro_Pumped_Storage' 'Hydro_Run-of-River_and_Poundage' 'Nuclear'\n",
      " 'Solar' 'Wind_Onshore' 'Wind_Offshore' 'Day_Ahead_Price' 'Intraday_Price']\n"
     ]
    }
   ],
   "source": [
    "# Set 'GMT Time' as index for each dataframe\n",
    "balancing_df.set_index('GMT Time', inplace=True)\n",
    "GB_demand_df.set_index('GMT Time', inplace=True)\n",
    "GB_generation_df.set_index('GMT Time', inplace=True)\n",
    "EPEX_price_df.set_index('GMT Time', inplace=True)\n",
    "\n",
    "# Merge using index\n",
    "merged_df = balancing_df.join([GB_demand_df, GB_generation_df, EPEX_price_df], how='inner')\n",
    "# We put back the datetime column into the merged DF and rename it for practicality\n",
    "merged_df.reset_index(inplace=True)\n",
    "merged_df.rename(columns={'GMT Time': 'Datetime'}, inplace=True)\n",
    "\n",
    "print(\"Merged columns:\")\n",
    "print(merged_df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fft(series, n_top_seasonalities, threshold_pc=0.02):\n",
    "    \"\"\"\n",
    "    Calculate significant positive frequencies and their amplitudes using Fast Fourier Transform (FFT),\n",
    "    selecting the lower of 2% of the max amplitude or the top `n` frequencies.\n",
    "\n",
    "    Parameters:\n",
    "    - series (pd.Series): The input time series data.\n",
    "    - n_top_seasonalities (int): The maximum number of significant frequencies to consider.\n",
    "    - threshold_pc (float): Percentage (0 < threshold_pc <= 1) of the maximum amplitude to filter significant frequencies.\n",
    "\n",
    "    Returns:\n",
    "    - zip: A generator yielding (positive frequency, amplitude) for each significant frequency.\n",
    "    \"\"\"\n",
    "    # Compute fast Fourier transform\n",
    "    price_fft = np.fft.fft(series.dropna())\n",
    "\n",
    "    # Get frequencies corresponding to FFT coefficients\n",
    "    freqs = np.fft.fftfreq(len(price_fft), d=1/48)\n",
    "\n",
    "    # Calculate amplitudes\n",
    "    amplitudes = np.abs(price_fft)\n",
    "\n",
    "    # Calculate the threshold based on 2% of the max amplitude\n",
    "    threshold = threshold_pc * np.max(amplitudes)\n",
    "\n",
    "    # Filter positive frequencies with amplitudes above threshold\n",
    "    positive_indices = np.where((amplitudes > threshold) & (freqs > 0))\n",
    "    positive_freqs = freqs[positive_indices]\n",
    "    positive_amplitudes = amplitudes[positive_indices]\n",
    "\n",
    "    # Sort by amplitude and select the lower of `n_top_seasonalities` or all significant frequencies\n",
    "    sorted_indices = np.argsort(positive_amplitudes)[::-1]\n",
    "    selected_indices = sorted_indices[:min(n_top_seasonalities, len(sorted_indices))]\n",
    "\n",
    "    # Select the top frequencies and amplitudes\n",
    "    significant_freqs = positive_freqs[selected_indices]\n",
    "    significant_amplitudes = positive_amplitudes[selected_indices]\n",
    "\n",
    "    return zip(significant_freqs, significant_amplitudes)\n",
    "\n",
    "\n",
    "def prophet_predictions(series, freq_amp):\n",
    "    \"\"\"\n",
    "    Generate predictions using Prophet with multiple seasonalities based on significant frequencies.\n",
    "\n",
    "    Parameters:\n",
    "    - series (pd.Series): The input time series data.\n",
    "    - freq_amp (list of tuples): A list of (frequency, amplitude) pairs, where each frequency represents \n",
    "                                 a significant periodic component to be modeled as seasonality.\n",
    "\n",
    "    Returns:\n",
    "    - forecast (DataFrame): The forecasted values for the specified period, including trend and seasonal components.\n",
    "    \"\"\"\n",
    "    # Prepare data for Prophet\n",
    "    df = pd.DataFrame({'ds': series.index, 'y': series})\n",
    "    model = Prophet()\n",
    "\n",
    "    # Adding seasonalities based on significant frequencies\n",
    "    for freq, amp in freq_amp:\n",
    "        if freq != 0:  # Ignore the DC component\n",
    "            period_in_days = 1 / freq\n",
    "            seasonality_name = f\"seasonal_freq_{freq:.4f}\"\n",
    "            fourier_order = 5 if period_in_days <= 1 else (10 if period_in_days <= 7 else 20)\n",
    "            model.add_seasonality(name=seasonality_name, period=period_in_days, fourier_order=fourier_order)\n",
    "\n",
    "    model.fit(df)\n",
    "    future = model.make_future_dataframe(periods=48, freq='30T')\n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    return forecast.set_index('ds')['yhat']\n",
    "\n",
    "def statsforecast_arima(df):\n",
    "    \"\"\"\n",
    "    Fit an AutoARIMA model to the time series data and forecast future values.\n",
    "\n",
    "    This function uses AutoARIMA from the statsforecast package to automatically select the best ARIMA model.\n",
    "    It performs both in-sample prediction and forecasts future values beyond the length of the data provided.\n",
    "\n",
    "    Parameters:\n",
    "    - df (Series): The input time series data. The index must be a DateTimeIndex.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A DataFrame containing the in-sample predictions and forecasted values over an extended range.\n",
    "    \"\"\"\n",
    "    # Ensure the input is a pandas DataFrame with required columns\n",
    "    df = pd.DataFrame({'unique_id': 1, 'ds': df.index, 'y': df.values})\n",
    "\n",
    "    # Initialize the StatsForecast object with the AutoARIMA model\n",
    "    sf = StatsForecast( models=[AutoARIMA()], freq='30min', n_jobs=-1)\n",
    "\n",
    "    sf.fit()\n",
    "    \n",
    "    # Define the forecast horizon\n",
    "    forecast_horizon = 48  # 24 hours at 30-minute intervals\n",
    "\n",
    "    # Forecast future values\n",
    "    forecast = sf.forecast(df=df, h=forecast_horizon, fitted=True)\n",
    "    values=sf.forecast_fitted_values()\n",
    "    values.set_index('ds', inplace=True)\n",
    "    forecast.set_index('ds', inplace=True)\n",
    "    result = pd.concat([values, forecast])\n",
    "    return result[\"AutoARIMA\"]\n",
    "\n",
    "\n",
    "def ensemble_model(series, fft_threshold):\n",
    "    \"\"\"\n",
    "    Generate an ensemble forecast by combining Prophet and ARIMA models directly on a time series.\n",
    "\n",
    "    Parameters:\n",
    "    - series (pd.Series): The input time series data with DateTimeIndex.\n",
    "    - fft_threshold (float): The threshold for filtering frequencies in the Fast Fourier Transform (FFT) for Prophet.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A DataFrame containing the original data, Prophet predictions, ARIMA residual forecasts,\n",
    "      and the final combined forecast.\n",
    "    \"\"\"\n",
    "    # Step 1: Calculate predictions with Prophet\n",
    "    freq_amp_pairs = calculate_fft(series, fft_threshold)\n",
    "    preds_prophet = prophet_predictions(series, freq_amp_pairs)\n",
    "    \n",
    "    # Step 2: Calculate Residuals\n",
    "    residuals = series - preds_prophet\n",
    "\n",
    "    # Step 3: Fit ARIMA on the Residuals\n",
    "    arima_forecast = statsforecast_arima(residuals.dropna())  # Ensure non-na data for ARIMA\n",
    "    \n",
    "    # Step 4: Combine the Predictions of Prophet and ARIMA\n",
    "    combined_forecast = preds_prophet.add(arima_forecast)  # Using fill_value to handle NaNs\n",
    "\n",
    "    # Prepare the result DataFrame\n",
    "    results = pd.DataFrame({\n",
    "        'original_data': series,\n",
    "        'prophet_forecast': preds_prophet,\n",
    "        'arima_residual_forecast': arima_forecast,\n",
    "        'combined_forecast': combined_forecast\n",
    "    })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the maximum number of consecutive NaNs filled in a column\n",
    "# As Angelica Asked\n",
    "def max_consecutive_nans_filled(df, column):\n",
    "    \"\"\"\n",
    "\n",
    "    This function calculates and returns the maximum number \n",
    "    of consecutive NaNs in a column that is to be filled\n",
    "\n",
    "    \"\"\"\n",
    "    # Identify consecutive NaNs\n",
    "    na_groups = df[column].isna().astype(int).groupby(df[column].notna().cumsum()).sum()\n",
    "    # Get the maximum number of consecutive NaNs that would be interpolated\n",
    "    max_consecutive_nans = na_groups.max()\n",
    "    nans_before = df[column].isna().sum()\n",
    "\n",
    "    print(f\"NaNs in {column}: {nans_before}\")\n",
    "    print(f\"Max consecutive NaNs filled for '{column}': {max_consecutive_nans}\")\n",
    "    return\n",
    "\n",
    "    \n",
    "def fill_nans_with_prophet(series):\n",
    "    \"\"\"\n",
    "    Fills NaNs in the original time series data using predictions from the Prophet model.\n",
    "\n",
    "    Parameters:\n",
    "    - series (pd.Series): The input time series data with potential NaNs.\n",
    "\n",
    "    Returns:\n",
    "    - pd.Series: The time series with NaNs filled using Prophet predictions.\n",
    "    \"\"\"\n",
    "    if series.isna().any():\n",
    "        print(\"NaN detected, proceeding to fill...\")\n",
    "        \n",
    "        # Step 1: Calculate significant frequencies\n",
    "        freq_amp = calculate_fft(series, n_top_seasonalities=12, threshold_pc=0.02)\n",
    "        print(\"Frequencies and amplitudes calculated:\", freq_amp)\n",
    "\n",
    "        # Step 2: Generate Prophet predictions\n",
    "        predictions = prophet_predictions(series, freq_amp)\n",
    "        print(f\"Predictions length: {len(predictions)}, Series length: {len(series)}\")\n",
    "        \n",
    "        # Step 3: Trim predictions to match series index\n",
    "        predictions = predictions.iloc[:len(series)]\n",
    "        predictions.index = series.index  # Align indices\n",
    "\n",
    "        # Step 4: Fill NaNs in the series\n",
    "        filled_series = series.combine_first(predictions)\n",
    "        print(\"NaNs filled in the series.\")\n",
    "        \n",
    "        return filled_series\n",
    "    else:\n",
    "        print(\"No NaNs detected. Returning original series.\")\n",
    "        return series\n",
    "def process_dataframe(df):\n",
    "    \"\"\"\n",
    "    Iterates over each column of the DataFrame, applying Prophet-based NaN filling where applicable.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame with multiple time series columns, potentially containing NaNs.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The DataFrame with NaNs filled where possible.\n",
    "    \"\"\"\n",
    "    for column in df.columns:\n",
    "        # Check if the column data type is numeric (Prophet requires numeric types)\n",
    "        if column != \"Datetime\":\n",
    "            print(f\"Processing column: {column}\")\n",
    "            df[column] = fill_nans_with_prophet(df[column])\n",
    "        else:\n",
    "            print(f\"Skipping column: {column} (non-numeric data)\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling in the NaNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So First, we try to fill in the columns that can be filled using other columns\n",
    "### NIV_Outturn = - (BM_Bid_Acceptances + BM_Offer_Acceptances) \n",
    "### and \n",
    "### BSAD_Total = BSAD_Turn_Down + BSAD_Turn_Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the \"No Data Available\" by 0s in the BSAD columns where applicable\n",
    "# if all three are missing we just let them be replaced by NaNs\n",
    "\n",
    "# Replace \"No Data Available\" in \"BSAD_Turn_Up\" with 0 if \"BSAD_Total\" is equal to other column\n",
    "merged_df.loc[(merged_df[\"BSAD_Turn_Up\"].isna()) & (merged_df[\"BSAD_Total\"] == merged_df[\"BSAD_Turn_Down\"]), \"BSAD_Turn_Up\"] = 0\n",
    "\n",
    "# Replace \"No Data Available\" in \"BSAD_Turn_Down\" with 0 if \"BSAD_Total\" is equal to other column\n",
    "merged_df.loc[(merged_df[\"BSAD_Turn_Down\"].isna()) & (merged_df[\"BSAD_Total\"] == merged_df[\"BSAD_Turn_Up\"]), \"BSAD_Turn_Down\"] = 0    \n",
    "\n",
    "# Replace 'NIV_Outturn' with NaN if both 'BM_Bid_Acceptances' and 'BM_Offer_Acceptances' are NaN and 'NIV_Outturn' is 0\n",
    "merged_df.loc[(merged_df['NIV_Outturn'] == 0) & merged_df['BM_Bid_Acceptances'].isna() & merged_df['BM_Offer_Acceptances'].isna(), 'NIV_Outturn'] = np.nan\n",
    "\n",
    "# Replace 'NIV_Outturn' with the negative of the sum of 'BM_Offer_Acceptances' and 'BM_Bid_Acceptances' \n",
    "# if 'NIV_Outturn' is zero and neither of the other two columns contains NaN\n",
    "merged_df.loc[(merged_df['NIV_Outturn'] == 0) & merged_df['BM_Offer_Acceptances'].notna() & merged_df['BM_Bid_Acceptances'].notna(), 'NIV_Outturn'] = -(merged_df['BM_Offer_Acceptances'] + merged_df['BM_Bid_Acceptances'])\n",
    "\n",
    "# Extrapolate 'BM_Bid_Acceptances' with condition to set both columns to NaN if bid check fails\n",
    "bid_values = -merged_df['NIV_Outturn'] - merged_df['BM_Offer_Acceptances']\n",
    "merged_df.loc[merged_df['BM_Bid_Acceptances'].isna() & merged_df['NIV_Outturn'].notna(), 'BM_Bid_Acceptances'] = bid_values.where(bid_values <= 0)\n",
    "merged_df.loc[merged_df['BM_Bid_Acceptances'].isna(), 'BM_Offer_Acceptances'] = np.nan\n",
    "\n",
    "# Extrapolate 'BM_Offer_Acceptances' with condition to set both columns to NaN if offer check fails\n",
    "offer_values = -merged_df['NIV_Outturn'] - merged_df['BM_Bid_Acceptances']\n",
    "merged_df.loc[merged_df['BM_Offer_Acceptances'].isna() & merged_df['NIV_Outturn'].notna(), 'BM_Offer_Acceptances'] = offer_values.where(offer_values >= 0)\n",
    "merged_df.loc[merged_df['BM_Offer_Acceptances'].isna(), 'BM_Bid_Acceptances'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markt the rows where there are missing values for each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in merged_df.columns:\n",
    "    merged_df[f'{column}_missing'] = merged_df[column].isnull().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.set_index(\"Datetime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then, we fill the other NaNs using Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN detected, proceeding to fill...\n",
      "Frequencies and amplitudes calculated: <zip object at 0x000001F5C7125200>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:09:19 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:18:35 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions length: 118368, Series length: 118320\n",
      "NaNs filled in the series.\n",
      "NaN detected, proceeding to fill...\n",
      "Frequencies and amplitudes calculated: <zip object at 0x000001F4FA1DE840>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:21:35 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m selected_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWind_Onshore\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWind_Offshore\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDay_Ahead_Price\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIntraday_Price\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m selected_cols:\n\u001b[1;32m----> 4\u001b[0m     merged_df[col] \u001b[38;5;241m=\u001b[39m fill_nans_with_prophet(merged_df[col])\n",
      "Cell \u001b[1;32mIn[11], line 39\u001b[0m, in \u001b[0;36mfill_nans_with_prophet\u001b[1;34m(series)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrequencies and amplitudes calculated:\u001b[39m\u001b[38;5;124m\"\u001b[39m, freq_amp)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Step 2: Generate Prophet predictions\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m predictions \u001b[38;5;241m=\u001b[39m prophet_predictions(series, freq_amp)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(predictions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Series length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(series)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Step 3: Trim predictions to match series index\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 66\u001b[0m, in \u001b[0;36mprophet_predictions\u001b[1;34m(series, freq_amp)\u001b[0m\n\u001b[0;32m     63\u001b[0m         fourier_order \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m period_in_days \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;241m10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m period_in_days \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m     64\u001b[0m         model\u001b[38;5;241m.\u001b[39madd_seasonality(name\u001b[38;5;241m=\u001b[39mseasonality_name, period\u001b[38;5;241m=\u001b[39mperiod_in_days, fourier_order\u001b[38;5;241m=\u001b[39mfourier_order)\n\u001b[1;32m---> 66\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(df)\n\u001b[0;32m     67\u001b[0m future \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmake_future_dataframe(periods\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m48\u001b[39m, freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m30T\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     68\u001b[0m forecast \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(future)\n",
      "File \u001b[1;32mc:\\Users\\nicki\\anaconda3\\Lib\\site-packages\\prophet\\forecaster.py:1232\u001b[0m, in \u001b[0;36mProphet.fit\u001b[1;34m(self, df, **kwargs)\u001b[0m\n\u001b[0;32m   1230\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstan_backend\u001b[38;5;241m.\u001b[39msampling(stan_init, dat, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmcmc_samples, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1231\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstan_backend\u001b[38;5;241m.\u001b[39mfit(stan_init, dat, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstan_fit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstan_backend\u001b[38;5;241m.\u001b[39mstan_fit\n\u001b[0;32m   1235\u001b[0m \u001b[38;5;66;03m# If no changepoints were requested, replace delta with 0s\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nicki\\anaconda3\\Lib\\site-packages\\prophet\\models.py:121\u001b[0m, in \u001b[0;36mCmdStanPyBackend.fit\u001b[1;34m(self, stan_init, stan_data, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m args\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstan_fit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;66;03m# Fall back on Newton\u001b[39;00m\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnewton_fallback \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malgorithm\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNewton\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\nicki\\anaconda3\\Lib\\site-packages\\cmdstanpy\\model.py:644\u001b[0m, in \u001b[0;36mCmdStanModel.optimize\u001b[1;34m(self, data, seed, inits, output_dir, sig_figs, save_profile, algorithm, init_alpha, tol_obj, tol_rel_obj, tol_grad, tol_rel_grad, tol_param, history_size, iter, save_iterations, require_converged, show_console, refresh, time_fmt, timeout, jacobian)\u001b[0m\n\u001b[0;32m    642\u001b[0m     dummy_chain_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    643\u001b[0m     runset \u001b[38;5;241m=\u001b[39m RunSet(args\u001b[38;5;241m=\u001b[39margs, chains\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, time_fmt\u001b[38;5;241m=\u001b[39mtime_fmt)\n\u001b[1;32m--> 644\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_cmdstan(\n\u001b[0;32m    645\u001b[0m         runset,\n\u001b[0;32m    646\u001b[0m         dummy_chain_id,\n\u001b[0;32m    647\u001b[0m         show_console\u001b[38;5;241m=\u001b[39mshow_console,\n\u001b[0;32m    648\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    649\u001b[0m     )\n\u001b[0;32m    650\u001b[0m runset\u001b[38;5;241m.\u001b[39mraise_for_timeouts()\n\u001b[0;32m    652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m runset\u001b[38;5;241m.\u001b[39m_check_retcodes():\n",
      "File \u001b[1;32mc:\\Users\\nicki\\anaconda3\\Lib\\site-packages\\cmdstanpy\\model.py:2087\u001b[0m, in \u001b[0;36mCmdStanModel._run_cmdstan\u001b[1;34m(self, runset, idx, show_progress, show_console, progress_hook, timeout)\u001b[0m\n\u001b[0;32m   2085\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mpoll() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2086\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mstdout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2087\u001b[0m         line \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mreadline()\n\u001b[0;32m   2088\u001b[0m         fd_out\u001b[38;5;241m.\u001b[39mwrite(line)\n\u001b[0;32m   2089\u001b[0m         line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[1;32m<frozen codecs>:319\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Process the DataFrame\n",
    "selected_cols = [\"Wind_Onshore\",\"Wind_Offshore\",\"Day_Ahead_Price\",\"Intraday_Price\"]\n",
    "for col in selected_cols:\n",
    "    merged_df[col] = fill_nans_with_prophet(merged_df[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the filled df in case we need to retrieve it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to save the merged to csv for easier retrieval\n",
    "merged_df = merged_df[selected_cols]\n",
    "merged_df.reset_index(inplace=True)\n",
    "merged_df.to_csv(\"merged_df_Nicole.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOP HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data for the next 48 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv(\"merged_df_prophet_filled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for predictions for different variables,we use different lenghts for training the ensemble model\n",
    "merged_df.set_index('Datetime', inplace=True)\n",
    "merged_df.index = pd.to_datetime(merged_df.index)\n",
    "\n",
    "merged_df_2024 = merged_df[(merged_df.index >= '2023-10-01') & (merged_df.index < '2025-01-01')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating rows to append generated predictions\n",
    "# Generate a date range that starts after the last date in merged_df_2024\n",
    "date_range_df_temp = pd.DataFrame({'value': [None] * 48}, index=pd.date_range(start=merged_df.index[-1] + pd.Timedelta(minutes=30), periods=48, freq='30T'))\n",
    "# Concatenate without resetting the index, preserving the datetime index\n",
    "df_with_preds = pd.concat([merged_df, date_range_df_temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "45\n",
      "45\n",
      "45\n",
      "45\n",
      "15\n",
      "45\n",
      "6\n",
      "45\n",
      "5\n",
      "41\n",
      "32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:18:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:20:15 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:31:16 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "45\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Function to handle the prediction for a single column\n",
    "def predict_for_column(column):\n",
    "    if column != \"System_Price\":\n",
    "        # Creating prediction using Prophet and ARIMA for the column\n",
    "        prophet_arima_preds = prophet_predictions(merged_df[[column]], 45)\n",
    "        # Extracting the predictions for the next 48 observations\n",
    "        predictions = prophet_arima_preds['combined_forecast'].iloc[-48:].values\n",
    "        return column, predictions\n",
    "    return None\n",
    "\n",
    "# Parallelizing the process\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    # Run the prediction function for each column in parallel\n",
    "    results = list(executor.map(predict_for_column, merged_df.columns))\n",
    "\n",
    "# Update df_with_preds with predictions\n",
    "for result in results:\n",
    "    if result:  # Ensure result is not None\n",
    "        column, predictions = result\n",
    "        df_with_preds.loc[df_with_preds.index[-48:], column] = predictions\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df_with_preds.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date related booleans\n",
    "import holidays\n",
    "\n",
    "# Get British holidays\n",
    "uk_holidays = holidays.UnitedKingdom()\n",
    "\n",
    "df_with_preds['Datetime'] = df_with_preds.index\n",
    "\n",
    "# Add a boolean column for British holidays\n",
    "df_with_preds['is_british_holiday'] = df_with_preds['Datetime'].isin(uk_holidays)\n",
    "\n",
    "print(df_with_preds.head())\n",
    "df_with_preds['day_of_week'] = df_with_preds['Datetime'].dt.dayofweek\n",
    "df_with_preds['is_weekday'] = df_with_preds['Datetime'].dt.weekday < 5  # Monday (0) to Friday (4)\n",
    "df_with_preds['is_weekend'] = df_with_preds['day_of_week'] >= 5  # Saturday and Sunday\n",
    "\n",
    "df_with_preds['hour_of_day'] = df_with_preds['Datetime'].dt.hour\n",
    "df_with_preds['is_peak_hour'] = df_with_preds['hour_of_day'].isin([7, 8, 9, 18, 19, 20])  # Example peak hours\n",
    "\n",
    "# Month, Quarter, and Seasons\n",
    "df_with_preds['month'] = df_with_preds['Datetime'].dt.month\n",
    "df_with_preds['quarter'] = df_with_preds['Datetime'].dt.quarter\n",
    "\"\"\"\n",
    "# Daylight Saving Time\n",
    "def is_dst(date):\n",
    "    return bool(pytz.timezone('Europe/London').dst(date))\n",
    "df_with_preds['is_dst'] = df_with_preds['Datetime'].apply(is_dst)\n",
    "\"\"\"\n",
    "df_with_preds['is_working_day'] = (~df_with_preds['is_british_holiday']) & (~df_with_preds['is_weekend'])\n",
    "\n",
    "def get_season(date):\n",
    "    year = date.year\n",
    "    seasons = {\n",
    "        'Winter': (pd.Timestamp(f'{year}-12-21'), pd.Timestamp(f'{year+1}-03-20')),\n",
    "        'Spring': (pd.Timestamp(f'{year}-03-21'), pd.Timestamp(f'{year}-06-20')),\n",
    "        'Summer': (pd.Timestamp(f'{year}-06-21'), pd.Timestamp(f'{year}-09-22')),\n",
    "        'Fall':   (pd.Timestamp(f'{year}-09-23'), pd.Timestamp(f'{year}-12-20'))\n",
    "    }\n",
    "    for season, (start, end) in seasons.items():\n",
    "        if start <= date <= end:\n",
    "            return season\n",
    "    return 'Unknown'\n",
    "\n",
    "# Add a season column\n",
    "df_with_preds['season'] = df_with_preds['Datetime'].apply(get_season)\n",
    "\n",
    "df_with_preds['is_christmas_season'] = df_with_preds['Datetime'].between(pd.Timestamp('2024-12-20'), pd.Timestamp('2024-12-31'))\n",
    "df_with_preds['is_summer_vacation'] = df_with_preds['Datetime'].dt.month.isin([7, 8])\n",
    "# Reset Datetime as the index\n",
    "df_with_preds = df_with_preds.set_index('Datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (17616) does not match length of index (118368)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m filtered_columns \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m prophet_arima_preds_SP\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseasonal_freq_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m col \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m col\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_lower\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_upper\u001b[39m\u001b[38;5;124m\"\u001b[39m))] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdaily\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweekly\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditive_terms\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrend\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m## Add the filtered columns from prophet_arima_preds_SP to df_with_preds\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m \u001b[43mdf_with_preds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfiltered_columns\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m prophet_arima_preds_SP[filtered_columns]\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4299\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4297\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_frame(key, value)\n\u001b[0;32m   4298\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, (Series, np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mlist\u001b[39m, Index)):\n\u001b[1;32m-> 4299\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4300\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, DataFrame):\n\u001b[0;32m   4301\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_item_frame_value(key, value)\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4350\u001b[0m, in \u001b[0;36mDataFrame._setitem_array\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4347\u001b[0m         \u001b[38;5;28mself\u001b[39m[col] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m   4349\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m-> 4350\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iset_not_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4352\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(value) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4353\u001b[0m     \u001b[38;5;66;03m# list of lists\u001b[39;00m\n\u001b[0;32m   4354\u001b[0m     value \u001b[38;5;241m=\u001b[39m DataFrame(value)\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4380\u001b[0m, in \u001b[0;36mDataFrame._iset_not_inplace\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4377\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns must be same length as key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4379\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(key):\n\u001b[1;32m-> 4380\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m igetitem(value, i)\n\u001b[0;32m   4382\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4383\u001b[0m     ilocs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_indexer_non_unique(key)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4311\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   4309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4310\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[1;32m-> 4311\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4524\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4515\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4516\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   4517\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4522\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   4523\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4524\u001b[0m     value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4527\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m   4528\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   4529\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[0;32m   4530\u001b[0m     ):\n\u001b[0;32m   4531\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   4532\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:5266\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   5263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m   5265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m-> 5266\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5267\u001b[0m arr \u001b[38;5;241m=\u001b[39m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   5268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   5269\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(value, Index)\n\u001b[0;32m   5270\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5273\u001b[0m     \u001b[38;5;66;03m# TODO: Remove kludge in sanitize_array for string mode when enforcing\u001b[39;00m\n\u001b[0;32m   5274\u001b[0m     \u001b[38;5;66;03m# this deprecation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\pandas\\core\\common.py:573\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m--> 573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    577\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    578\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (17616) does not match length of index (118368)"
     ]
    }
   ],
   "source": [
    "# Variable that tracks the difference between total load and demand\n",
    "df_with_preds[\"Load-Demand\"] = df_with_preds[\"Total_Load\"] - df_with_preds[\"Demand_Outturn\"]\n",
    "\n",
    "# Recalculate the LoLP using the Normal CDF (we only use the difference btw Total load and demand outturn)\n",
    "# For instructions, we consulted:\n",
    "# approximation since we do not have some of the information\n",
    "# https://bscdocs.elexon.co.uk/category-3-documents/loss-of-load-probability-calculation-methodolgy-statement\n",
    "df_with_preds['LoLP'] = 1 - norm.cdf(df_with_preds[\"Load-Demand\"], loc=0, scale=np.sqrt(700))\n",
    "# Calculate the LoLP lag 1 as proxy for prediction of not enough electricity for next day since the load and demand are super autocorellated\n",
    "df_with_preds['LoLP_lag1'] = df_with_preds['LoLP'].shift(1).copy()\n",
    "\n",
    "\n",
    "# Feature engineering to create wind+solar variable, ignoring NaNs (if there is NaN in one of them, the sum is not NaN)\n",
    "df_with_preds[\"Wind_Solar\"] = df_with_preds[[\"Solar\", \"Wind_Onshore\", \"Wind_Offshore\"]].sum(axis=1, skipna=True)\n",
    "# Sum all columns except 'GMT Time', ignoring NaNs\n",
    "df_with_preds['Total_Generation'] = df_with_preds[['Biomass', 'Fossil_Gas', 'Fossil_Hard_Coal', 'Fossil_Oil',\n",
    "                                                    'Hydro_Pumped_Storage', 'Hydro_Run-of-River_and_Poundage', 'Nuclear',\n",
    "                                                    'Solar', 'Wind_Onshore', 'Wind_Offshore']].sum(axis=1, skipna=True)\n",
    "\n",
    "\n",
    "# Total_Load = Total Generation + Exports - Imports - Stored Energy\n",
    "# So we create a column that is the difference between exports, imports and stored energy\n",
    "df_with_preds[\"Exports-Imports-Stored\"] = df_with_preds[\"Total_Load\"] - df_with_preds[\"Total_Generation\"]\n",
    "df_with_preds[\"Generation-Demand\"] = df_with_preds[\"Total_Generation\"] - df_with_preds[\"Demand_Outturn\"]\n",
    "\n",
    "# Recalculate the LoLP using Generation, using the Normal CDF (we only use the difference btw Total Generation and demand outturn)\n",
    "# For instructions, we consulted:\n",
    "# https://bscdocs.elexon.co.uk/category-3-documents/loss-of-load-probability-calculation-methodolgy-statement\n",
    "df_with_preds['LoLP_Gen'] = 1 - norm.cdf(df_with_preds[\"Total_Generation\"] - df_with_preds[\"Demand_Outturn\"], loc=0, scale=np.sqrt(700))\n",
    "# Calculate the LoLP lag 1 as proxy for prediction of not enough electricity for next day since the load and demand are super autocorellated\n",
    "df_with_preds['LoLP_Gen_lag1'] = df_with_preds['LoLP_Gen'].shift(1).copy()\n",
    "\n",
    "\n",
    "# Caolumn with Day Ahead Price but lag 48, since they are predictions for next day\n",
    "df_with_preds['Day_Ahead_Price_lag48'] = df_with_preds['Day_Ahead_Price'].shift(48).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, we save the df with predictions for all columns to a csv for easier retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_preds.reset_index(inplace=True)\n",
    "# In order to save the merged to csv for easier retrieval\n",
    "df_with_preds.to_csv(\"merged_df_with_preds.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
