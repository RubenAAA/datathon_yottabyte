{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the data\n",
    "### setting comprehensible col names and right types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the necessary packages\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore FutureWarning for deprecated 'T' frequency in Prophet\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"'T' is deprecated\")\n",
    "\n",
    "# Ignore PerformanceWarning from pandas\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mix of different sources, mostly ESO\n",
    "balancing_df = pd.read_csv(\"balancing_data.csv\")\n",
    "# Demand data only for GB\n",
    "GB_demand_df = pd.read_csv(\"demand_load_data.csv\")\n",
    "# Generation data only for GB\n",
    "GB_generation_df = pd.read_csv(\"generation_data.csv\")\n",
    "# the price dataframe only concerns EPEX (only prices from there)\n",
    "EPEX_price_df = pd.read_csv(\"price_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final columns:\n",
      "['GMT Time' 'System_Price' 'NIV_Outturn' 'BM_Bid_Acceptances'\n",
      " 'BM_Offer_Acceptances' 'BSAD_Turn_Up' 'BSAD_Turn_Down' 'BSAD_Total'\n",
      " 'EPEX_Intraday_Volume']\n"
     ]
    }
   ],
   "source": [
    "def rename_balancing_columns(df):\n",
    "    # Define a dictionary for concise renaming\n",
    "    rename_map = {\n",
    "        'GMT Time': 'GMT Time',\n",
    "        'System Price (ESO Outturn) - GB (£/MWh)': 'System_Price',\n",
    "        'NIV Outturn (+ve long) - GB (MW)': 'NIV_Outturn',\n",
    "        'BM Bid Acceptances (total) - GB (MW)': 'BM_Bid_Acceptances',\n",
    "        'BM Offer Acceptances (total) - GB (MW)': 'BM_Offer_Acceptances',\n",
    "        'Total BSAD Volume - Turn Up - GB (MW)': 'BSAD_Turn_Up',\n",
    "        'Total BSAD Volume - Turn Down - GB (MW)': 'BSAD_Turn_Down',\n",
    "        'Total BSAD Volume - Total - GB (MW)': 'BSAD_Total',\n",
    "        'Intraday Volume (EPEX Outturn, APX, MID) - GB (MWh)': 'EPEX_Intraday_Volume'\n",
    "    }\n",
    "    \n",
    "    # Apply the renaming map\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    # Force all the non datetime columns to numeric\n",
    "    for column in df.columns:\n",
    "        if column != 'GMT Time':  # Skip the 'GMT Time' column\n",
    "            df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "    return df\n",
    "\n",
    "# Apply the function to rename columns in balancing_df\n",
    "balancing_df = rename_balancing_columns(balancing_df)\n",
    "\n",
    "print(\"Final columns:\")\n",
    "print(balancing_df.columns.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final columns:\n",
      "['GMT Time' 'Loss_of_Load_Prob' 'Total_Load' 'Demand_Outturn']\n"
     ]
    }
   ],
   "source": [
    "def rename_demand_columns(df):\n",
    "    \"\"\"\n",
    "    Rename columns for easier reference and convert non-datetime columns to numeric.\n",
    "    \"\"\"\n",
    "    # Define a dictionary for concise renaming\n",
    "    rename_map = {\n",
    "        'GMT Time': 'GMT Time',\n",
    "        'Loss of Load Probability - Latest - GB ()': 'Loss_of_Load_Prob',\n",
    "        'Actual Total Load - GB (MW)': 'Total_Load',\n",
    "        'Demand Outturn (ITSDO) - GB (MW)': 'Demand_Outturn'\n",
    "    }\n",
    "    \n",
    "    # Apply the renaming map\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    # Force all the non-datetime columns to numeric\n",
    "    for column in df.columns:\n",
    "        if column != 'GMT Time':  # Skip the 'GMT Time' column\n",
    "            df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "    return df\n",
    "\n",
    "# Apply the renaming and filling functions\n",
    "GB_demand_df = rename_demand_columns(GB_demand_df)\n",
    "\n",
    "\n",
    "print(\"Final columns:\")\n",
    "print(GB_demand_df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final columns:\n",
      "['GMT Time' 'Biomass' 'Fossil_Gas' 'Fossil_Hard_Coal' 'Fossil_Oil'\n",
      " 'Hydro_Pumped_Storage' 'Hydro_Run-of-River_and_Poundage' 'Nuclear'\n",
      " 'Solar' 'Wind_Onshore' 'Wind_Offshore']\n"
     ]
    }
   ],
   "source": [
    "def rename_columns_generation(df):\n",
    "    # Define a function to clean each column name\n",
    "    def clean_column_name(col):\n",
    "        # Extract the generation type using regex\n",
    "        match = re.search(r'Actual Aggregated Generation By Type - (.+?) - GB', col)\n",
    "        if match:\n",
    "            # Replace spaces with underscores for readability\n",
    "            return match.group(1).replace(\" \", \"_\")\n",
    "        return col  # Return the column as is if no match is found\n",
    "\n",
    "    # Rename columns using the clean_column_name function\n",
    "    df.columns = [clean_column_name(col) for col in df.columns]\n",
    "    for column in df.columns:\n",
    "        if column != 'GMT Time':  # Skip the 'GMT Time' column\n",
    "            df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the function to rename columns in generation_df\n",
    "GB_generation_df = rename_columns_generation(GB_generation_df)\n",
    "\n",
    "\n",
    "print(\"Final columns:\")\n",
    "print(GB_generation_df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final columns:\n",
      "['GMT Time' 'Day_Ahead_Price' 'Intraday_Price']\n"
     ]
    }
   ],
   "source": [
    "def rename_epex_columns(df):\n",
    "    # Define a dictionary for manual renaming based on your desired column names\n",
    "    rename_map = {\n",
    "        'GMT Time': 'GMT Time',\n",
    "        'Day Ahead Price (EPEX half-hourly, local) - GB (LC/MWh)': 'Day_Ahead_Price',\n",
    "        'Intraday Price (EPEX Outturn, APX, MID) - GB (£/MWh)': 'Intraday_Price'\n",
    "    }\n",
    "\n",
    "    # Rename columns using the dictionary\n",
    "    df = df.rename(columns=rename_map)\n",
    "    for column in df.columns:\n",
    "        if column != 'GMT Time':  # Skip the 'GMT Time' column\n",
    "            df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the function to rename columns in EPEX_price_df\n",
    "EPEX_price_df = rename_epex_columns(EPEX_price_df)\n",
    "\n",
    "\n",
    "print(\"Final columns:\")\n",
    "print(EPEX_price_df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged columns:\n",
      "['Datetime' 'System_Price' 'NIV_Outturn' 'BM_Bid_Acceptances'\n",
      " 'BM_Offer_Acceptances' 'BSAD_Turn_Up' 'BSAD_Turn_Down' 'BSAD_Total'\n",
      " 'EPEX_Intraday_Volume' 'Loss_of_Load_Prob' 'Total_Load' 'Demand_Outturn'\n",
      " 'Biomass' 'Fossil_Gas' 'Fossil_Hard_Coal' 'Fossil_Oil'\n",
      " 'Hydro_Pumped_Storage' 'Hydro_Run-of-River_and_Poundage' 'Nuclear'\n",
      " 'Solar' 'Wind_Onshore' 'Wind_Offshore' 'Day_Ahead_Price' 'Intraday_Price']\n"
     ]
    }
   ],
   "source": [
    "# Set 'GMT Time' as index for each dataframe\n",
    "balancing_df.set_index('GMT Time', inplace=True)\n",
    "GB_demand_df.set_index('GMT Time', inplace=True)\n",
    "GB_generation_df.set_index('GMT Time', inplace=True)\n",
    "EPEX_price_df.set_index('GMT Time', inplace=True)\n",
    "\n",
    "# Merge using index\n",
    "merged_df = balancing_df.join([GB_demand_df, GB_generation_df, EPEX_price_df], how='inner')\n",
    "# We put back the datetime column into the merged DF and rename it for practicality\n",
    "merged_df.reset_index(inplace=True)\n",
    "merged_df.rename(columns={'GMT Time': 'Datetime'}, inplace=True)\n",
    "\n",
    "print(\"Merged columns:\")\n",
    "print(merged_df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fft(series, n_top_seasonalities, threshold_pc=0.02):\n",
    "    \"\"\"\n",
    "    Calculate significant positive frequencies and their amplitudes using Fast Fourier Transform (FFT),\n",
    "    selecting the lower of 2% of the max amplitude or the top `n` frequencies.\n",
    "\n",
    "    Parameters:\n",
    "    - series (pd.Series): The input time series data.\n",
    "    - n_top_seasonalities (int): The maximum number of significant frequencies to consider.\n",
    "    - threshold_pc (float): Percentage (0 < threshold_pc <= 1) of the maximum amplitude to filter significant frequencies.\n",
    "\n",
    "    Returns:\n",
    "    - zip: A generator yielding (positive frequency, amplitude) for each significant frequency.\n",
    "    \"\"\"\n",
    "    # Compute fast Fourier transform\n",
    "    price_fft = np.fft.fft(series.dropna())\n",
    "\n",
    "    # Get frequencies corresponding to FFT coefficients\n",
    "    freqs = np.fft.fftfreq(len(price_fft), d=1/48)\n",
    "\n",
    "    # Calculate amplitudes\n",
    "    amplitudes = np.abs(price_fft)\n",
    "\n",
    "    # Calculate the threshold based on 2% of the max amplitude\n",
    "    threshold = threshold_pc * np.max(amplitudes)\n",
    "\n",
    "    # Filter positive frequencies with amplitudes above threshold\n",
    "    positive_indices = np.where((amplitudes > threshold) & (freqs > 0))\n",
    "    positive_freqs = freqs[positive_indices]\n",
    "    positive_amplitudes = amplitudes[positive_indices]\n",
    "\n",
    "    # Sort by amplitude and select the lower of `n_top_seasonalities` or all significant frequencies\n",
    "    sorted_indices = np.argsort(positive_amplitudes)[::-1]\n",
    "    selected_indices = sorted_indices[:min(n_top_seasonalities, len(sorted_indices))]\n",
    "\n",
    "    # Select the top frequencies and amplitudes\n",
    "    significant_freqs = positive_freqs[selected_indices]\n",
    "    significant_amplitudes = positive_amplitudes[selected_indices]\n",
    "\n",
    "    return zip(significant_freqs, significant_amplitudes)\n",
    "\n",
    "\n",
    "def prophet_predictions(series, freq_amp):\n",
    "    \"\"\"\n",
    "    Generate predictions using Prophet with multiple seasonalities based on significant frequencies.\n",
    "\n",
    "    Parameters:\n",
    "    - series (pd.Series): The input time series data.\n",
    "    - freq_amp (list of tuples): A list of (frequency, amplitude) pairs, where each frequency represents \n",
    "                                 a significant periodic component to be modeled as seasonality.\n",
    "\n",
    "    Returns:\n",
    "    - forecast (DataFrame): The forecasted values for the specified period, including trend and seasonal components.\n",
    "    \"\"\"\n",
    "    # Prepare data for Prophet\n",
    "    df = pd.DataFrame({'ds': series.index, 'y': series})\n",
    "    model = Prophet()\n",
    "\n",
    "    # Adding seasonalities based on significant frequencies\n",
    "    for freq, amp in freq_amp:\n",
    "        if freq != 0:  # Ignore the DC component\n",
    "            period_in_days = 1 / freq\n",
    "            seasonality_name = f\"seasonal_freq_{freq:.4f}\"\n",
    "            fourier_order = 5 if period_in_days <= 1 else (10 if period_in_days <= 7 else 20)\n",
    "            model.add_seasonality(name=seasonality_name, period=period_in_days, fourier_order=fourier_order)\n",
    "\n",
    "    model.fit(df)\n",
    "    future = model.make_future_dataframe(periods=48, freq='30T')\n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    return forecast.set_index('ds')['yhat']\n",
    "\n",
    "\n",
    "# Function to find the maximum number of consecutive NaNs filled in a column\n",
    "# As Angelica Asked\n",
    "def max_consecutive_nans_filled(df, column):\n",
    "    \"\"\"\n",
    "\n",
    "    This function calculates and returns the maximum number \n",
    "    of consecutive NaNs in a column that is to be filled\n",
    "\n",
    "    \"\"\"\n",
    "    # Identify consecutive NaNs\n",
    "    na_groups = df[column].isna().astype(int).groupby(df[column].notna().cumsum()).sum()\n",
    "    # Get the maximum number of consecutive NaNs that would be interpolated\n",
    "    max_consecutive_nans = na_groups.max()\n",
    "    nans_before = df[column].isna().sum()\n",
    "\n",
    "    print(f\"NaNs in {column}: {nans_before}\")\n",
    "    print(f\"Max consecutive NaNs filled for '{column}': {max_consecutive_nans}\")\n",
    "    return\n",
    "\n",
    "\n",
    "def fill_nans_with_prophet(series):\n",
    "    \"\"\"\n",
    "    Fills NaNs in the original time series data using predictions from the Prophet model.\n",
    "\n",
    "    Parameters:\n",
    "    - series (pd.Series): The input time series data with potential NaNs.\n",
    "\n",
    "    Returns:\n",
    "    - pd.Series: The time series with NaNs filled using Prophet predictions.\n",
    "    \"\"\"\n",
    "    if series.isna().any():\n",
    "        print(\"NaN detected, proceeding to fill...\")\n",
    "        \n",
    "        # Step 1: Calculate significant frequencies\n",
    "        freq_amp = calculate_fft(series, n_top_seasonalities=12, threshold_pc=0.02)\n",
    "        print(\"Frequencies and amplitudes calculated:\", freq_amp)\n",
    "\n",
    "        # Step 2: Generate Prophet predictions\n",
    "        predictions = prophet_predictions(series, freq_amp)\n",
    "        print(f\"Predictions length: {len(predictions)}, Series length: {len(series)}\")\n",
    "        \n",
    "        # Step 3: Trim predictions to match series index\n",
    "        predictions = predictions.iloc[:len(series)]\n",
    "        predictions.index = series.index  # Align indices\n",
    "\n",
    "        # Step 4: Fill NaNs in the series\n",
    "        filled_series = series.combine_first(predictions)\n",
    "        print(\"NaNs filled in the series.\")\n",
    "        \n",
    "        return filled_series\n",
    "    else:\n",
    "        print(\"No NaNs detected. Returning original series.\")\n",
    "        return series\n",
    "\n",
    "\n",
    "def process_dataframe(df):\n",
    "    \"\"\"\n",
    "    Iterates over each column of the DataFrame, applying Prophet-based NaN filling where applicable.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame with multiple time series columns, potentially containing NaNs.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The DataFrame with NaNs filled where possible.\n",
    "    \"\"\"\n",
    "    for column in df.columns:\n",
    "        # Check if the column data type is numeric (Prophet requires numeric types)\n",
    "        if column != \"Datetime\":\n",
    "            print(f\"Processing column: {column}\")\n",
    "            df[column] = fill_nans_with_prophet(df[column])\n",
    "        else:\n",
    "            print(f\"Skipping column: {column} (non-numeric data)\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling in the NaNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So First, we try to fill in the columns that can be filled using other columns\n",
    "### NIV_Outturn = - (BM_Bid_Acceptances + BM_Offer_Acceptances) \n",
    "### and \n",
    "### BSAD_Total = BSAD_Turn_Down + BSAD_Turn_Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the \"No Data Available\" by 0s in the BSAD columns where applicable\n",
    "# if all three are missing we just let them be replaced by NaNs\n",
    "\n",
    "# Replace \"No Data Available\" in \"BSAD_Turn_Up\" with 0 if \"BSAD_Total\" is equal to other column\n",
    "merged_df.loc[(merged_df[\"BSAD_Turn_Up\"].isna()) & (merged_df[\"BSAD_Total\"] == merged_df[\"BSAD_Turn_Down\"]), \"BSAD_Turn_Up\"] = 0\n",
    "\n",
    "# Replace \"No Data Available\" in \"BSAD_Turn_Down\" with 0 if \"BSAD_Total\" is equal to other column\n",
    "merged_df.loc[(merged_df[\"BSAD_Turn_Down\"].isna()) & (merged_df[\"BSAD_Total\"] == merged_df[\"BSAD_Turn_Up\"]), \"BSAD_Turn_Down\"] = 0    \n",
    "\n",
    "# Replace 'NIV_Outturn' with NaN if both 'BM_Bid_Acceptances' and 'BM_Offer_Acceptances' are NaN and 'NIV_Outturn' is 0\n",
    "merged_df.loc[(merged_df['NIV_Outturn'] == 0) & merged_df['BM_Bid_Acceptances'].isna() & merged_df['BM_Offer_Acceptances'].isna(), 'NIV_Outturn'] = np.nan\n",
    "\n",
    "# Replace 'NIV_Outturn' with the negative of the sum of 'BM_Offer_Acceptances' and 'BM_Bid_Acceptances' \n",
    "# if 'NIV_Outturn' is zero and neither of the other two columns contains NaN\n",
    "merged_df.loc[(merged_df['NIV_Outturn'] == 0) & merged_df['BM_Offer_Acceptances'].notna() & merged_df['BM_Bid_Acceptances'].notna(), 'NIV_Outturn'] = -(merged_df['BM_Offer_Acceptances'] + merged_df['BM_Bid_Acceptances'])\n",
    "\n",
    "# Extrapolate 'BM_Bid_Acceptances' with condition to set both columns to NaN if bid check fails\n",
    "bid_values = -merged_df['NIV_Outturn'] - merged_df['BM_Offer_Acceptances']\n",
    "merged_df.loc[merged_df['BM_Bid_Acceptances'].isna() & merged_df['NIV_Outturn'].notna(), 'BM_Bid_Acceptances'] = bid_values.where(bid_values <= 0)\n",
    "merged_df.loc[merged_df['BM_Bid_Acceptances'].isna(), 'BM_Offer_Acceptances'] = np.nan\n",
    "\n",
    "# Extrapolate 'BM_Offer_Acceptances' with condition to set both columns to NaN if offer check fails\n",
    "offer_values = -merged_df['NIV_Outturn'] - merged_df['BM_Bid_Acceptances']\n",
    "merged_df.loc[merged_df['BM_Offer_Acceptances'].isna() & merged_df['NIV_Outturn'].notna(), 'BM_Offer_Acceptances'] = offer_values.where(offer_values >= 0)\n",
    "merged_df.loc[merged_df['BM_Offer_Acceptances'].isna(), 'BM_Bid_Acceptances'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mark the rows where there are missing values for each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in merged_df.columns:\n",
    "    merged_df[f'{column}_missing'] = merged_df[column].isnull().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.set_index(\"Datetime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then, we fill the other NaNs using Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN detected, proceeding to fill...\n",
      "Frequencies and amplitudes calculated: <zip object at 0x0000014F68B3DC40>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:10:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:17:37 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions length: 118368, Series length: 118320\n",
      "NaNs filled in the series.\n",
      "NaN detected, proceeding to fill...\n",
      "Frequencies and amplitudes calculated: <zip object at 0x0000014F69616280>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:18:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:25:15 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions length: 118368, Series length: 118320\n",
      "NaNs filled in the series.\n",
      "NaN detected, proceeding to fill...\n",
      "Frequencies and amplitudes calculated: <zip object at 0x0000014F7C0EB380>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:26:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:39:29 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions length: 118368, Series length: 118320\n",
      "NaNs filled in the series.\n"
     ]
    }
   ],
   "source": [
    "# Process the DataFrame\n",
    "merged_df = process_dataframe(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the filled df in order to retrieve it later for the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.reset_index(inplace=True)\n",
    "merged_df.to_csv(\"merged_df_Combined.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
