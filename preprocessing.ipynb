{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the data\n",
    "### setting comprehensible col names and right types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the necessary packages\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "import pandas as pd\n",
    "from pandas.errors import PerformanceWarning\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from prophet import Prophet\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "\n",
    "# For legibility, we mute some warnings\n",
    "import warnings\n",
    "\n",
    "# Ignore FutureWarning for deprecated 'T' frequency in Prophet\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"'T' is deprecated\")\n",
    "\n",
    "# Ignore PerformanceWarning from pandas\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mix of different sources, mostly ESO\n",
    "balancing_df = pd.read_csv(\"balancing_data.csv\")\n",
    "# Demand data only for GB\n",
    "GB_demand_df = pd.read_csv(\"demand_load_data.csv\")\n",
    "# Generation data only for GB\n",
    "GB_generation_df = pd.read_csv(\"generation_data.csv\")\n",
    "# the price dataframe only concerns EPEX (only prices from there)\n",
    "EPEX_price_df = pd.read_csv(\"price_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the maximum number of consecutive NaNs filled in a column\n",
    "# As Angelica Asked\n",
    "def max_consecutive_nans_filled(df, column):\n",
    "    \"\"\"\n",
    "\n",
    "    This function calculates and returns the maximum number \n",
    "    of consecutive NaNs in a column that is to be filled\n",
    "\n",
    "    \"\"\"\n",
    "    # Identify consecutive NaNs\n",
    "    na_groups = df[column].isna().astype(int).groupby(df[column].notna().cumsum()).sum()\n",
    "    # Get the maximum number of consecutive NaNs that would be interpolated\n",
    "    max_consecutive_nans = na_groups.max()\n",
    "    nans_before = df[column].isna().sum()\n",
    "\n",
    "    print(f\"NaNs in {column}: {nans_before}\")\n",
    "    print(f\"Max consecutive NaNs filled for '{column}': {max_consecutive_nans}\")\n",
    "    return\n",
    "\n",
    "\n",
    "def fill_missing_with_prophet(df, column_name, time_column=\"Datetime\"):\n",
    "    \"\"\"\n",
    "    Use Prophet to fill missing values in a specific column of a DataFrame.\n",
    "    \"\"\"\n",
    "    # Prepare the data for Prophet\n",
    "    temp_df = df[[time_column, column_name]].rename(columns={time_column: 'ds', column_name: 'y'})\n",
    "\n",
    "    # Separate known and missing data\n",
    "    known_data = temp_df.dropna()\n",
    "\n",
    "    # Initialize and fit the Prophet model\n",
    "    model = Prophet(daily_seasonality=True, weekly_seasonality=True, yearly_seasonality=True)\n",
    "    model.fit(known_data)\n",
    "\n",
    "    # Make predictions for the full range of dates in the original data\n",
    "    future = pd.DataFrame({'ds': temp_df['ds']})\n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    # Fill missing values with Prophet predictions\n",
    "    temp_df.set_index('ds', inplace=True)\n",
    "    temp_df['yhat'] = forecast.set_index('ds')['yhat']\n",
    "    temp_df[column_name] = temp_df['y'].combine_first(temp_df['yhat'])\n",
    "\n",
    "    # Update the original DataFrame with the filled values\n",
    "    df[column_name] = temp_df[column_name].reindex(df[time_column].values).values\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final columns:\n",
      "['GMT Time' 'System_Price' 'NIV_Outturn' 'BM_Bid_Acceptances'\n",
      " 'BM_Offer_Acceptances' 'BSAD_Turn_Up' 'BSAD_Turn_Down' 'BSAD_Total'\n",
      " 'EPEX_Intraday_Volume']\n"
     ]
    }
   ],
   "source": [
    "def rename_balancing_columns(df):\n",
    "    # Define a dictionary for concise renaming\n",
    "    rename_map = {\n",
    "        'GMT Time': 'GMT Time',\n",
    "        'System Price (ESO Outturn) - GB (£/MWh)': 'System_Price',\n",
    "        'NIV Outturn (+ve long) - GB (MW)': 'NIV_Outturn',\n",
    "        'BM Bid Acceptances (total) - GB (MW)': 'BM_Bid_Acceptances',\n",
    "        'BM Offer Acceptances (total) - GB (MW)': 'BM_Offer_Acceptances',\n",
    "        'Total BSAD Volume - Turn Up - GB (MW)': 'BSAD_Turn_Up',\n",
    "        'Total BSAD Volume - Turn Down - GB (MW)': 'BSAD_Turn_Down',\n",
    "        'Total BSAD Volume - Total - GB (MW)': 'BSAD_Total',\n",
    "        'Intraday Volume (EPEX Outturn, APX, MID) - GB (MWh)': 'EPEX_Intraday_Volume'\n",
    "    }\n",
    "    \n",
    "    # Apply the renaming map\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    # Force all the non datetime columns to numeric\n",
    "    for column in df.columns:\n",
    "        if column != 'GMT Time':  # Skip the 'GMT Time' column\n",
    "            df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "    return df\n",
    "\n",
    "# Apply the function to rename columns in balancing_df\n",
    "balancing_df = rename_balancing_columns(balancing_df)\n",
    "\n",
    "print(\"Final columns:\")\n",
    "print(balancing_df.columns.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final columns:\n",
      "['GMT Time' 'Loss_of_Load_Prob' 'Total_Load' 'Demand_Outturn']\n"
     ]
    }
   ],
   "source": [
    "def rename_demand_columns(df):\n",
    "    \"\"\"\n",
    "    Rename columns for easier reference and convert non-datetime columns to numeric.\n",
    "    \"\"\"\n",
    "    # Define a dictionary for concise renaming\n",
    "    rename_map = {\n",
    "        'GMT Time': 'GMT Time',\n",
    "        'Loss of Load Probability - Latest - GB ()': 'Loss_of_Load_Prob',\n",
    "        'Actual Total Load - GB (MW)': 'Total_Load',\n",
    "        'Demand Outturn (ITSDO) - GB (MW)': 'Demand_Outturn'\n",
    "    }\n",
    "    \n",
    "    # Apply the renaming map\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    # Force all the non-datetime columns to numeric\n",
    "    for column in df.columns:\n",
    "        if column != 'GMT Time':  # Skip the 'GMT Time' column\n",
    "            df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "    return df\n",
    "\n",
    "# Apply the renaming and filling functions\n",
    "GB_demand_df = rename_demand_columns(GB_demand_df)\n",
    "\n",
    "\n",
    "print(\"Final columns:\")\n",
    "print(GB_demand_df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final columns:\n",
      "['GMT Time' 'Biomass' 'Fossil_Gas' 'Fossil_Hard_Coal' 'Fossil_Oil'\n",
      " 'Hydro_Pumped_Storage' 'Hydro_Run-of-River_and_Poundage' 'Nuclear'\n",
      " 'Solar' 'Wind_Onshore' 'Wind_Offshore']\n"
     ]
    }
   ],
   "source": [
    "def rename_columns_generation(df):\n",
    "    # Define a function to clean each column name\n",
    "    def clean_column_name(col):\n",
    "        # Extract the generation type using regex\n",
    "        match = re.search(r'Actual Aggregated Generation By Type - (.+?) - GB', col)\n",
    "        if match:\n",
    "            # Replace spaces with underscores for readability\n",
    "            return match.group(1).replace(\" \", \"_\")\n",
    "        return col  # Return the column as is if no match is found\n",
    "\n",
    "    # Rename columns using the clean_column_name function\n",
    "    df.columns = [clean_column_name(col) for col in df.columns]\n",
    "    for column in df.columns:\n",
    "        if column != 'GMT Time':  # Skip the 'GMT Time' column\n",
    "            df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the function to rename columns in generation_df\n",
    "GB_generation_df = rename_columns_generation(GB_generation_df)\n",
    "\n",
    "\n",
    "print(\"Final columns:\")\n",
    "print(GB_generation_df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final columns:\n",
      "['GMT Time' 'Day_Ahead_Price' 'Intraday_Price']\n"
     ]
    }
   ],
   "source": [
    "def rename_epex_columns(df):\n",
    "    # Define a dictionary for manual renaming based on your desired column names\n",
    "    rename_map = {\n",
    "        'GMT Time': 'GMT Time',\n",
    "        'Day Ahead Price (EPEX half-hourly, local) - GB (LC/MWh)': 'Day_Ahead_Price',\n",
    "        'Intraday Price (EPEX Outturn, APX, MID) - GB (£/MWh)': 'Intraday_Price'\n",
    "    }\n",
    "\n",
    "    # Rename columns using the dictionary\n",
    "    df = df.rename(columns=rename_map)\n",
    "    for column in df.columns:\n",
    "        if column != 'GMT Time':  # Skip the 'GMT Time' column\n",
    "            df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the function to rename columns in EPEX_price_df\n",
    "EPEX_price_df = rename_epex_columns(EPEX_price_df)\n",
    "\n",
    "\n",
    "print(\"Final columns:\")\n",
    "print(EPEX_price_df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged columns:\n",
      "['Datetime' 'System_Price' 'NIV_Outturn' 'BM_Bid_Acceptances'\n",
      " 'BM_Offer_Acceptances' 'BSAD_Turn_Up' 'BSAD_Turn_Down' 'BSAD_Total'\n",
      " 'EPEX_Intraday_Volume' 'Loss_of_Load_Prob' 'Total_Load' 'Demand_Outturn'\n",
      " 'Biomass' 'Fossil_Gas' 'Fossil_Hard_Coal' 'Fossil_Oil'\n",
      " 'Hydro_Pumped_Storage' 'Hydro_Run-of-River_and_Poundage' 'Nuclear'\n",
      " 'Solar' 'Wind_Onshore' 'Wind_Offshore' 'Day_Ahead_Price' 'Intraday_Price']\n"
     ]
    }
   ],
   "source": [
    "# Set 'GMT Time' as index for each dataframe\n",
    "balancing_df.set_index('GMT Time', inplace=True)\n",
    "GB_demand_df.set_index('GMT Time', inplace=True)\n",
    "GB_generation_df.set_index('GMT Time', inplace=True)\n",
    "EPEX_price_df.set_index('GMT Time', inplace=True)\n",
    "\n",
    "# Merge using index\n",
    "merged_df = balancing_df.join([GB_demand_df, GB_generation_df, EPEX_price_df], how='inner')\n",
    "# We put back the datetime column into the merged DF and rename it for practicality\n",
    "merged_df.reset_index(inplace=True)\n",
    "merged_df.rename(columns={'GMT Time': 'Datetime'}, inplace=True)\n",
    "\n",
    "print(\"Merged columns:\")\n",
    "print(merged_df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fft(df, variable, n_top_seasonalities, threshold_pc=0.02):\n",
    "    \"\"\"\n",
    "    Calculate significant positive frequencies and their amplitudes using Fast Fourier Transform (FFT),\n",
    "    selecting the lower of 2% of the max amplitude or the top `n` frequencies.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The input DataFrame containing the time series data.\n",
    "    - variable (str): The name of the column in `df` on which to perform FFT.\n",
    "    - n_top_seasonalities (int): The maximum number of significant frequencies to consider.\n",
    "    - threshold_pc (float): Percentage (0 < threshold_pc <= 1) of the maximum amplitude to filter significant frequencies.\n",
    "\n",
    "    Returns:\n",
    "    - zip: A generator yielding (positive frequency, amplitude) for each significant frequency.\n",
    "    \"\"\"\n",
    "    # Compute fast Fourier transform\n",
    "    price_fft = np.fft.fft(df[variable].dropna())\n",
    "\n",
    "    # Get frequencies corresponding to FFT coefficients\n",
    "    freqs = np.fft.fftfreq(len(price_fft), d=1/48)\n",
    "\n",
    "    # Calculate amplitudes\n",
    "    amplitudes = np.abs(price_fft)\n",
    "\n",
    "    # Calculate the threshold based on 2% of the max amplitude\n",
    "    threshold = threshold_pc * np.max(amplitudes)\n",
    "\n",
    "    # Filter positive frequencies with amplitudes above threshold\n",
    "    positive_indices = np.where((amplitudes > threshold) & (freqs > 0))\n",
    "    positive_freqs = freqs[positive_indices]\n",
    "    positive_amplitudes = amplitudes[positive_indices]\n",
    "\n",
    "    # Sort by amplitude and select the lower of `n_top_seasonalities` or all significant frequencies\n",
    "    sorted_indices = np.argsort(positive_amplitudes)[::-1]\n",
    "    selected_indices = sorted_indices[:min(n_top_seasonalities, len(sorted_indices))]\n",
    "\n",
    "    # Select the top frequencies and amplitudes\n",
    "    significant_freqs = positive_freqs[selected_indices]\n",
    "    significant_amplitudes = positive_amplitudes[selected_indices]\n",
    "\n",
    "    return zip(significant_freqs, significant_amplitudes)\n",
    "\n",
    "\n",
    "\n",
    "def prophet_predictions(df, variable, freq_amp):\n",
    "    \"\"\"\n",
    "    Generate predictions using Prophet with multiple seasonalities based on significant frequencies.\n",
    "\n",
    "    This function applies Prophet to model and predict a specified variable, adding custom seasonalities \n",
    "    derived from significant frequencies (e.g., daily, weekly patterns). The seasonalities are added \n",
    "    dynamically based on the frequency components identified through FFT, with Fourier orders adjusted \n",
    "    for shorter and longer periods.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The input DataFrame containing the time series data.\n",
    "    - variable (str): The name of the column in `df` to be modeled by Prophet.\n",
    "    - freq_amp (list of tuples): A list of (frequency, amplitude) pairs, where each frequency represents \n",
    "                                 a significant periodic component to be modeled as seasonality.\n",
    "\n",
    "    Returns:\n",
    "    - forecast (DataFrame): The forecasted values for the specified period, including trend and seasonal components.\n",
    "    \"\"\"\n",
    "    # Use Prophet to model_F multiple seasonalities\n",
    "    prophet_balancing_df = df.reset_index().rename(columns={'Datetime': 'ds', variable: 'y'})\n",
    "    model_F = Prophet(outlier_prior_scale=0.05)\n",
    "\n",
    "    # Adding seasonalities based on significant frequencies\n",
    "    for freq, amp in freq_amp:\n",
    "        if freq != 0:  # Ignore the DC component\n",
    "            period_in_days = 1 / freq\n",
    "            # Add seasonality to Prophet\n",
    "            seasonality_name = f\"seasonal_freq_{freq:.4f}\"\n",
    "            if period_in_days <= 1:\n",
    "                fourier_order = 5\n",
    "            elif period_in_days > 1 and period_in_days > 7:\n",
    "                fourier_order = 10\n",
    "            else:\n",
    "                fourier_order = 20\n",
    "            model_F.add_seasonality(name=seasonality_name, period=period_in_days, fourier_order=fourier_order)\n",
    "\n",
    "    # Fit the model_F\n",
    "    model_F.fit(prophet_balancing_df)\n",
    "\n",
    "    # Make future dataframe for predictions, 48 rows because we predict for next day\n",
    "    future = model_F.make_future_dataframe(periods=48, freq='30T')\n",
    "\n",
    "    forecast = model_F.predict(future)\n",
    "\n",
    "    # Plot the forecast\n",
    "    # model_F.plot(forecast)\n",
    "    # plt.show()\n",
    "    return forecast\n",
    "\n",
    "\n",
    "def t_arima(df, p, q):\n",
    "    \"\"\"\n",
    "    Fit an ARIMA model to the time series data and forecast future values.\n",
    "\n",
    "    This function applies an ARIMA model to a specified variable within a DataFrame, determining the degree \n",
    "    of differencing (d) based on stationarity tests. It then forecasts future values beyond the length of \n",
    "    the data provided, accommodating additional time steps for further predictions.\n",
    "\n",
    "    Parameters:\n",
    "    - df (Series): The input time series data.\n",
    "    - p (int): The order of the autoregressive part.\n",
    "    - q (int): The order of the moving average part.\n",
    "\n",
    "    Returns:\n",
    "    - Series: The forecasted values over the extended range, including additional time steps.\n",
    "    \"\"\"\n",
    "    # Fit ARIMA\n",
    "\n",
    "    # setting the frequency for the arima\n",
    "    # df = df.asfreq('30T')\n",
    "    \n",
    "    if adfuller(df.dropna())[1] < 0.05:\n",
    "        d = 0\n",
    "        print(\"d = 0\")\n",
    "    elif adfuller(df.diff().dropna())[1] < 0.05:\n",
    "        d = 1\n",
    "        print(\"d = 1\")\n",
    "    else:\n",
    "        d = 2\n",
    "        print(\"d = 2\")\n",
    "    arima_model = ARIMA(df.dropna(), order=(p, d, q))\n",
    "    arima_fit = arima_model.fit()\n",
    "\n",
    "    forecast = arima_fit.predict(start=0, end=len(df) - 1 + 48)  # we predict for the 48 rows after\n",
    "    return pd.Series(forecast, index=df.index)\n",
    "\n",
    "\n",
    "def metrics(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate and display error metrics for model evaluation.\n",
    "\n",
    "    This function computes standard error metrics, including Mean Absolute Error (MAE), Mean Squared Error (MSE),\n",
    "    Root Mean Squared Error (RMSE), and R-squared (R2), to evaluate the accuracy of model predictions.\n",
    "\n",
    "    Parameters:\n",
    "    - y_test (Series or array-like): The true values for the target variable in the test set.\n",
    "    - y_pred (Series or array-like): The predicted values for the target variable in the test set.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Calculate error metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Print error metrics\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"R-squared (R2):\", r2)\n",
    "    return rmse\n",
    "\n",
    "\n",
    "def ensemble_model(merged_df_2024, variable, fft_threshold, p, q):\n",
    "    \"\"\"\n",
    "    Generate an ensemble forecast by combining Prophet and ARIMA models.\n",
    "\n",
    "    Parameters:\n",
    "    - merged_df_2024 (DataFrame): The main DataFrame containing time series data to be used for forecasting.\n",
    "    - variable (str): The target variable for which predictions are to be generated.\n",
    "    - fft_threshold (float): The threshold for filtering frequencies in the Fast Fourier Transform (FFT) for Prophet.\n",
    "    - p (int): The order of the autoregressive (AR) term in the ARIMA model for the residuals.\n",
    "    - q (int): The order of the moving average (MA) term in the ARIMA model for the residuals.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A DataFrame containing the original data, Prophet predictions, ARIMA residual forecasts, \n",
    "      and the final combined forecast.\n",
    "\n",
    "    Steps:\n",
    "    1. Calculate initial predictions with Prophet:\n",
    "       - Use Prophet to generate predictions for the target variable, applying FFT with the specified threshold to \n",
    "         preprocess the time series data and isolate important frequencies.\n",
    "       - Set 'ds' as the index for Prophet predictions to align with the datetime index of the main DataFrame.\n",
    "    \n",
    "    2. Calculate residuals between actual values and Prophet predictions:\n",
    "       - Compute residuals by subtracting Prophet’s forecast from the actual values in `variable`.\n",
    "    \n",
    "    3. Fit ARIMA on the residuals:\n",
    "       - Fit an ARIMA model on the residuals, using specified AR and MA orders (`p` and `q`), to capture any \n",
    "         remaining patterns not accounted for by Prophet.\n",
    "\n",
    "    4. Combine Prophet and ARIMA forecasts:\n",
    "       - Add the ARIMA forecasted residuals back to the initial Prophet predictions to produce the final ensemble \n",
    "         forecast.\n",
    "       - Calculate the final residuals as the difference between actual values and the combined forecast to assess \n",
    "         the accuracy of the ensemble model.\n",
    "\n",
    "    Returns:\n",
    "    - The updated DataFrame with columns for Prophet's forecast, ARIMA residuals, combined forecast, and final residuals.\n",
    "    \"\"\"\n",
    "    # Step 1: Calculate predictions with Prophet\n",
    "    preds_SP = prophet_predictions(merged_df_2024, variable, calculate_fft(merged_df_2024, variable, fft_threshold))\n",
    "    # Merging the predictions of prophet to merged df\n",
    "    preds_SP = preds_SP.set_index('ds')\n",
    "    preds_SP.index.name = 'Datetime'\n",
    "\n",
    "    # Convert both indexes to datetime format\n",
    "    preds_SP.index = pd.to_datetime(preds_SP.index)\n",
    "    merged_df_2024.index = pd.to_datetime(merged_df_2024.index)\n",
    "    # Merge both dataframes for convenience\n",
    "    merged_df_SP = merged_df_2024.join(preds_SP, how='outer')\n",
    "    merged_df_SP.reset_index(inplace=True)\n",
    "\n",
    "    # Step 2: Calculate Residuals\n",
    "    # Calculate residuals as the difference between actual values and Prophet's forecast\n",
    "    merged_df_SP['residuals'] = merged_df_SP[variable] - merged_df_SP[\"yhat\"]\n",
    "\n",
    "    # Step 3: Fit ARIMA on the Residuals\n",
    "    # Using the residuals, fit an ARIMA model\n",
    "    residuals_forecast_series = t_arima(merged_df_SP['residuals'], p, q)\n",
    "\n",
    "    # Step 4: Combine the Predictions\n",
    "    # Add the ARIMA residuals forecast back to the Prophet forecast\n",
    "    merged_df_SP['combined_forecast'] = merged_df_SP['yhat'] + residuals_forecast_series\n",
    "\n",
    "    merged_df_SP['final_residuals'] = merged_df_SP[variable] - merged_df_SP['combined_forecast']\n",
    "    \n",
    "    return merged_df_SP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling in the NaNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So First, we try to fill in the columns that can be filled using other columns\n",
    "### NIV_Outturn = - (BM_Bid_Acceptances + BM_Offer_Acceptances) \n",
    "### and \n",
    "### BSAD_Total = BSAD_Turn_Down + BSAD_Turn_Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the \"No Data Available\" by 0s in the BSAD columns where applicable\n",
    "# if all three are missing we just let them be replaced by NaNs\n",
    "\n",
    "# Replace \"No Data Available\" in \"BSAD_Turn_Up\" with 0 if \"BSAD_Total\" is equal to other column\n",
    "merged_df.loc[(merged_df[\"BSAD_Turn_Up\"].isna()) & (merged_df[\"BSAD_Total\"] == merged_df[\"BSAD_Turn_Down\"]), \"BSAD_Turn_Up\"] = 0\n",
    "\n",
    "# Replace \"No Data Available\" in \"BSAD_Turn_Down\" with 0 if \"BSAD_Total\" is equal to other column\n",
    "merged_df.loc[(merged_df[\"BSAD_Turn_Down\"].isna()) & (merged_df[\"BSAD_Total\"] == merged_df[\"BSAD_Turn_Up\"]), \"BSAD_Turn_Down\"] = 0    \n",
    "\n",
    "# Replace 'NIV_Outturn' with NaN if both 'BM_Bid_Acceptances' and 'BM_Offer_Acceptances' are NaN and 'NIV_Outturn' is 0\n",
    "merged_df.loc[(merged_df['NIV_Outturn'] == 0) & merged_df['BM_Bid_Acceptances'].isna() & merged_df['BM_Offer_Acceptances'].isna(), 'NIV_Outturn'] = np.nan\n",
    "\n",
    "# Replace 'NIV_Outturn' with the negative of the sum of 'BM_Offer_Acceptances' and 'BM_Bid_Acceptances' \n",
    "# if 'NIV_Outturn' is zero and neither of the other two columns contains NaN\n",
    "merged_df.loc[(merged_df['NIV_Outturn'] == 0) & merged_df['BM_Offer_Acceptances'].notna() & merged_df['BM_Bid_Acceptances'].notna(), 'NIV_Outturn'] = -(merged_df['BM_Offer_Acceptances'] + merged_df['BM_Bid_Acceptances'])\n",
    "\n",
    "# Extrapolate 'BM_Bid_Acceptances' with condition to set both columns to NaN if bid check fails\n",
    "bid_values = -merged_df['NIV_Outturn'] - merged_df['BM_Offer_Acceptances']\n",
    "merged_df.loc[merged_df['BM_Bid_Acceptances'].isna() & merged_df['NIV_Outturn'].notna(), 'BM_Bid_Acceptances'] = bid_values.where(bid_values <= 0)\n",
    "merged_df.loc[merged_df['BM_Bid_Acceptances'].isna(), 'BM_Offer_Acceptances'] = np.nan\n",
    "\n",
    "# Extrapolate 'BM_Offer_Acceptances' with condition to set both columns to NaN if offer check fails\n",
    "offer_values = -merged_df['NIV_Outturn'] - merged_df['BM_Bid_Acceptances']\n",
    "merged_df.loc[merged_df['BM_Offer_Acceptances'].isna() & merged_df['NIV_Outturn'].notna(), 'BM_Offer_Acceptances'] = offer_values.where(offer_values >= 0)\n",
    "merged_df.loc[merged_df['BM_Offer_Acceptances'].isna(), 'BM_Bid_Acceptances'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markt the rows where there are missing values for each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in merged_df.columns:\n",
    "    merged_df[f'{column}_missing'] = merged_df[column].isnull().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then, we fill the other NaNs using Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs in System_Price: 45\n",
      "Max consecutive NaNs filled for 'System_Price': 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:33:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:35:42 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in 'System_Price' have been filled using Prophet.\n",
      "\n",
      "NaNs in NIV_Outturn: 23\n",
      "Max consecutive NaNs filled for 'NIV_Outturn': 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:36:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:36:50 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in 'NIV_Outturn' have been filled using Prophet.\n",
      "\n",
      "NaNs in BM_Bid_Acceptances: 420\n",
      "Max consecutive NaNs filled for 'BM_Bid_Acceptances': 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:37:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:38:34 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in 'BM_Bid_Acceptances' have been filled using Prophet.\n",
      "\n",
      "NaNs in BM_Offer_Acceptances: 420\n",
      "Max consecutive NaNs filled for 'BM_Offer_Acceptances': 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:39:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:40:39 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in 'BM_Offer_Acceptances' have been filled using Prophet.\n",
      "\n",
      "NaNs in EPEX_Intraday_Volume: 640\n",
      "Max consecutive NaNs filled for 'EPEX_Intraday_Volume': 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:41:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:42:07 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in 'EPEX_Intraday_Volume' have been filled using Prophet.\n",
      "\n",
      "NaNs in Loss_of_Load_Prob: 556\n",
      "Max consecutive NaNs filled for 'Loss_of_Load_Prob': 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:42:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:42:44 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in 'Loss_of_Load_Prob' have been filled using Prophet.\n",
      "\n",
      "NaNs in Total_Load: 2091\n",
      "Max consecutive NaNs filled for 'Total_Load': 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:43:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:44:13 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in 'Total_Load' have been filled using Prophet.\n",
      "\n",
      "NaNs in Demand_Outturn: 571\n",
      "Max consecutive NaNs filled for 'Demand_Outturn': 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:44:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:46:27 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in 'Demand_Outturn' have been filled using Prophet.\n",
      "\n",
      "NaNs in Biomass: 2195\n",
      "Max consecutive NaNs filled for 'Biomass': 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:46:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:49:36 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in 'Biomass' have been filled using Prophet.\n",
      "\n",
      "NaNs in Fossil_Gas: 2195\n",
      "Max consecutive NaNs filled for 'Fossil_Gas': 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:50:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:52:51 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in 'Fossil_Gas' have been filled using Prophet.\n",
      "\n",
      "NaNs in Fossil_Hard_Coal: 2195\n",
      "Max consecutive NaNs filled for 'Fossil_Hard_Coal': 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:53:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:54:54 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in 'Fossil_Hard_Coal' have been filled using Prophet.\n",
      "\n",
      "NaNs in Fossil_Oil: 2195\n",
      "Max consecutive NaNs filled for 'Fossil_Oil': 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:55:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:55:22 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in 'Fossil_Oil' have been filled using Prophet.\n",
      "\n",
      "NaNs in Hydro_Pumped_Storage: 2195\n",
      "Max consecutive NaNs filled for 'Hydro_Pumped_Storage': 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:55:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:57:01 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in 'Hydro_Pumped_Storage' have been filled using Prophet.\n",
      "\n",
      "NaNs in Hydro_Run-of-River_and_Poundage: 2195\n",
      "Max consecutive NaNs filled for 'Hydro_Run-of-River_and_Poundage': 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:57:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:59:26 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in 'Hydro_Run-of-River_and_Poundage' have been filled using Prophet.\n",
      "\n",
      "NaNs in Nuclear: 2195\n",
      "Max consecutive NaNs filled for 'Nuclear': 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:59:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:02:33 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in 'Nuclear' have been filled using Prophet.\n",
      "\n",
      "NaNs in Solar: 2195\n",
      "Max consecutive NaNs filled for 'Solar': 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:03:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:03:38 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in 'Solar' have been filled using Prophet.\n",
      "\n",
      "NaNs in Wind_Onshore: 2195\n",
      "Max consecutive NaNs filled for 'Wind_Onshore': 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:04:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:05:49 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in 'Wind_Onshore' have been filled using Prophet.\n",
      "\n",
      "NaNs in Wind_Offshore: 2195\n",
      "Max consecutive NaNs filled for 'Wind_Offshore': 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:06:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:08:23 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in 'Wind_Offshore' have been filled using Prophet.\n",
      "\n",
      "NaNs in Day_Ahead_Price: 64\n",
      "Max consecutive NaNs filled for 'Day_Ahead_Price': 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:08:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:10:48 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in 'Day_Ahead_Price' have been filled using Prophet.\n",
      "\n",
      "NaNs in Intraday_Price: 640\n",
      "Max consecutive NaNs filled for 'Intraday_Price': 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:11:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:13:04 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in 'Intraday_Price' have been filled using Prophet.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply Prophet-based filling for each column with missing data, except 'GMT Time'\n",
    "for column in merged_df.columns:\n",
    "    if column != 'Datetime':  # time col is never empty and anyway not numerical\n",
    "        if column not in [\"BSAD_Turn_Up\", \"BSAD_Turn_Down\", \"BSAD_Total\"]:  # there are simply too many Nans in here to fill (we would just be training on our own created data)\n",
    "            max_consecutive_nans_filled(merged_df, column)  # print information about the NaNs before filling, including max consecutive\n",
    "            merged_df = fill_missing_with_prophet(merged_df, column)\n",
    "            print(f\"Missing values in '{column}' have been filled using Prophet.\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the filled df in case we need to retrieve it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to save the merged to csv for easier retrieval\n",
    "merged_df.to_csv(\"merged_df_filled.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data for the next 48 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv(\"merged_df_filled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for predictions for different variables,we use different lenghts for training the ensemble model\n",
    "merged_df.set_index('Datetime', inplace=True)\n",
    "merged_df.index = pd.to_datetime(merged_df.index)\n",
    "\n",
    "merged_df_2024 = merged_df[(merged_df.index >= '2023-10-01') & (merged_df.index < '2025-01-01')]  # we only look at the last 365 (for System Price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating rows to append generated predictions\n",
    "# Generate a date range that starts after the last date in merged_df_2024\n",
    "date_range_df_temp = pd.DataFrame({'value': [None] * 48}, index=pd.date_range(start=merged_df.index[-1] + pd.Timedelta(minutes=30), periods=48, freq='30T'))\n",
    "# Concatenate without resetting the index, preserving the datetime index\n",
    "df_with_preds = pd.concat([merged_df, date_range_df_temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21:45:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:53:05 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21:57:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:04:05 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:07:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:15:55 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:19:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:23:55 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:836: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "  return get_prediction_index(\n",
      "c:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:836: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n",
      "  return get_prediction_index(\n",
      "22:36:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:42:03 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:836: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "  return get_prediction_index(\n",
      "c:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:836: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n",
      "  return get_prediction_index(\n",
      "22:45:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:52:56 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:836: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "  return get_prediction_index(\n",
      "c:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:836: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n",
      "  return get_prediction_index(\n",
      "22:55:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:57:47 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py:966: UserWarning: Non-stationary starting autoregressive parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-stationary starting autoregressive parameters'\n",
      "c:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-invertible starting MA parameters found.'\n",
      "23:01:44 - cmdstanpy - INFO - Chain [1] start processing\n",
      "23:04:49 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:06:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "23:10:26 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:12:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "23:19:10 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:21:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "23:35:13 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:37:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "23:47:40 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:51:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "23:56:09 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:58:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "00:00:07 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:02:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "00:05:14 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:08:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "00:14:09 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:16:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "00:35:42 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py:966: UserWarning: Non-stationary starting autoregressive parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-stationary starting autoregressive parameters'\n",
      "c:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-invertible starting MA parameters found.'\n",
      "00:39:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "00:51:49 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:55:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "01:09:44 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:14:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "01:23:10 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:26:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "01:32:52 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:36:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "01:48:22 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d = 0\n"
     ]
    }
   ],
   "source": [
    "# Making the predictions for next 48 observations using Prophet + ARIMA\n",
    "\n",
    "# Apply Prophet (using FFT inputs) + ARIMA for each column in order to create predictions for the next 48 observations\n",
    "for column in merged_df.columns:\n",
    "    if column != \"System_Price\":  # For System Price, there is a different procedure\n",
    "        # Creating prediction using Prophet and ARIMA for each column for next 48\n",
    "        prophet_arima_preds = ensemble_model(merged_df, column, 45, 2, 3)\n",
    "        # Appending the predictions to the end of the df with predictions\n",
    "        df_with_preds.loc[df_with_preds.index[-48:], column] = prophet_arima_preds['combined_forecast'].iloc[-48:].values\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:49:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "01:50:25 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d = 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (17616) does not match length of index (118368)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m prophet_arima_preds_SP \u001b[38;5;241m=\u001b[39m ensemble_model(merged_df_2024, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSystem_Price\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m45\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# We append the predictions for System Price\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mdf_with_preds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcombined_predictions_SP\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m prophet_arima_preds_SP[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined_forecast\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Calculate the residuals from the SP prediction (for XGBoost input later)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m df_with_preds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombined_predictions_SP_residuals\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df_with_preds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSystem_Price\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m-\u001b[39m df_with_preds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombined_predictions_SP\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4311\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   4309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4310\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[1;32m-> 4311\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4524\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4515\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4516\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   4517\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4522\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   4523\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4524\u001b[0m     value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4527\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m   4528\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   4529\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[0;32m   4530\u001b[0m     ):\n\u001b[0;32m   4531\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   4532\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:5266\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   5263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m   5265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m-> 5266\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5267\u001b[0m arr \u001b[38;5;241m=\u001b[39m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   5268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   5269\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(value, Index)\n\u001b[0;32m   5270\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5273\u001b[0m     \u001b[38;5;66;03m# TODO: Remove kludge in sanitize_array for string mode when enforcing\u001b[39;00m\n\u001b[0;32m   5274\u001b[0m     \u001b[38;5;66;03m# this deprecation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\pandas\\core\\common.py:573\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m--> 573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    577\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    578\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (17616) does not match length of index (118368)"
     ]
    }
   ],
   "source": [
    "# For System Price there is another system, we keep more information\n",
    "# Prediction using Prophet (using FFT inputs) + ARIMA for System_Price\n",
    "prophet_arima_preds_SP = ensemble_model(merged_df_2024, 'System_Price', 45, 2, 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We append the predictions for System Price\n",
    "df_with_preds.loc[df_with_preds.index[-17616:], \"combined_predictions_SP\"] = prophet_arima_preds_SP['combined_forecast'].values\n",
    "# Calculate the residuals from the SP prediction (for XGBoost input later)\n",
    "df_with_preds.loc[df_with_preds.index[-17616:], \"combined_predictions_SP_residuals\"] = df_with_preds[\"System_Price\"] - df_with_preds[\"combined_predictions_SP\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (17616) does not match length of index (118368)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m filtered_columns \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m prophet_arima_preds_SP\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseasonal_freq_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m col \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m col\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_lower\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_upper\u001b[39m\u001b[38;5;124m\"\u001b[39m))] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdaily\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweekly\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditive_terms\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrend\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m## Add the filtered columns from prophet_arima_preds_SP to df_with_preds\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m \u001b[43mdf_with_preds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfiltered_columns\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m prophet_arima_preds_SP[filtered_columns]\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4299\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4297\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_frame(key, value)\n\u001b[0;32m   4298\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, (Series, np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mlist\u001b[39m, Index)):\n\u001b[1;32m-> 4299\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4300\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, DataFrame):\n\u001b[0;32m   4301\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_item_frame_value(key, value)\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4350\u001b[0m, in \u001b[0;36mDataFrame._setitem_array\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4347\u001b[0m         \u001b[38;5;28mself\u001b[39m[col] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m   4349\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m-> 4350\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iset_not_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4352\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(value) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4353\u001b[0m     \u001b[38;5;66;03m# list of lists\u001b[39;00m\n\u001b[0;32m   4354\u001b[0m     value \u001b[38;5;241m=\u001b[39m DataFrame(value)\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4380\u001b[0m, in \u001b[0;36mDataFrame._iset_not_inplace\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4377\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns must be same length as key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4379\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(key):\n\u001b[1;32m-> 4380\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m igetitem(value, i)\n\u001b[0;32m   4382\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4383\u001b[0m     ilocs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_indexer_non_unique(key)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4311\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   4309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4310\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[1;32m-> 4311\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4524\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4515\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4516\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   4517\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4522\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   4523\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4524\u001b[0m     value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4527\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m   4528\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   4529\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[0;32m   4530\u001b[0m     ):\n\u001b[0;32m   4531\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   4532\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:5266\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   5263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m   5265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m-> 5266\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5267\u001b[0m arr \u001b[38;5;241m=\u001b[39m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   5268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   5269\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(value, Index)\n\u001b[0;32m   5270\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5273\u001b[0m     \u001b[38;5;66;03m# TODO: Remove kludge in sanitize_array for string mode when enforcing\u001b[39;00m\n\u001b[0;32m   5274\u001b[0m     \u001b[38;5;66;03m# this deprecation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\pandas\\core\\common.py:573\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m--> 573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    577\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    578\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (17616) does not match length of index (118368)"
     ]
    }
   ],
   "source": [
    "# Variable that tracks the difference between total load and demand\n",
    "df_with_preds[\"Load-Demand\"] = df_with_preds[\"Total_Load\"] - df_with_preds[\"Demand_Outturn\"]\n",
    "\n",
    "# Recalculate the LoLP using the Normal CDF (we only use the difference btw Total load and demand outturn)\n",
    "# For instructions, we consulted:\n",
    "# approximation since we do not have some of the information\n",
    "# https://bscdocs.elexon.co.uk/category-3-documents/loss-of-load-probability-calculation-methodolgy-statement\n",
    "df_with_preds['LoLP'] = 1 - norm.cdf(df_with_preds[\"Load-Demand\"], loc=0, scale=np.sqrt(700))\n",
    "# Calculate the LoLP lag 1 as proxy for prediction of not enough electricity for next day since the load and demand are super autocorellated\n",
    "df_with_preds['LoLP_lag1'] = df_with_preds['LoLP'].shift(1).copy()\n",
    "\n",
    "\n",
    "# Feature engineering to create wind+solar variable, ignoring NaNs (if there is NaN in one of them, the sum is not NaN)\n",
    "df_with_preds[\"Wind_Solar\"] = df_with_preds[[\"Solar\", \"Wind_Onshore\", \"Wind_Offshore\"]].sum(axis=1, skipna=True)\n",
    "# Sum all columns except 'GMT Time', ignoring NaNs\n",
    "df_with_preds['Total_Generation'] = df_with_preds[['Biomass', 'Fossil_Gas', 'Fossil_Hard_Coal', 'Fossil_Oil',\n",
    "                                                    'Hydro_Pumped_Storage', 'Hydro_Run-of-River_and_Poundage', 'Nuclear',\n",
    "                                                    'Solar', 'Wind_Onshore', 'Wind_Offshore']].sum(axis=1, skipna=True)\n",
    "\n",
    "\n",
    "# Total_Load = Total Generation + Exports - Imports - Stored Energy\n",
    "# So we create a column that is the difference between exports, imports and stored energy\n",
    "df_with_preds[\"Exports-Imports-Stored\"] = df_with_preds[\"Total_Load\"] - df_with_preds[\"Total_Generation\"]\n",
    "df_with_preds[\"Generation-Demand\"] = df_with_preds[\"Total_Generation\"] - df_with_preds[\"Demand_Outturn\"]\n",
    "\n",
    "# Recalculate the LoLP using Generation, using the Normal CDF (we only use the difference btw Total Generation and demand outturn)\n",
    "# For instructions, we consulted:\n",
    "# https://bscdocs.elexon.co.uk/category-3-documents/loss-of-load-probability-calculation-methodolgy-statement\n",
    "df_with_preds['LoLP_Gen'] = 1 - norm.cdf(df_with_preds[\"Total_Generation\"] - df_with_preds[\"Demand_Outturn\"], loc=0, scale=np.sqrt(700))\n",
    "# Calculate the LoLP lag 1 as proxy for prediction of not enough electricity for next day since the load and demand are super autocorellated\n",
    "df_with_preds['LoLP_Gen_lag1'] = df_with_preds['LoLP_Gen'].shift(1).copy()\n",
    "\n",
    "\n",
    "# Caolumn with Day Ahead Price but lag 48, since they are predictions for next day\n",
    "df_with_preds['Day_Ahead_Price_lag48'] = df_with_preds['Day_Ahead_Price'].shift(48).copy()\n",
    "\n",
    "\n",
    "# We want to keep some of the columns from the Prophet prediction of SP\n",
    "## Filter columns that contain \"seasonal_freq_\" but do not end with \"_lower\" or \"_upper\"\n",
    "filtered_columns = [col for col in prophet_arima_preds_SP.columns if \"seasonal_freq_\" in col and not col.endswith((\"_lower\", \"_upper\"))] + [\"daily\", \"weekly\",\"additive_terms\", \"trend\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add the filtered columns from prophet_arima_preds_SP to df_with_preds\n",
    "df_with_preds.loc[df_with_preds.index[-17616:], filtered_columns] = prophet_arima_preds_SP[filtered_columns].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, we save the df with predictions for all columns to a csv for easier retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_preds.reset_index(inplace=True)\n",
    "# In order to save the merged to csv for easier retrieval\n",
    "df_with_preds.to_csv(\"merged_df_with_preds.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'index                                0.000000\\nSystem_Price                         0.000406\\nNIV_Outturn                          0.000000\\nBM_Bid_Acceptances                   0.000000\\nBM_Offer_Acceptances                 0.000000\\nBSAD_Turn_Up                         0.312998\\nBSAD_Turn_Down                       0.312998\\nBSAD_Total                           0.312998\\nEPEX_Intraday_Volume                 0.000000\\nLoss_of_Load_Prob                    0.000000\\nTotal_Load                           0.000000\\nDemand_Outturn                       0.000000\\nBiomass                              0.000000\\nFossil_Gas                           0.000000\\nFossil_Hard_Coal                     0.000000\\nFossil_Oil                           0.000000\\nHydro_Pumped_Storage                 0.000000\\nHydro_Run-of-River_and_Poundage      0.000000\\nNuclear                              0.000000\\nSolar                                0.000000\\nWind_Onshore                         0.000000\\nWind_Offshore                        0.000000\\nDay_Ahead_Price                      0.000000\\nIntraday_Price                       0.000000\\nvalue                                1.000000\\ncombined_predictions_SP              0.851176\\ncombined_predictions_SP_residuals    0.851582\\nLoad-Demand                          0.000000\\nLoLP                                 0.000000\\nLoLP_lag1                            0.000008\\nWind_Solar                           0.000000\\nTotal_Generation                     0.000000\\nExports-Imports-Stored               0.000000\\nGeneration-Demand                    0.000000\\nLoLP_Gen                             0.000000\\nLoLP_Gen_lag1                        0.000008\\nDay_Ahead_Price_lag48                0.000406\\nseasonal_freq_0.0027                 0.851176\\nseasonal_freq_0.0055                 0.851176\\nseasonal_freq_0.0082                 0.851176\\nseasonal_freq_0.0109                 0.851176\\nseasonal_freq_0.0164                 0.851176\\nseasonal_freq_0.0191                 0.851176\\nseasonal_freq_0.0219                 0.851176\\nseasonal_freq_0.0246                 0.851176\\nseasonal_freq_0.0273                 0.851176\\nseasonal_freq_0.0301                 0.851176\\nseasonal_freq_0.0355                 0.851176\\nseasonal_freq_0.0383                 0.851176\\nseasonal_freq_0.0410                 0.851176\\nseasonal_freq_0.0437                 0.851176\\nseasonal_freq_0.0519                 0.851176\\nseasonal_freq_0.0546                 0.851176\\nseasonal_freq_0.0574                 0.851176\\nseasonal_freq_0.0656                 0.851176\\nseasonal_freq_0.0765                 0.851176\\nseasonal_freq_0.0820                 0.851176\\nseasonal_freq_0.0847                 0.851176\\nseasonal_freq_0.1038                 0.851176\\nseasonal_freq_0.1421                 0.851176\\nseasonal_freq_0.1448                 0.851176\\nseasonal_freq_0.1694                 0.851176\\nseasonal_freq_0.1885                 0.851176\\nseasonal_freq_0.1995                 0.851176\\nseasonal_freq_0.2459                 0.851176\\nseasonal_freq_0.2568                 0.851176\\nseasonal_freq_0.2678                 0.851176\\nseasonal_freq_0.3333                 0.851176\\nseasonal_freq_0.3497                 0.851176\\nseasonal_freq_0.3525                 0.851176\\nseasonal_freq_0.4290                 0.851176\\nseasonal_freq_0.4454                 0.851176\\nseasonal_freq_0.5301                 0.851176\\nseasonal_freq_0.6366                 0.851176\\nseasonal_freq_0.6393                 0.851176\\nseasonal_freq_0.9973                 0.851176\\nseasonal_freq_1.0000                 0.851176\\nseasonal_freq_1.0027                 0.851176\\nseasonal_freq_2.0000                 0.851176\\nseasonal_freq_2.0027                 0.851176\\nseasonal_freq_3.0000                 0.851176\\nseasonal_freq_4.0000                 0.851176\\ndaily                                0.851176\\nweekly                               0.851176\\nadditive_terms                       0.851176\\ntrend                                0.851176'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_preds.isna().mean().to_string()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
