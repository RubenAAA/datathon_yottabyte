{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the data\n",
    "### setting comprehensible col names and right types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the necessary packages\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "from arch import arch_model\n",
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import AutoARIMA\n",
    "from statsmodels.stats.diagnostic import het_arch\n",
    "import pandas as pd\n",
    "from pandas.errors import PerformanceWarning\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from prophet import Prophet\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import pmdarima as pm\n",
    "from pmdarima.arima import ndiffs, nsdiffs\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "\n",
    "# For legibility, we mute some warnings\n",
    "import warnings\n",
    "\n",
    "# Ignore FutureWarning for deprecated 'T' frequency in Prophet\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"'T' is deprecated\")\n",
    "\n",
    "# Ignore PerformanceWarning from pandas\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mix of different sources, mostly ESO\n",
    "balancing_df = pd.read_csv(\"balancing_data.csv\")\n",
    "# Demand data only for GB\n",
    "GB_demand_df = pd.read_csv(\"demand_load_data.csv\")\n",
    "# Generation data only for GB\n",
    "GB_generation_df = pd.read_csv(\"generation_data.csv\")\n",
    "# the price dataframe only concerns EPEX (only prices from there)\n",
    "EPEX_price_df = pd.read_csv(\"price_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final columns:\n",
      "['GMT Time' 'System_Price' 'NIV_Outturn' 'BM_Bid_Acceptances'\n",
      " 'BM_Offer_Acceptances' 'BSAD_Turn_Up' 'BSAD_Turn_Down' 'BSAD_Total'\n",
      " 'EPEX_Intraday_Volume']\n"
     ]
    }
   ],
   "source": [
    "def rename_balancing_columns(df):\n",
    "    # Define a dictionary for concise renaming\n",
    "    rename_map = {\n",
    "        'GMT Time': 'GMT Time',\n",
    "        'System Price (ESO Outturn) - GB (£/MWh)': 'System_Price',\n",
    "        'NIV Outturn (+ve long) - GB (MW)': 'NIV_Outturn',\n",
    "        'BM Bid Acceptances (total) - GB (MW)': 'BM_Bid_Acceptances',\n",
    "        'BM Offer Acceptances (total) - GB (MW)': 'BM_Offer_Acceptances',\n",
    "        'Total BSAD Volume - Turn Up - GB (MW)': 'BSAD_Turn_Up',\n",
    "        'Total BSAD Volume - Turn Down - GB (MW)': 'BSAD_Turn_Down',\n",
    "        'Total BSAD Volume - Total - GB (MW)': 'BSAD_Total',\n",
    "        'Intraday Volume (EPEX Outturn, APX, MID) - GB (MWh)': 'EPEX_Intraday_Volume'\n",
    "    }\n",
    "    \n",
    "    # Apply the renaming map\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    # Force all the non datetime columns to numeric\n",
    "    for column in df.columns:\n",
    "        if column != 'GMT Time':  # Skip the 'GMT Time' column\n",
    "            df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "    return df\n",
    "\n",
    "# Apply the function to rename columns in balancing_df\n",
    "balancing_df = rename_balancing_columns(balancing_df)\n",
    "\n",
    "print(\"Final columns:\")\n",
    "print(balancing_df.columns.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final columns:\n",
      "['GMT Time' 'Loss_of_Load_Prob' 'Total_Load' 'Demand_Outturn']\n"
     ]
    }
   ],
   "source": [
    "def rename_demand_columns(df):\n",
    "    \"\"\"\n",
    "    Rename columns for easier reference and convert non-datetime columns to numeric.\n",
    "    \"\"\"\n",
    "    # Define a dictionary for concise renaming\n",
    "    rename_map = {\n",
    "        'GMT Time': 'GMT Time',\n",
    "        'Loss of Load Probability - Latest - GB ()': 'Loss_of_Load_Prob',\n",
    "        'Actual Total Load - GB (MW)': 'Total_Load',\n",
    "        'Demand Outturn (ITSDO) - GB (MW)': 'Demand_Outturn'\n",
    "    }\n",
    "    \n",
    "    # Apply the renaming map\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    # Force all the non-datetime columns to numeric\n",
    "    for column in df.columns:\n",
    "        if column != 'GMT Time':  # Skip the 'GMT Time' column\n",
    "            df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "    return df\n",
    "\n",
    "# Apply the renaming and filling functions\n",
    "GB_demand_df = rename_demand_columns(GB_demand_df)\n",
    "\n",
    "\n",
    "print(\"Final columns:\")\n",
    "print(GB_demand_df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final columns:\n",
      "['GMT Time' 'Biomass' 'Fossil_Gas' 'Fossil_Hard_Coal' 'Fossil_Oil'\n",
      " 'Hydro_Pumped_Storage' 'Hydro_Run-of-River_and_Poundage' 'Nuclear'\n",
      " 'Solar' 'Wind_Onshore' 'Wind_Offshore']\n"
     ]
    }
   ],
   "source": [
    "def rename_columns_generation(df):\n",
    "    # Define a function to clean each column name\n",
    "    def clean_column_name(col):\n",
    "        # Extract the generation type using regex\n",
    "        match = re.search(r'Actual Aggregated Generation By Type - (.+?) - GB', col)\n",
    "        if match:\n",
    "            # Replace spaces with underscores for readability\n",
    "            return match.group(1).replace(\" \", \"_\")\n",
    "        return col  # Return the column as is if no match is found\n",
    "\n",
    "    # Rename columns using the clean_column_name function\n",
    "    df.columns = [clean_column_name(col) for col in df.columns]\n",
    "    for column in df.columns:\n",
    "        if column != 'GMT Time':  # Skip the 'GMT Time' column\n",
    "            df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the function to rename columns in generation_df\n",
    "GB_generation_df = rename_columns_generation(GB_generation_df)\n",
    "\n",
    "\n",
    "print(\"Final columns:\")\n",
    "print(GB_generation_df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final columns:\n",
      "['GMT Time' 'Day_Ahead_Price' 'Intraday_Price']\n"
     ]
    }
   ],
   "source": [
    "def rename_epex_columns(df):\n",
    "    # Define a dictionary for manual renaming based on your desired column names\n",
    "    rename_map = {\n",
    "        'GMT Time': 'GMT Time',\n",
    "        'Day Ahead Price (EPEX half-hourly, local) - GB (LC/MWh)': 'Day_Ahead_Price',\n",
    "        'Intraday Price (EPEX Outturn, APX, MID) - GB (£/MWh)': 'Intraday_Price'\n",
    "    }\n",
    "\n",
    "    # Rename columns using the dictionary\n",
    "    df = df.rename(columns=rename_map)\n",
    "    for column in df.columns:\n",
    "        if column != 'GMT Time':  # Skip the 'GMT Time' column\n",
    "            df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the function to rename columns in EPEX_price_df\n",
    "EPEX_price_df = rename_epex_columns(EPEX_price_df)\n",
    "\n",
    "\n",
    "print(\"Final columns:\")\n",
    "print(EPEX_price_df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged columns:\n",
      "['Datetime' 'System_Price' 'NIV_Outturn' 'BM_Bid_Acceptances'\n",
      " 'BM_Offer_Acceptances' 'BSAD_Turn_Up' 'BSAD_Turn_Down' 'BSAD_Total'\n",
      " 'EPEX_Intraday_Volume' 'Loss_of_Load_Prob' 'Total_Load' 'Demand_Outturn'\n",
      " 'Biomass' 'Fossil_Gas' 'Fossil_Hard_Coal' 'Fossil_Oil'\n",
      " 'Hydro_Pumped_Storage' 'Hydro_Run-of-River_and_Poundage' 'Nuclear'\n",
      " 'Solar' 'Wind_Onshore' 'Wind_Offshore' 'Day_Ahead_Price' 'Intraday_Price']\n"
     ]
    }
   ],
   "source": [
    "# Set 'GMT Time' as index for each dataframe\n",
    "balancing_df.set_index('GMT Time', inplace=True)\n",
    "GB_demand_df.set_index('GMT Time', inplace=True)\n",
    "GB_generation_df.set_index('GMT Time', inplace=True)\n",
    "EPEX_price_df.set_index('GMT Time', inplace=True)\n",
    "\n",
    "# Merge using index\n",
    "merged_df = balancing_df.join([GB_demand_df, GB_generation_df, EPEX_price_df], how='inner')\n",
    "# We put back the datetime column into the merged DF and rename it for practicality\n",
    "merged_df.reset_index(inplace=True)\n",
    "merged_df.rename(columns={'GMT Time': 'Datetime'}, inplace=True)\n",
    "\n",
    "print(\"Merged columns:\")\n",
    "print(merged_df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fft(series, n_top_seasonalities, threshold_pc=0.02):\n",
    "    \"\"\"\n",
    "    Calculate significant positive frequencies and their amplitudes using Fast Fourier Transform (FFT),\n",
    "    selecting the lower of 2% of the max amplitude or the top `n` frequencies.\n",
    "\n",
    "    Parameters:\n",
    "    - series (pd.Series): The input time series data.\n",
    "    - n_top_seasonalities (int): The maximum number of significant frequencies to consider.\n",
    "    - threshold_pc (float): Percentage (0 < threshold_pc <= 1) of the maximum amplitude to filter significant frequencies.\n",
    "\n",
    "    Returns:\n",
    "    - zip: A generator yielding (positive frequency, amplitude) for each significant frequency.\n",
    "    \"\"\"\n",
    "    # Compute fast Fourier transform\n",
    "    price_fft = np.fft.fft(series.dropna())\n",
    "\n",
    "    # Get frequencies corresponding to FFT coefficients\n",
    "    freqs = np.fft.fftfreq(len(price_fft), d=1/48)\n",
    "\n",
    "    # Calculate amplitudes\n",
    "    amplitudes = np.abs(price_fft)\n",
    "\n",
    "    # Calculate the threshold based on 2% of the max amplitude\n",
    "    threshold = threshold_pc * np.max(amplitudes)\n",
    "\n",
    "    # Filter positive frequencies with amplitudes above threshold\n",
    "    positive_indices = np.where((amplitudes > threshold) & (freqs > 0))\n",
    "    positive_freqs = freqs[positive_indices]\n",
    "    positive_amplitudes = amplitudes[positive_indices]\n",
    "\n",
    "    # Sort by amplitude and select the lower of `n_top_seasonalities` or all significant frequencies\n",
    "    sorted_indices = np.argsort(positive_amplitudes)[::-1]\n",
    "    selected_indices = sorted_indices[:min(n_top_seasonalities, len(sorted_indices))]\n",
    "\n",
    "    # Select the top frequencies and amplitudes\n",
    "    significant_freqs = positive_freqs[selected_indices]\n",
    "    significant_amplitudes = positive_amplitudes[selected_indices]\n",
    "\n",
    "    return zip(significant_freqs, significant_amplitudes)\n",
    "\n",
    "\n",
    "def prophet_predictions(series, freq_amp):\n",
    "    \"\"\"\n",
    "    Generate predictions using Prophet with multiple seasonalities based on significant frequencies.\n",
    "\n",
    "    Parameters:\n",
    "    - series (pd.Series): The input time series data.\n",
    "    - freq_amp (list of tuples): A list of (frequency, amplitude) pairs, where each frequency represents \n",
    "                                 a significant periodic component to be modeled as seasonality.\n",
    "\n",
    "    Returns:\n",
    "    - forecast (DataFrame): The forecasted values for the specified period, including trend and seasonal components.\n",
    "    \"\"\"\n",
    "    # Prepare data for Prophet\n",
    "    df = pd.DataFrame({'ds': series.index, 'y': series})\n",
    "    model = Prophet()\n",
    "\n",
    "    # Adding seasonalities based on significant frequencies\n",
    "    for freq, amp in freq_amp:\n",
    "        if freq != 0:  # Ignore the DC component\n",
    "            period_in_days = 1 / freq\n",
    "            seasonality_name = f\"seasonal_freq_{freq:.4f}\"\n",
    "            fourier_order = 5 if period_in_days <= 1 else (10 if period_in_days <= 7 else 20)\n",
    "            model.add_seasonality(name=seasonality_name, period=period_in_days, fourier_order=fourier_order)\n",
    "\n",
    "    model.fit(df)\n",
    "    future = model.make_future_dataframe(periods=48, freq='30T')\n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    return forecast.set_index('ds')['yhat']\n",
    "\n",
    "def statsforecast_arima(df):\n",
    "    \"\"\"\n",
    "    Fit an AutoARIMA model to the time series data and forecast future values.\n",
    "\n",
    "    This function uses AutoARIMA from the statsforecast package to automatically select the best ARIMA model.\n",
    "    It performs both in-sample prediction and forecasts future values beyond the length of the data provided.\n",
    "\n",
    "    Parameters:\n",
    "    - df (Series): The input time series data. The index must be a DateTimeIndex.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A DataFrame containing the in-sample predictions and forecasted values over an extended range.\n",
    "    \"\"\"\n",
    "    # Ensure the input is a pandas DataFrame with required columns\n",
    "    df = pd.DataFrame({'unique_id': 1, 'ds': df.index, 'y': df.values})\n",
    "\n",
    "    # Initialize the StatsForecast object with the AutoARIMA model\n",
    "    sf = StatsForecast( models=[AutoARIMA()], freq='30min', n_jobs=-1)\n",
    "\n",
    "    sf.fit()\n",
    "    \n",
    "    # Define the forecast horizon\n",
    "    forecast_horizon = 48  # 24 hours at 30-minute intervals\n",
    "\n",
    "    # Forecast future values\n",
    "    forecast = sf.forecast(df=df, h=forecast_horizon, fitted=True)\n",
    "    values=sf.forecast_fitted_values()\n",
    "    values.set_index('ds', inplace=True)\n",
    "    forecast.set_index('ds', inplace=True)\n",
    "    result = pd.concat([values, forecast])\n",
    "    return result[\"AutoARIMA\"]\n",
    "\n",
    "\n",
    "def ensemble_model(series, fft_threshold):\n",
    "    \"\"\"\n",
    "    Generate an ensemble forecast by combining Prophet and ARIMA models directly on a time series.\n",
    "\n",
    "    Parameters:\n",
    "    - series (pd.Series): The input time series data with DateTimeIndex.\n",
    "    - fft_threshold (float): The threshold for filtering frequencies in the Fast Fourier Transform (FFT) for Prophet.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A DataFrame containing the original data, Prophet predictions, ARIMA residual forecasts,\n",
    "      and the final combined forecast.\n",
    "    \"\"\"\n",
    "    # Step 1: Calculate predictions with Prophet\n",
    "    freq_amp_pairs = calculate_fft(series, fft_threshold)\n",
    "    preds_prophet = prophet_predictions(series, freq_amp_pairs)\n",
    "    \n",
    "    # Step 2: Calculate Residuals\n",
    "    residuals = series - preds_prophet\n",
    "\n",
    "    # Step 3: Fit ARIMA on the Residuals\n",
    "    arima_forecast = statsforecast_arima(residuals.dropna())  # Ensure non-na data for ARIMA\n",
    "    \n",
    "    # Step 4: Combine the Predictions of Prophet and ARIMA\n",
    "    combined_forecast = preds_prophet.add(arima_forecast)  # Using fill_value to handle NaNs\n",
    "\n",
    "    # Prepare the result DataFrame\n",
    "    results = pd.DataFrame({\n",
    "        'original_data': series,\n",
    "        'prophet_forecast': preds_prophet,\n",
    "        'arima_residual_forecast': arima_forecast,\n",
    "        'combined_forecast': combined_forecast\n",
    "    })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the maximum number of consecutive NaNs filled in a column\n",
    "# As Angelica Asked\n",
    "def max_consecutive_nans_filled(df, column):\n",
    "    \"\"\"\n",
    "\n",
    "    This function calculates and returns the maximum number \n",
    "    of consecutive NaNs in a column that is to be filled\n",
    "\n",
    "    \"\"\"\n",
    "    # Identify consecutive NaNs\n",
    "    na_groups = df[column].isna().astype(int).groupby(df[column].notna().cumsum()).sum()\n",
    "    # Get the maximum number of consecutive NaNs that would be interpolated\n",
    "    max_consecutive_nans = na_groups.max()\n",
    "    nans_before = df[column].isna().sum()\n",
    "\n",
    "    print(f\"NaNs in {column}: {nans_before}\")\n",
    "    print(f\"Max consecutive NaNs filled for '{column}': {max_consecutive_nans}\")\n",
    "    return\n",
    "\n",
    "def fill_nans_with_prophet(series):\n",
    "    \"\"\"\n",
    "    Fills NaNs in the original time series data using predictions from the Prophet model.\n",
    "\n",
    "    Parameters:\n",
    "    - series (pd.Series): The input time series data with potential NaNs.\n",
    "\n",
    "    Returns:\n",
    "    - pd.Series: The time series with NaNs filled using Prophet predictions.\n",
    "    \"\"\"\n",
    "    # Check if there are any NaNs to fill\n",
    "    if series.isna().any():\n",
    "        # Calculate significant frequencies for seasonal adjustments\n",
    "        freq_amp = calculate_fft(series, n_top_seasonalities=12, threshold_pc=0.02)\n",
    "        \n",
    "        # Generate predictions with Prophet\n",
    "        predictions = prophet_predictions(series, freq_amp)\n",
    "        \n",
    "        # Fill NaNs in the original series with predictions\n",
    "        filled_series = series.combine_first(predictions)\n",
    "        \n",
    "        return filled_series\n",
    "    else:\n",
    "        # Return original series if no NaNs\n",
    "        return series\n",
    "def process_dataframe(df):\n",
    "    \"\"\"\n",
    "    Iterates over each column of the DataFrame, applying Prophet-based NaN filling where applicable.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame with multiple time series columns, potentially containing NaNs.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The DataFrame with NaNs filled where possible.\n",
    "    \"\"\"\n",
    "    for column in df.columns:\n",
    "        # Check if the column data type is numeric (Prophet requires numeric types)\n",
    "        if column != \"Datetime\":\n",
    "            print(f\"Processing column: {column}\")\n",
    "            df[column] = fill_nans_with_prophet(df[column])\n",
    "        else:\n",
    "            print(f\"Skipping column: {column} (non-numeric data)\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling in the NaNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So First, we try to fill in the columns that can be filled using other columns\n",
    "### NIV_Outturn = - (BM_Bid_Acceptances + BM_Offer_Acceptances) \n",
    "### and \n",
    "### BSAD_Total = BSAD_Turn_Down + BSAD_Turn_Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the \"No Data Available\" by 0s in the BSAD columns where applicable\n",
    "# if all three are missing we just let them be replaced by NaNs\n",
    "\n",
    "# Replace \"No Data Available\" in \"BSAD_Turn_Up\" with 0 if \"BSAD_Total\" is equal to other column\n",
    "merged_df.loc[(merged_df[\"BSAD_Turn_Up\"].isna()) & (merged_df[\"BSAD_Total\"] == merged_df[\"BSAD_Turn_Down\"]), \"BSAD_Turn_Up\"] = 0\n",
    "\n",
    "# Replace \"No Data Available\" in \"BSAD_Turn_Down\" with 0 if \"BSAD_Total\" is equal to other column\n",
    "merged_df.loc[(merged_df[\"BSAD_Turn_Down\"].isna()) & (merged_df[\"BSAD_Total\"] == merged_df[\"BSAD_Turn_Up\"]), \"BSAD_Turn_Down\"] = 0    \n",
    "\n",
    "# Replace 'NIV_Outturn' with NaN if both 'BM_Bid_Acceptances' and 'BM_Offer_Acceptances' are NaN and 'NIV_Outturn' is 0\n",
    "merged_df.loc[(merged_df['NIV_Outturn'] == 0) & merged_df['BM_Bid_Acceptances'].isna() & merged_df['BM_Offer_Acceptances'].isna(), 'NIV_Outturn'] = np.nan\n",
    "\n",
    "# Replace 'NIV_Outturn' with the negative of the sum of 'BM_Offer_Acceptances' and 'BM_Bid_Acceptances' \n",
    "# if 'NIV_Outturn' is zero and neither of the other two columns contains NaN\n",
    "merged_df.loc[(merged_df['NIV_Outturn'] == 0) & merged_df['BM_Offer_Acceptances'].notna() & merged_df['BM_Bid_Acceptances'].notna(), 'NIV_Outturn'] = -(merged_df['BM_Offer_Acceptances'] + merged_df['BM_Bid_Acceptances'])\n",
    "\n",
    "# Extrapolate 'BM_Bid_Acceptances' with condition to set both columns to NaN if bid check fails\n",
    "bid_values = -merged_df['NIV_Outturn'] - merged_df['BM_Offer_Acceptances']\n",
    "merged_df.loc[merged_df['BM_Bid_Acceptances'].isna() & merged_df['NIV_Outturn'].notna(), 'BM_Bid_Acceptances'] = bid_values.where(bid_values <= 0)\n",
    "merged_df.loc[merged_df['BM_Bid_Acceptances'].isna(), 'BM_Offer_Acceptances'] = np.nan\n",
    "\n",
    "# Extrapolate 'BM_Offer_Acceptances' with condition to set both columns to NaN if offer check fails\n",
    "offer_values = -merged_df['NIV_Outturn'] - merged_df['BM_Bid_Acceptances']\n",
    "merged_df.loc[merged_df['BM_Offer_Acceptances'].isna() & merged_df['NIV_Outturn'].notna(), 'BM_Offer_Acceptances'] = offer_values.where(offer_values >= 0)\n",
    "merged_df.loc[merged_df['BM_Offer_Acceptances'].isna(), 'BM_Bid_Acceptances'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markt the rows where there are missing values for each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in merged_df.columns:\n",
    "    merged_df[f'{column}_missing'] = merged_df[column].isnull().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.set_index(\"Datetime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then, we fill the other NaNs using Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing column: System_Price\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:47:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:56:09 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing column: NIV_Outturn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:57:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:59:57 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing column: BM_Bid_Acceptances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:01:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:06:52 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing column: BM_Offer_Acceptances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:08:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:23:22 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing column: BSAD_Turn_Up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:24:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:26:29 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing column: BSAD_Turn_Down\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:27:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:30:56 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing column: BSAD_Total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:32:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:35:17 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing column: EPEX_Intraday_Volume\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:36:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:38:05 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing column: Loss_of_Load_Prob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:39:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:39:43 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing column: Total_Load\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:40:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:45:06 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing column: Demand_Outturn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:46:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:51:46 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing column: Biomass\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:53:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:57:11 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing column: Fossil_Gas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:58:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:02:12 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing column: Fossil_Hard_Coal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:03:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:19:59 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing column: Fossil_Oil\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:22:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:23:57 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing column: Hydro_Pumped_Storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:24:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:28:35 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing column: Hydro_Run-of-River_and_Poundage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:30:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:42:47 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing column: Nuclear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:45:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:08:08 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing column: Solar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:09:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:14:14 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing column: Wind_Onshore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:16:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:28:27 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing column: Wind_Offshore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:30:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:37:26 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing column: Day_Ahead_Price\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:39:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:45:48 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing column: Intraday_Price\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:47:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:54:17 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing column: Datetime_missing\n",
      "Processing column: System_Price_missing\n",
      "Processing column: NIV_Outturn_missing\n",
      "Processing column: BM_Bid_Acceptances_missing\n",
      "Processing column: BM_Offer_Acceptances_missing\n",
      "Processing column: BSAD_Turn_Up_missing\n",
      "Processing column: BSAD_Turn_Down_missing\n",
      "Processing column: BSAD_Total_missing\n",
      "Processing column: EPEX_Intraday_Volume_missing\n",
      "Processing column: Loss_of_Load_Prob_missing\n",
      "Processing column: Total_Load_missing\n",
      "Processing column: Demand_Outturn_missing\n",
      "Processing column: Biomass_missing\n",
      "Processing column: Fossil_Gas_missing\n",
      "Processing column: Fossil_Hard_Coal_missing\n",
      "Processing column: Fossil_Oil_missing\n",
      "Processing column: Hydro_Pumped_Storage_missing\n",
      "Processing column: Hydro_Run-of-River_and_Poundage_missing\n",
      "Processing column: Nuclear_missing\n",
      "Processing column: Solar_missing\n",
      "Processing column: Wind_Onshore_missing\n",
      "Processing column: Wind_Offshore_missing\n",
      "Processing column: Day_Ahead_Price_missing\n",
      "Processing column: Intraday_Price_missing\n",
      "                     System_Price  NIV_Outturn  BM_Bid_Acceptances  \\\n",
      "Datetime                                                             \n",
      "2018-01-01T00:00:00         55.94       -77.05            -1833.86   \n",
      "2018-01-01T00:30:00         55.94      -334.76            -1443.78   \n",
      "2018-01-01T01:00:00         62.94      -219.78            -1580.12   \n",
      "2018-01-01T01:30:00         31.00       286.63            -1699.05   \n",
      "2018-01-01T02:00:00         60.81      -141.41            -1413.27   \n",
      "\n",
      "                     BM_Offer_Acceptances  BSAD_Turn_Up  BSAD_Turn_Down  \\\n",
      "Datetime                                                                  \n",
      "2018-01-01T00:00:00               1910.98        1104.0          -900.0   \n",
      "2018-01-01T00:30:00               1778.09        1104.0          -900.0   \n",
      "2018-01-01T01:00:00               1799.90        1104.0          -900.0   \n",
      "2018-01-01T01:30:00               1413.04        1104.0          -900.0   \n",
      "2018-01-01T02:00:00               1554.31        1104.0         -1050.0   \n",
      "\n",
      "                     BSAD_Total  EPEX_Intraday_Volume  Loss_of_Load_Prob  \\\n",
      "Datetime                                                                   \n",
      "2018-01-01T00:00:00       204.0                781.35                0.0   \n",
      "2018-01-01T00:30:00       204.0                655.40                0.0   \n",
      "2018-01-01T01:00:00       204.0                821.50                0.0   \n",
      "2018-01-01T01:30:00       204.0                815.20                0.0   \n",
      "2018-01-01T02:00:00        54.0                709.05                0.0   \n",
      "\n",
      "                     Total_Load  ...  Fossil_Hard_Coal_missing  \\\n",
      "Datetime                         ...                             \n",
      "2018-01-01T00:00:00     30303.0  ...                         0   \n",
      "2018-01-01T00:30:00     31096.0  ...                         0   \n",
      "2018-01-01T01:00:00     30599.0  ...                         0   \n",
      "2018-01-01T01:30:00     29402.0  ...                         0   \n",
      "2018-01-01T02:00:00     28096.0  ...                         0   \n",
      "\n",
      "                     Fossil_Oil_missing  Hydro_Pumped_Storage_missing  \\\n",
      "Datetime                                                                \n",
      "2018-01-01T00:00:00                   0                             0   \n",
      "2018-01-01T00:30:00                   0                             0   \n",
      "2018-01-01T01:00:00                   0                             0   \n",
      "2018-01-01T01:30:00                   0                             0   \n",
      "2018-01-01T02:00:00                   0                             0   \n",
      "\n",
      "                     Hydro_Run-of-River_and_Poundage_missing  Nuclear_missing  \\\n",
      "Datetime                                                                        \n",
      "2018-01-01T00:00:00                                        0                0   \n",
      "2018-01-01T00:30:00                                        0                0   \n",
      "2018-01-01T01:00:00                                        0                0   \n",
      "2018-01-01T01:30:00                                        0                0   \n",
      "2018-01-01T02:00:00                                        0                0   \n",
      "\n",
      "                     Solar_missing  Wind_Onshore_missing  \\\n",
      "Datetime                                                   \n",
      "2018-01-01T00:00:00              0                     0   \n",
      "2018-01-01T00:30:00              0                     0   \n",
      "2018-01-01T01:00:00              0                     0   \n",
      "2018-01-01T01:30:00              0                     0   \n",
      "2018-01-01T02:00:00              0                     0   \n",
      "\n",
      "                     Wind_Offshore_missing  Day_Ahead_Price_missing  \\\n",
      "Datetime                                                              \n",
      "2018-01-01T00:00:00                      0                        0   \n",
      "2018-01-01T00:30:00                      0                        0   \n",
      "2018-01-01T01:00:00                      0                        0   \n",
      "2018-01-01T01:30:00                      0                        0   \n",
      "2018-01-01T02:00:00                      0                        0   \n",
      "\n",
      "                     Intraday_Price_missing  \n",
      "Datetime                                     \n",
      "2018-01-01T00:00:00                       0  \n",
      "2018-01-01T00:30:00                       0  \n",
      "2018-01-01T01:00:00                       0  \n",
      "2018-01-01T01:30:00                       0  \n",
      "2018-01-01T02:00:00                       0  \n",
      "\n",
      "[5 rows x 47 columns]\n"
     ]
    }
   ],
   "source": [
    "# Process the DataFrame\n",
    "processed_df = process_dataframe(merged_df[:])\n",
    "print(processed_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the filled df in case we need to retrieve it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to save the merged to csv for easier retrieval\n",
    "processed_df.reset_index(inplace=True)\n",
    "processed_df.to_csv(\"merged_df_prophet_filled.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOP HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data for the next 48 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv(\"merged_df_prophet_filled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for predictions for different variables,we use different lenghts for training the ensemble model\n",
    "merged_df.set_index('Datetime', inplace=True)\n",
    "merged_df.index = pd.to_datetime(merged_df.index)\n",
    "\n",
    "merged_df_2024 = merged_df[(merged_df.index >= '2023-10-01') & (merged_df.index < '2025-01-01')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating rows to append generated predictions\n",
    "# Generate a date range that starts after the last date in merged_df_2024\n",
    "date_range_df_temp = pd.DataFrame({'value': [None] * 48}, index=pd.date_range(start=merged_df.index[-1] + pd.Timedelta(minutes=30), periods=48, freq='30T'))\n",
    "# Concatenate without resetting the index, preserving the datetime index\n",
    "df_with_preds = pd.concat([merged_df, date_range_df_temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "45\n",
      "45\n",
      "45\n",
      "45\n",
      "15\n",
      "45\n",
      "6\n",
      "45\n",
      "5\n",
      "41\n",
      "32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:18:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:20:15 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:31:16 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "45\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Function to handle the prediction for a single column\n",
    "def predict_for_column(column):\n",
    "    if column != \"System_Price\":\n",
    "        # Creating prediction using Prophet and ARIMA for the column\n",
    "        prophet_arima_preds = prophet_predictions(merged_df[[column]], 45)\n",
    "        # Extracting the predictions for the next 48 observations\n",
    "        predictions = prophet_arima_preds['combined_forecast'].iloc[-48:].values\n",
    "        return column, predictions\n",
    "    return None\n",
    "\n",
    "# Parallelizing the process\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    # Run the prediction function for each column in parallel\n",
    "    results = list(executor.map(predict_for_column, merged_df.columns))\n",
    "\n",
    "# Update df_with_preds with predictions\n",
    "for result in results:\n",
    "    if result:  # Ensure result is not None\n",
    "        column, predictions = result\n",
    "        df_with_preds.loc[df_with_preds.index[-48:], column] = predictions\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df_with_preds.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date related booleans\n",
    "import holidays\n",
    "\n",
    "# Get British holidays\n",
    "uk_holidays = holidays.UnitedKingdom()\n",
    "\n",
    "df_with_preds['Datetime'] = df_with_preds.index\n",
    "\n",
    "# Add a boolean column for British holidays\n",
    "df_with_preds['is_british_holiday'] = df_with_preds['Datetime'].isin(uk_holidays)\n",
    "\n",
    "print(df_with_preds.head())\n",
    "df_with_preds['day_of_week'] = df_with_preds['Datetime'].dt.dayofweek\n",
    "df_with_preds['is_weekday'] = df_with_preds['Datetime'].dt.weekday < 5  # Monday (0) to Friday (4)\n",
    "df_with_preds['is_weekend'] = df_with_preds['day_of_week'] >= 5  # Saturday and Sunday\n",
    "\n",
    "df_with_preds['hour_of_day'] = df_with_preds['Datetime'].dt.hour\n",
    "df_with_preds['is_peak_hour'] = df_with_preds['hour_of_day'].isin([7, 8, 9, 18, 19, 20])  # Example peak hours\n",
    "\n",
    "# Month, Quarter, and Seasons\n",
    "df_with_preds['month'] = df_with_preds['Datetime'].dt.month\n",
    "df_with_preds['quarter'] = df_with_preds['Datetime'].dt.quarter\n",
    "\"\"\"\n",
    "# Daylight Saving Time\n",
    "def is_dst(date):\n",
    "    return bool(pytz.timezone('Europe/London').dst(date))\n",
    "df_with_preds['is_dst'] = df_with_preds['Datetime'].apply(is_dst)\n",
    "\"\"\"\n",
    "df_with_preds['is_working_day'] = (~df_with_preds['is_british_holiday']) & (~df_with_preds['is_weekend'])\n",
    "\n",
    "def get_season(date):\n",
    "    year = date.year\n",
    "    seasons = {\n",
    "        'Winter': (pd.Timestamp(f'{year}-12-21'), pd.Timestamp(f'{year+1}-03-20')),\n",
    "        'Spring': (pd.Timestamp(f'{year}-03-21'), pd.Timestamp(f'{year}-06-20')),\n",
    "        'Summer': (pd.Timestamp(f'{year}-06-21'), pd.Timestamp(f'{year}-09-22')),\n",
    "        'Fall':   (pd.Timestamp(f'{year}-09-23'), pd.Timestamp(f'{year}-12-20'))\n",
    "    }\n",
    "    for season, (start, end) in seasons.items():\n",
    "        if start <= date <= end:\n",
    "            return season\n",
    "    return 'Unknown'\n",
    "\n",
    "# Add a season column\n",
    "df_with_preds['season'] = df_with_preds['Datetime'].apply(get_season)\n",
    "\n",
    "df_with_preds['is_christmas_season'] = df_with_preds['Datetime'].between(pd.Timestamp('2024-12-20'), pd.Timestamp('2024-12-31'))\n",
    "df_with_preds['is_summer_vacation'] = df_with_preds['Datetime'].dt.month.isin([7, 8])\n",
    "# Reset Datetime as the index\n",
    "df_with_preds = df_with_preds.set_index('Datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (17616) does not match length of index (118368)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m filtered_columns \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m prophet_arima_preds_SP\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseasonal_freq_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m col \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m col\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_lower\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_upper\u001b[39m\u001b[38;5;124m\"\u001b[39m))] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdaily\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweekly\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditive_terms\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrend\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m## Add the filtered columns from prophet_arima_preds_SP to df_with_preds\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m \u001b[43mdf_with_preds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfiltered_columns\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m prophet_arima_preds_SP[filtered_columns]\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4299\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4297\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_frame(key, value)\n\u001b[0;32m   4298\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, (Series, np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mlist\u001b[39m, Index)):\n\u001b[1;32m-> 4299\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4300\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, DataFrame):\n\u001b[0;32m   4301\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_item_frame_value(key, value)\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4350\u001b[0m, in \u001b[0;36mDataFrame._setitem_array\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4347\u001b[0m         \u001b[38;5;28mself\u001b[39m[col] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m   4349\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m-> 4350\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iset_not_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4352\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(value) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4353\u001b[0m     \u001b[38;5;66;03m# list of lists\u001b[39;00m\n\u001b[0;32m   4354\u001b[0m     value \u001b[38;5;241m=\u001b[39m DataFrame(value)\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4380\u001b[0m, in \u001b[0;36mDataFrame._iset_not_inplace\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4377\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns must be same length as key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4379\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(key):\n\u001b[1;32m-> 4380\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m igetitem(value, i)\n\u001b[0;32m   4382\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4383\u001b[0m     ilocs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_indexer_non_unique(key)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4311\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   4309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4310\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[1;32m-> 4311\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4524\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4515\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4516\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   4517\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4522\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   4523\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4524\u001b[0m     value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4527\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m   4528\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   4529\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[0;32m   4530\u001b[0m     ):\n\u001b[0;32m   4531\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   4532\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:5266\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   5263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m   5265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m-> 5266\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5267\u001b[0m arr \u001b[38;5;241m=\u001b[39m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   5268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   5269\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(value, Index)\n\u001b[0;32m   5270\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5273\u001b[0m     \u001b[38;5;66;03m# TODO: Remove kludge in sanitize_array for string mode when enforcing\u001b[39;00m\n\u001b[0;32m   5274\u001b[0m     \u001b[38;5;66;03m# this deprecation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\pandas\\core\\common.py:573\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m--> 573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    577\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    578\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (17616) does not match length of index (118368)"
     ]
    }
   ],
   "source": [
    "# Variable that tracks the difference between total load and demand\n",
    "df_with_preds[\"Load-Demand\"] = df_with_preds[\"Total_Load\"] - df_with_preds[\"Demand_Outturn\"]\n",
    "\n",
    "# Recalculate the LoLP using the Normal CDF (we only use the difference btw Total load and demand outturn)\n",
    "# For instructions, we consulted:\n",
    "# approximation since we do not have some of the information\n",
    "# https://bscdocs.elexon.co.uk/category-3-documents/loss-of-load-probability-calculation-methodolgy-statement\n",
    "df_with_preds['LoLP'] = 1 - norm.cdf(df_with_preds[\"Load-Demand\"], loc=0, scale=np.sqrt(700))\n",
    "# Calculate the LoLP lag 1 as proxy for prediction of not enough electricity for next day since the load and demand are super autocorellated\n",
    "df_with_preds['LoLP_lag1'] = df_with_preds['LoLP'].shift(1).copy()\n",
    "\n",
    "\n",
    "# Feature engineering to create wind+solar variable, ignoring NaNs (if there is NaN in one of them, the sum is not NaN)\n",
    "df_with_preds[\"Wind_Solar\"] = df_with_preds[[\"Solar\", \"Wind_Onshore\", \"Wind_Offshore\"]].sum(axis=1, skipna=True)\n",
    "# Sum all columns except 'GMT Time', ignoring NaNs\n",
    "df_with_preds['Total_Generation'] = df_with_preds[['Biomass', 'Fossil_Gas', 'Fossil_Hard_Coal', 'Fossil_Oil',\n",
    "                                                    'Hydro_Pumped_Storage', 'Hydro_Run-of-River_and_Poundage', 'Nuclear',\n",
    "                                                    'Solar', 'Wind_Onshore', 'Wind_Offshore']].sum(axis=1, skipna=True)\n",
    "\n",
    "\n",
    "# Total_Load = Total Generation + Exports - Imports - Stored Energy\n",
    "# So we create a column that is the difference between exports, imports and stored energy\n",
    "df_with_preds[\"Exports-Imports-Stored\"] = df_with_preds[\"Total_Load\"] - df_with_preds[\"Total_Generation\"]\n",
    "df_with_preds[\"Generation-Demand\"] = df_with_preds[\"Total_Generation\"] - df_with_preds[\"Demand_Outturn\"]\n",
    "\n",
    "# Recalculate the LoLP using Generation, using the Normal CDF (we only use the difference btw Total Generation and demand outturn)\n",
    "# For instructions, we consulted:\n",
    "# https://bscdocs.elexon.co.uk/category-3-documents/loss-of-load-probability-calculation-methodolgy-statement\n",
    "df_with_preds['LoLP_Gen'] = 1 - norm.cdf(df_with_preds[\"Total_Generation\"] - df_with_preds[\"Demand_Outturn\"], loc=0, scale=np.sqrt(700))\n",
    "# Calculate the LoLP lag 1 as proxy for prediction of not enough electricity for next day since the load and demand are super autocorellated\n",
    "df_with_preds['LoLP_Gen_lag1'] = df_with_preds['LoLP_Gen'].shift(1).copy()\n",
    "\n",
    "\n",
    "# Caolumn with Day Ahead Price but lag 48, since they are predictions for next day\n",
    "df_with_preds['Day_Ahead_Price_lag48'] = df_with_preds['Day_Ahead_Price'].shift(48).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, we save the df with predictions for all columns to a csv for easier retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_preds.reset_index(inplace=True)\n",
    "# In order to save the merged to csv for easier retrieval\n",
    "df_with_preds.to_csv(\"merged_df_with_preds.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
