{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "\n",
    "# Core Python packages and utilities\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Data handling and preprocessing\n",
    "import pandas as pd\n",
    "from pandas.errors import PerformanceWarning\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Time series analysis and modeling\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from statsmodels.stats.diagnostic import het_arch\n",
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import AutoARIMA\n",
    "from arch import arch_model\n",
    "from prophet import Prophet\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.optimize import nnls\n",
    "from xgboost import DMatrix\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Neural networks and deep learning\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "import tensorflow as tf\n",
    "from optuna.integration import TFKerasPruningCallback\n",
    "import optuna\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parallel computing\n",
    "import pyopencl as cl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality of life settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform 0: NVIDIA CUDA\n",
      "  Device 0: NVIDIA GeForce GTX 1060\n",
      "Platform 1: Intel(R) OpenCL HD Graphics\n",
      "  Device 0: Intel(R) UHD Graphics 620\n"
     ]
    }
   ],
   "source": [
    "# Suppress warnings (e.g., FutureWarnings, PerformanceWarnings)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=PerformanceWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"No frequency information was provided, so inferred frequency 30min will be used.\")\n",
    "\n",
    "# Parallel computing configuration for TensorFlow\n",
    "num_of_threads = 8  # Set the number of threads manuallly according to your PC specifications\n",
    "tf.config.threading.set_intra_op_parallelism_threads(num_of_threads)\n",
    "\n",
    "# Checks which devices are available and sets the cuda gpu in path\n",
    "platforms = cl.get_platforms()\n",
    "for i, platform in enumerate(platforms):\n",
    "    print(f\"Platform {i}: {platform.name}\")\n",
    "    for j, device in enumerate(platform.get_devices()):\n",
    "        if platform.name == \"NVIDIA CUDA\":\n",
    "            gpu_platform_id = i\n",
    "            gpu_device_id = j\n",
    "        print(f\"  Device {j}: {device.name}\")\n",
    "os.environ[\"LIGHTGBM_GPU_PLATFORM_ID\"] = str(gpu_platform_id)  # Makes sure the Platform id that will be used is set correctly in the System Varibles\n",
    "os.environ[\"LIGHTGBM_GPU_DEVICE_ID\"] = str(gpu_platform_id)  # Makes sure the GPU id that will be used is set correctly in the System Varibles\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv(\"merged_df_Combined.csv\", parse_dates=['Datetime'])  # df merging four initial csvs with missing filled from previous preprocessing steps\n",
    "merged_df.set_index('Datetime', inplace=True)\n",
    "merged_df_2024 = merged_df[(merged_df.index >= '2023-09-30') & (merged_df.index < '2025-01-01')]  # we only look at the last 365 days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Defined Functions for quality of life reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fft(series, n_top_seasonalities, threshold_pc=0.02):\n",
    "    \"\"\"\n",
    "    Calculate significant positive frequencies and their amplitudes using Fast Fourier Transform (FFT),\n",
    "    selecting the lower of 2% of the max amplitude or the top `n` frequencies.\n",
    "\n",
    "    Parameters:\n",
    "    - series (pd.Series): The input time series data.\n",
    "    - n_top_seasonalities (int): The maximum number of significant frequencies to consider.\n",
    "    - threshold_pc (float): Percentage (0 < threshold_pc <= 1) of the maximum amplitude to filter significant frequencies.\n",
    "\n",
    "    Returns:\n",
    "    - zip: A generator yielding (positive frequency, amplitude) for each significant frequency.\n",
    "    \"\"\"\n",
    "    # Compute fast Fourier transform\n",
    "    price_fft = np.fft.fft(series.dropna())\n",
    "\n",
    "    # Get frequencies corresponding to FFT coefficients\n",
    "    freqs = np.fft.fftfreq(len(price_fft), d=1/48)  # because half-hourly data\n",
    "\n",
    "    # Calculate amplitudes\n",
    "    amplitudes = np.abs(price_fft)\n",
    "\n",
    "    # Calculate the threshold based on 2% of the max amplitude\n",
    "    threshold = threshold_pc * np.max(amplitudes)\n",
    "\n",
    "    # Filter positive frequencies with amplitudes above threshold\n",
    "    positive_indices = np.where((amplitudes > threshold) & (freqs > 0))\n",
    "    positive_freqs = freqs[positive_indices]\n",
    "    positive_amplitudes = amplitudes[positive_indices]\n",
    "\n",
    "    # Sort by amplitude and select the lower of `n_top_seasonalities` or all significant frequencies\n",
    "    sorted_indices = np.argsort(positive_amplitudes)[::-1]\n",
    "    selected_indices = sorted_indices[:min(n_top_seasonalities, len(sorted_indices))]\n",
    "\n",
    "    # Select the top frequencies and amplitudes\n",
    "    significant_freqs = positive_freqs[selected_indices]\n",
    "    significant_amplitudes = positive_amplitudes[selected_indices]\n",
    "\n",
    "    return zip(significant_freqs, significant_amplitudes)\n",
    "\n",
    "\n",
    "def metrics(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate and display error metrics for model evaluation.\n",
    "\n",
    "    This function computes standard error metrics, including Mean Absolute Error (MAE), Mean Squared Error (MSE),\n",
    "    Root Mean Squared Error (RMSE), and R-squared (R2), to evaluate the accuracy of model predictions.\n",
    "\n",
    "    Parameters:\n",
    "    - y_test (Series or array-like): The true values for the target variable in the test set.\n",
    "    - y_pred (Series or array-like): The predicted values for the target variable in the test set.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Calculate error metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Print error metrics\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"R-squared (R2):\", r2)\n",
    "    return rmse\n",
    "\n",
    "\n",
    "# Memory reduction for our dataset by setting optimal dtypes\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    \"\"\"\n",
    "    Reduces the memory usage of a pandas DataFrame by downcasting numeric columns \n",
    "    to the smallest possible dtype that can fit the range of values, and converting \n",
    "    other columns to 'category' type where applicable.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame to optimize.\n",
    "    verbose (bool): If True, prints the memory usage reduction.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The optimized DataFrame with reduced memory usage.\n",
    "\n",
    "    Function Details:\n",
    "    - Identifies numeric columns with data types like integers and floats.\n",
    "    - For integer columns, downcasts to the smallest possible integer type \n",
    "      (e.g., int8, int16, int32, int64) that can accommodate the data's range.\n",
    "    - For float columns, downcasts to the smallest possible float type \n",
    "      (e.g., float16, float32, float64) that can accommodate the data's range.\n",
    "    - Converts non-numeric columns to 'category' type for memory efficiency.\n",
    "    - Calculates and optionally prints the reduction in memory usage.\n",
    "\n",
    "    Notes:\n",
    "    - Ensures no data loss during type conversion by checking the range of values \n",
    "      before downcasting.\n",
    "    - Particularly useful for large datasets where memory constraints are a concern.\n",
    "    \"\"\"\n",
    "    numerics = [\"int8\", 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "             df[col] = (df[col]).astype(\"category\")\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: \n",
    "        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "\n",
    "# Plotting series, ACF, and PACF\n",
    "def ACF_PACF(residuals):\n",
    "    \"\"\"\n",
    "    Plots the Time Series, Auto-Correlation Function (ACF), and Partial Auto-Correlation Function (PACF)\n",
    "    for a given set of residuals to analyze time series behavior.\n",
    "\n",
    "    Parameters:\n",
    "    residuals (pd.Series): A pandas Series containing the residuals or time series data.\n",
    "\n",
    "    Returns:\n",
    "    None: The function displays the plots but does not return any values.\n",
    "\n",
    "    Function Details:\n",
    "    - Creates a 3-row subplot layout for visual analysis.\n",
    "    - The first plot shows the time series of the residuals to observe any patterns or trends over time.\n",
    "    - The second plot displays the ACF (Auto-Correlation Function) to measure the correlation between \n",
    "      the time series and its lagged values.\n",
    "    - The third plot displays the PACF (Partial Auto-Correlation Function) to measure the correlation \n",
    "      between the time series and its lagged values while controlling for the correlations of intervening lags.\n",
    "    - The number of lags to display in ACF and PACF can be adjusted in the `lags` parameter.\n",
    "    - Uses `tight_layout()` to optimize spacing between subplots for better visualization.\n",
    "    \"\"\"\n",
    "    # Create the figure and axes\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 16))\n",
    "\n",
    "    # Plot the time series\n",
    "    axes[0].plot(residuals.index, residuals, color='blue')\n",
    "    axes[0].set_title('Time Series')\n",
    "    axes[0].set_xlabel('Date')\n",
    "    axes[0].set_ylabel('Value')\n",
    "\n",
    "    # Plot ACF (Auto-Correlation Function)\n",
    "    plot_acf(residuals, ax=axes[1], lags=100)  # Set the lags as per your requirement\n",
    "    axes[1].set_title('ACF (Auto-Correlation Function)')\n",
    "\n",
    "    # Plot PACF (Partial Auto-Correlation Function)\n",
    "    plot_pacf(residuals, ax=axes[2], lags=100)  # Set the lags as per your requirement\n",
    "    axes[2].set_title('PACF (Partial Auto-Correlation Function)')\n",
    "\n",
    "    # Adjust layout for better appearance\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def train_test_split_index_univariate(data, test_size=48):\n",
    "    \"\"\"\n",
    "    Splits a dataset into sequential training and testing sets.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame or pd.Series): Dataset to split.\n",
    "    test_size (int): Number of samples in the test set (default: 48).\n",
    "\n",
    "    Returns:\n",
    "    tuple: (train, test) where:\n",
    "        - train: All rows except the last `test_size` rows.\n",
    "        - test: Last `test_size` rows.\n",
    "\n",
    "    Notes:\n",
    "    - Preserves order, suitable for time-series data.\n",
    "    \"\"\"\n",
    "    train = data.iloc[:-test_size]\n",
    "    test = data.iloc[-test_size:]\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def train_test_split_index_multivariate(data, test_size=48):\n",
    "    \"\"\"\n",
    "    Splits a dataset into sequential training and testing sets.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame or pd.Series): Dataset to split.\n",
    "    test_size (int): Number of samples in the test set (default: 48).\n",
    "\n",
    "    Returns:\n",
    "    tuple: (train, test) where:\n",
    "        - train: All rows except the last `test_size` rows.\n",
    "        - test: Last `test_size` rows.\n",
    "\n",
    "    Notes:\n",
    "    - Preserves order, suitable for time-series data.\n",
    "    \"\"\"\n",
    "    data = data.iloc[2:-(test_size+1)]  # remove the missing because of lags and the predicted\n",
    "    train = data.iloc[:-test_size]\n",
    "    test = data.iloc[-test_size:]\n",
    "    return train, test\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "\n",
    "\n",
    "def Yeo_Johnson(series):\n",
    "    pt = PowerTransformer(method='yeo-johnson')\n",
    "    transformed = pt.fit_transform(series.values.reshape(-1, 1)).flatten()\n",
    "    return pd.Series(transformed, index=pd.to_datetime(series.index)), pt\n",
    "\n",
    "def inverse_Yeo_Johnson(transformed_series, pt):\n",
    "    # Reshape the transformed data to 2D\n",
    "    transformed_2d = transformed_series.values.reshape(-1, 1)\n",
    "    \n",
    "    # Apply the inverse transformation\n",
    "    original_data = pt.inverse_transform(transformed_2d)\n",
    "    \n",
    "    # Flatten the result and return as a Pandas Series\n",
    "    return pd.Series(original_data.flatten(), index=transformed_series.index)\n",
    "\n",
    "\n",
    "def plot_results(test, test_original, test_predictions, model_name):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.plot(test.index, test_original, label='Actual Data', color='blue')\n",
    "    plt.plot(test.index, test_predictions, label=f'{model_name} Predictions', linestyle='dashed', color='red')\n",
    "    plt.title(f'{model_name} Predictions vs Actual Data')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Defined Functions for Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsforecast.mfles import MFLES # Import MFLES directly\n",
    "def univariate_MFLES(series, num_rows):\n",
    "    temp_index = series.index\n",
    "    series, pt = Yeo_Johnson(series)\n",
    "    series.index.freq = pd.infer_freq(temp_index)\n",
    "    # Initialize and fit MFLES\n",
    "    mfles_model = MFLES(verbose=1)\n",
    "\n",
    "    # Define seasonal periods and multiple `ma` values\n",
    "    seasonal_periods = [48, 48*7, 48*30, 48*366, 12, 16]\n",
    "    ma_values = [3, 4, int(min(seasonal_periods)), int(min(seasonal_periods)/2),None]\n",
    "\n",
    "    # Perform optimization\n",
    "    optimal_params = mfles_model.optimize(\n",
    "        y=series.values,  # Time series as a NumPy array\n",
    "        test_size=48*10,  # Test set size\n",
    "        n_steps=3,  # Number of train-test splits\n",
    "        seasonal_period=seasonal_periods,  # Seasonal periods\n",
    "        metric=\"smape\",  # Metric for evaluation\n",
    "        params={\n",
    "            \"ma\": ma_values,  # Include multiple `ma` values\n",
    "            \"smoother\": [True],\n",
    "            \"seasonality_weights\": [True],\n",
    "            \"changepoints\": [False],\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Fit the model using optimal parameters\n",
    "    fitted_values = mfles_model.fit(\n",
    "        y=series.values,\n",
    "        seasonal_period=optimal_params.get(\"seasonal_period\", seasonal_periods),\n",
    "        max_rounds=10,  # Adjust this if needed\n",
    "        ma=optimal_params.get(\"ma\"),\n",
    "        smoother=optimal_params.get(\"smoother\"),\n",
    "        seasonality_weights=optimal_params.get(\"seasonality_weights\"),\n",
    "        changepoints=optimal_params.get(\"changepoints\"),\n",
    "    )\n",
    "\n",
    "\n",
    "    # Predict out-of-sample (next 48 steps)\n",
    "    forecast_horizon = num_rows\n",
    "    future_predictions = mfles_model.predict(forecast_horizon)\n",
    "\n",
    "\n",
    "    # Combine original series and predictions for plotting\n",
    "    future_index = pd.date_range(start=temp_index[-1], periods=forecast_horizon+1, freq='30T')[1:]\n",
    "    future_series = pd.Series(future_predictions, index=future_index)\n",
    "\n",
    "    # Align fitted with series index\n",
    "    fitted_series = pd.Series(fitted_values, index=temp_index)\n",
    "\n",
    "    # Combine fitted_values and predicted\n",
    "    combined_series = pd.concat([fitted_series, future_series])\n",
    "\n",
    "\n",
    "    #########################################################################\n",
    "    # Step 2: Calculate Residuals\n",
    "    residuals = series - fitted_values\n",
    "\n",
    "    # Step 3: Fit ARIMA on the Residuals\n",
    "    arima_forecast = statsforecast_arima(residuals, num_rows)  # Ensure non-na data for ARIMA\n",
    "\n",
    "    # Step 4: Combine the Predictions of Prophet and ARIMA\n",
    "    combined_forecast = combined_series.add(arima_forecast)  # Using fill_value to handle NaNs\n",
    "\n",
    "    #########################################################################\n",
    "\n",
    "    # Plot\n",
    "\n",
    "    # plt.plot(series, label='Original Series')\n",
    "    # plt.plot(combined_series, linestyle='dashed', color='red', label='Fitted + Predicted')\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "    # Ensure fitted and series have the same length for comparison\n",
    "    common_arima = combined_forecast[:len(series)]\n",
    "    \n",
    "    # Calculate MAPE without ARIMA\n",
    "    actual_series = inverse_Yeo_Johnson(series, pt)\n",
    "    fitted_series_inv = inverse_Yeo_Johnson(fitted_series, pt)\n",
    "    # Calculate MAPE with ARIMA\n",
    "    common_arima_inv = inverse_Yeo_Johnson(common_arima, pt)\n",
    "\n",
    "    # Calculate SMAPE without ARIMA\n",
    "    smape_without_arima = (\n",
    "        np.mean(2 * np.abs(actual_series - fitted_series_inv) / (np.abs(actual_series) + np.abs(fitted_series_inv)))\n",
    "    ) * 100\n",
    "    print(f\"SMAPE in-sample without ARIMA: {smape_without_arima:.2f}%\")\n",
    "\n",
    "    # Calculate SMAPE with ARIMA\n",
    "    smape_with_arima = (\n",
    "        np.mean(2 * np.abs(actual_series - common_arima_inv) / (np.abs(actual_series) + np.abs(common_arima_inv)))\n",
    "    ) * 100\n",
    "    print(f\"SMAPE in-sample with ARIMA: {smape_with_arima:.2f}%\")\n",
    "\n",
    "    return inverse_Yeo_Johnson(combined_forecast, pt)\n",
    "\n",
    "\n",
    "def extend_df_MFLES_univariate(df, num_rows):\n",
    "    to_predict = [\"Intraday_Price\", \"Total_Load\",\"Demand_Outturn\", \"System_Price\", \"NIV_Outturn\",\n",
    "                  # \"BSAD_Turn_Up\",\"BSAD_Turn_Down\",\n",
    "                  \"EPEX_Intraday_Volume\",  \"Wind_Solar\", \"Total_Generation\",\n",
    "                  # \"Biomass\", \"Fossil_Gas\", \"Fossil_Hard_Coal\", \"Hydro_Pumped_Storage\", \"Hydro_Run-of-River_and_Poundage\", \"Nuclear\", \"Solar\", \"Wind_Onshore\", \"Wind_Offshore\"  # Energy generation which is cyclical, except Fossil Oil which is WN\n",
    "                  ]  # Also don't predict Loss of Load Prob which is WN\n",
    "    # to_derive = [\"BSAD_Total\"]  # Calculate as the sum of BSAD_Turn_Up and BSAD_Turn_Down\n",
    "\n",
    "    # Creating rows to append generated predictions\n",
    "    # Generate a date range that starts after the last date in df, respecting the half hour datetime\n",
    "    date_range_df_temp = pd.DataFrame({'value': [None] * num_rows}, index=pd.date_range(start=df.index[-1] + pd.Timedelta(minutes=30), periods=num_rows, freq='30T'))\n",
    "\n",
    "    # Concatenate without resetting the index, preserving the datetime index\n",
    "    df_with_preds = pd.concat([df, date_range_df_temp])\n",
    "    date_range_df_temp = date_range_df_temp.drop(columns=[\"value\"])  # Remove the temporary column we created in order to introduce rows\n",
    "\n",
    "    for col in to_predict:\n",
    "        print(\"Working on\", col)\n",
    "        df_with_preds.loc[df_with_preds.index[-num_rows:], col] = univariate_MFLES(df[[col]], num_rows)[-num_rows:].values\n",
    "\n",
    "    # df_with_preds[to_derive[0]] = df_with_preds[\"BSAD_Turn_Up\"] + df_with_preds[\"BSAD_Turn_Down\"]  # calculate the BSAD toal wiht the two columns that make it up\n",
    "\n",
    "    not_predicted_columns = set(df_with_preds.columns) - set(to_predict)  # Columns for which predictions are basically just their respective mean\n",
    "\n",
    "    return df_with_preds, list(not_predicted_columns)\n",
    "\n",
    "\n",
    "def feature_engineering(df):\n",
    "    # Feature engineering to create wind+solar variable, ignoring NaNs (if there is NaN in one of them, the sum is not NaN)\n",
    "    df.loc[:,\"Wind_Solar\"] = df[[\"Solar\", \"Wind_Onshore\", \"Wind_Offshore\"]].sum(axis=1, skipna=True)\n",
    "    # Sum all columns except 'GMT Time', ignoring NaNs\n",
    "    df.loc[:,'Total_Generation'] = df.sum(axis=1, skipna=True)\n",
    "\n",
    "    df_with_preds, not_predicted_columns = extend_df_MFLES_univariate(df, 49)  # takes in the df and appends new rows, fills with LSTM for select columns, 49 becaus of how we set up LSTM\n",
    "\n",
    "    df_with_preds.drop(columns=[\"value\"], inplace=True, errors=\"ignore\")  # make sure the placeholder had been deleted\n",
    "\n",
    "    # Variable that tracks the difference between total load and demand\n",
    "    df_with_preds[\"Load-Demand\"] = df_with_preds[\"Total_Load\"] - df_with_preds[\"Demand_Outturn\"]\n",
    "\n",
    "    # Recalculate the LoLP using the Normal CDF (we only use the difference btw Total load and demand outturn, not entirely correct, but Loss of Load probability had a lot of missing values so we wanted to calculate something of our own). \n",
    "    # For instructions, we consulted:\n",
    "    # approximation since we do not have some of the information\n",
    "    # https://bscdocs.elexon.co.uk/category-3-documents/loss-of-load-probability-calculation-methodolgy-statement\n",
    "    df_with_preds['LoLP'] = 1 - norm.cdf(df_with_preds[\"Load-Demand\"], loc=0, scale=np.sqrt(700))\n",
    "    # Calculate the LoLP lag 1 as proxy for prediction of not enough electricity for next day since the load and demand are super autocorellated\n",
    "    df_with_preds['LoLP_lag1'] = df_with_preds['LoLP'].shift(1)\n",
    "\n",
    "\n",
    "    # Feature engineering to create wind+solar variable, ignoring NaNs (if there is NaN in one of them, the sum is not NaN)\n",
    "    df_with_preds[\"Wind_Solar\"] = df_with_preds[[\"Solar\", \"Wind_Onshore\", \"Wind_Offshore\"]].sum(axis=1, skipna=True)\n",
    "    # Sum all columns except 'GMT Time', ignoring NaNs\n",
    "    df_with_preds['Total_Generation'] = df_with_preds.sum(axis=1, skipna=True)\n",
    "\n",
    "\n",
    "    # Total_Load = Total Generation + Exports - Imports - Stored Energy\n",
    "    # So we create a column that is the difference between exports, imports and stored energy\n",
    "    df_with_preds[\"Exports-Imports-Stored\"] = df_with_preds[\"Total_Load\"] - df_with_preds[\"Total_Generation\"]\n",
    "    df_with_preds[\"Generation-Demand\"] = df_with_preds[\"Total_Generation\"] - df_with_preds[\"Demand_Outturn\"]\n",
    "    \n",
    "    Day_Ahead_Price_lag_48 = df_with_preds[\"Day_Ahead_Price\"].shift(48)\n",
    "\n",
    "    for col in df_with_preds:\n",
    "        if df_with_preds[col].dtype == \"object\":\n",
    "            if col != \"value\":\n",
    "                df_with_preds[col] = pd.to_numeric(df_with_preds[col], errors='coerce')\n",
    "    \n",
    "    df_with_preds['Datetime'] = df_with_preds.index\n",
    "\n",
    "    df_with_preds['day_of_week'] = df_with_preds['Datetime'].dt.dayofweek\n",
    "    df_with_preds['is_weekend'] = (df_with_preds['Datetime'].dt.weekday >= 5).astype('int8')  # Monday (0) to Friday (4)\n",
    "\n",
    "    df_with_preds['hour_of_day'] = df_with_preds['Datetime'].dt.hour\n",
    "    df_with_preds['hour_of_day_half'] = df_with_preds['Datetime'].dt.hour + df_with_preds['Datetime'].dt.minute / 60\n",
    "\n",
    "    df_with_preds['is_peak_hour'] = df_with_preds['hour_of_day'].isin([16, 17, 18, 19, 20]).astype('int8')\n",
    "    df_with_preds['is_night'] = df_with_preds['hour_of_day'].isin([0, 1, 2, 3, 4, 5, 23]).astype('int8')\n",
    "\n",
    "    # Create a boolean column 'close_price' for the last observation of the day\n",
    "    df_with_preds['close_price'] = (df_with_preds['Datetime'].dt.time == pd.Timestamp('23:30').time()).astype('int8')\n",
    "    # Create a new column for prev_day_close\n",
    "    df_with_preds['prev_day_close'] = None\n",
    "    # Assign the shifted close prices to the 'prev_day_close' column\n",
    "    df_with_preds.loc[:, 'prev_day_close'] = df_with_preds.loc[df_with_preds['close_price'] == 1, \"Intraday_Price\"].shift()\n",
    "    # Fill missing values in the 'prev_day_close' column with backward fill\n",
    "    df_with_preds['prev_day_close'] = df_with_preds['prev_day_close'].fillna(method='bfill')\n",
    "    # Create a boolean column 'open_price' for the first observation of the day\n",
    "    df_with_preds['open_price'] = (df_with_preds['Datetime'].dt.time == pd.Timestamp('00:00').time()).astype('int8') * df_with_preds['prev_day_close']\n",
    "\n",
    "    df_with_preds[\"System_Price_lag_1\"] = df_with_preds[\"System_Price\"].shift(1)\n",
    "    df_with_preds[\"System_Price_lag_2\"] = df_with_preds[\"System_Price\"].shift(2)\n",
    "    df_with_preds[\"NIV_Outturn_lag_1\"] = df_with_preds[\"NIV_Outturn\"].shift(1)\n",
    "    df_with_preds[\"NIV_Outturn_lag_2\"] = df_with_preds[\"NIV_Outturn\"].shift(2)\n",
    "\n",
    "\n",
    "    # seasonalities using the results from the fft\n",
    "    freqs = []\n",
    "    for freq, amp in calculate_fft(df_with_preds[\"System_Price\"], 3, threshold_pc=0.02):  # Outputs same seasonalities for System Price and NIV_outturn\n",
    "        freqs.append(freq)\n",
    "    freqs = np.array(freqs)\n",
    "    all_periods_in_days = 1 / freqs\n",
    "    # Multiply each unique period by 24 to get values in hours\n",
    "    unique_cycles_in_days = np.unique(all_periods_in_days.round(2))\n",
    "    unique_cycles_in_hours = unique_cycles_in_days * 48  # because half hourly data\n",
    "\n",
    "    # Format the results from scientific notation to decimal for readability\n",
    "    formatted_cycles = [f\"{value:.2f}\" for value in np.sort(unique_cycles_in_days)]\n",
    "    print(\"Unique cycles:\", formatted_cycles, \"in days\")\n",
    "    # Format the output to show in hours without scientific notation\n",
    "    formatted_cycles_in_hours = [f\"{value:.2f}\" for value in np.sort(unique_cycles_in_hours)]\n",
    "    print(\"Unique cycles:\", formatted_cycles_in_hours, \"in hours\")\n",
    "\n",
    "    for cycle in unique_cycles_in_hours:\n",
    "        df_with_preds[f\"hr_{cycle:.2f}h_sin\"] = np.sin(2 * np.pi * df_with_preds['hour_of_day_half'] / cycle)\n",
    "        df_with_preds[f\"hr_{cycle:.2f}h_cos\"] = np.cos(2 * np.pi * df_with_preds['hour_of_day_half'] / cycle)\n",
    "\n",
    "\n",
    "    # Month, Quarter, and Seasons\n",
    "    df_with_preds['year'] = df_with_preds['Datetime'].dt.year\n",
    "    df_with_preds['month'] = df_with_preds['Datetime'].dt.month\n",
    "    df_with_preds['quarter'] = df_with_preds['Datetime'].dt.quarter\n",
    "\n",
    "    # Reset Datetime as the index\n",
    "    df_with_preds = df_with_preds.set_index('Datetime')\n",
    "\n",
    "    df_with_preds = df_with_preds.drop(columns=[\"prev_day_close\"])\n",
    "    df_with_preds = df_with_preds.drop(columns=[col for col in df_with_preds.columns if col.endswith(\"_diff\")])\n",
    "\n",
    "    cols_not_to_differentiate = [\"year\", \"month\", \"quarter\", \"hour_of_day_half\", \"hour_of_day\" ,\"day_of_week\" ,  # remove the time ones\n",
    "                                \"is_peak_hour\", \"is_night\", \"is_weekend\",  # remove the boolean ones\n",
    "                                \"open_price\", \"close_price\", \"prev_day_close\",  # remove the price ones\n",
    "                                \"LoLP_lag1\", \"System_Price_lag_1\", \"System_Price_lag_2\", \"Day_Ahead_Price_lag_48\", \"NIV_Outturn_lag_1\", \"NIV_Outturn_lag_2\"]  # remove the lag ones\n",
    "    cols_not_to_differentiate = cols_not_to_differentiate + not_predicted_columns  # We also do not do differentiation on the cols we did not predict\n",
    "\n",
    "    for col in [cols for cols in df_with_preds.columns if not cols.endswith(\"sin\") and not cols.endswith(\"cos\")]:\n",
    "        if col not in cols_not_to_differentiate:\n",
    "            df_with_preds.loc[:,f\"{col}_diff\"] = df_with_preds[col].diff()\n",
    "    \n",
    "    return df_with_preds, Day_Ahead_Price_lag_48, not_predicted_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Defined Functions for ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto ARIMA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statsforecast_arima(df, forecast_horizon):\n",
    "    \"\"\"\n",
    "    Fit an AutoARIMA model to the time series data and forecast future values.\n",
    "\n",
    "    This function uses AutoARIMA from the statsforecast package to automatically select the best ARIMA model.\n",
    "    It performs both in-sample prediction and forecasts future values beyond the length of the data provided.\n",
    "\n",
    "    Parameters:\n",
    "    - df (Series): The input time series data. The index must be a DateTimeIndex.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A DataFrame containing the in-sample predictions and forecasted values over an extended range.\n",
    "    \"\"\"\n",
    "    # Ensure the input is a pandas DataFrame with required columns\n",
    "    df = pd.DataFrame({'unique_id': 1, 'ds': df.index, 'y': df.values})\n",
    "\n",
    "    # Initialize the StatsForecast object with the AutoARIMA model\n",
    "    sf = StatsForecast(models=[AutoARIMA()], freq='30min', n_jobs=-1)\n",
    "\n",
    "    # Forecast future values\n",
    "    forecast = sf.forecast(df=df, h=forecast_horizon, fitted=True)\n",
    "    values=sf.forecast_fitted_values()\n",
    "    values.set_index('ds', inplace=True)\n",
    "    forecast.set_index('ds', inplace=True)\n",
    "    result = pd.concat([values, forecast])\n",
    "    return result[\"AutoARIMA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def make_future_predictions_multivariate(model, data, exog_future, n_future_steps, time_step):\n",
    "    \"\"\"\n",
    "    Generates future predictions using a trained LSTM model with future exogenous data.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained LSTM model.\n",
    "    - data (array-like): The input time series data (target variable), shape [time_steps, 1].\n",
    "    - exog_future (array-like): Future exogenous variables, shape [n_future_steps, num_features].\n",
    "    - n_future_steps (int): The number of future steps to predict.\n",
    "    - time_step (int): The length of the input sequence required by the LSTM model.\n",
    "\n",
    "    Returns:\n",
    "    - future_preds (np.array): Predicted target values for the next `n_future_steps`.\n",
    "    \"\"\"\n",
    "    future_preds = []\n",
    "\n",
    "    # Initialize input sequence with the last `time_step` rows of `data`\n",
    "    input_seq = data[-time_step:].reshape(1, time_step, 1)\n",
    "    \n",
    "    for i in range(n_future_steps):\n",
    "        # Predict the next target value\n",
    "        pred = model.predict(input_seq)[0, 0]  # Predict for the target variable\n",
    "        future_preds.append(pred)\n",
    "\n",
    "        # Update the input sequence\n",
    "        # Combine prediction (target) with future exogenous data for the next time step\n",
    "        future_exog = exog_future[i].reshape(1, 1, -1)  # Exogenous variables for the next step\n",
    "        pred_with_exog = np.concatenate((pred.reshape(1, 1, 1), future_exog), axis=2)\n",
    "        \n",
    "        # Update input_seq: Remove the oldest step and append the new prediction and exogenous features\n",
    "        input_seq = np.append(input_seq[:, 1:, :], pred_with_exog, axis=1)\n",
    "    return np.array(future_preds)\n",
    " \n",
    "def create_dataset_with_exog(data, exog_data, time_step=1):\n",
    "    \"\"\"\n",
    "    Creates a dataset for time series modeling, including exogenous variables.\n",
    "\n",
    "    Parameters:\n",
    "    - data (np.array): The main time series data (e.g., the target variable), assumed to be a 2D array.\n",
    "    - exog_data (np.array): Exogenous variables (features other than the target), assumed to be a 2D array.\n",
    "    - time_step (int): The number of time steps in each input sequence.\n",
    "\n",
    "    Returns:\n",
    "    - X (np.array): Input sequences for the target variable, of shape [samples, time_step].\n",
    "    - exog_X (np.array): Input sequences for exogenous variables, of shape [samples, time_step, exog_features].\n",
    "    - y (np.array): Target values for the next time step, of shape [samples].\n",
    "\n",
    "    Function Details:\n",
    "    1. **Input Sequences (`X`)**:\n",
    "       - Extracts sequences of length `time_step` from the target variable (`data`).\n",
    "       - Each sequence represents the target's past `time_step` values.\n",
    "\n",
    "    2. **Exogenous Sequences (`exog_X`)**:\n",
    "       - Extracts sequences of length `time_step` from the exogenous data (`exog_data`).\n",
    "       - Includes all features in the exogenous dataset.\n",
    "\n",
    "    3. **Target Values (`y`)**:\n",
    "       - Sets the target as the value immediately following each input sequence (`data[i + time_step, 0]`).\n",
    "    \"\"\"\n",
    "    X, exog_X, y = [], [], []\n",
    "    for i in range(len(data) - time_step):\n",
    "        # Correctly align sequences and target\n",
    "        X.append(data[i:(i + time_step), 0])  # Input sequence\n",
    "        exog_X.append(exog_data[i:(i + time_step), :])  # Exogenous sequence\n",
    "        y.append(data[i + time_step, 0])  # Target is the next step\n",
    "    return np.array(X), np.array(exog_X), np.array(y)\n",
    "\n",
    "\n",
    "def inverse_transform_preds(array, target_scaler):\n",
    "    \"\"\"\n",
    "    Reverses the scaling applied to predictions to restore them to the original scale.\n",
    "    \n",
    "    Parameters:\n",
    "    - array (np.array): The scaled predictions.\n",
    "    - target_scaler: The scaler object used for scaling the target variable.\n",
    "    Returns:\n",
    "    - final_preds (np.array): The predictions in the original scale.\n",
    "\n",
    "    Details:\n",
    "    - Reshapes the scaled predictions into a 2D array to use the scaler's `inverse_transform` method.\n",
    "    - Flattens the resulting array back to 1D after the inverse transformation.\n",
    "    - Removes the first element from the predictions due to a mismatch during alignment.\n",
    "    \"\"\"\n",
    "    final_preds = target_scaler.inverse_transform(array.reshape(-1, 1))\n",
    "    final_preds = final_preds.flatten()\n",
    "    final_preds = final_preds[1:]  # Shift the array by removing the first element, as there is an error in matching.\n",
    "    return final_preds\n",
    "\n",
    "\n",
    "def LSTM_prepare_data(df_with_preds, time_step, test_size, cols_to_drop, target):\n",
    "   \"\"\"\n",
    "   Prepares data for training and testing an LSTM model with exogenous variables, including scaling and sequence creation.\n",
    "\n",
    "   Parameters:\n",
    "   - df_with_preds (pd.DataFrame): The input DataFrame containing the target variable and exogenous variables.\n",
    "   - time_step (int): The number of time steps in each input sequence.\n",
    "   - test_size (int): The number of rows to use for the test set.\n",
    "   - cols_to_drop (list): List of columns to drop from the dataset.\n",
    "\n",
    "   Returns:\n",
    "   - target_scaler (StandardScaler): Fitted scaler for the target variable.\n",
    "   - X_train (np.array): Training input sequences for the target variable.\n",
    "   - X_test (np.array): Testing input sequences for the target variable.\n",
    "   - exog_X_train (np.array): Training input sequences for exogenous variables.\n",
    "   - exog_X_test (np.array): Testing input sequences for exogenous variables.\n",
    "   - y_train (np.array): Training target sequences.\n",
    "   - y_test (np.array): Testing target sequences.\n",
    "   - index (pd.Index): Index of the original DataFrame.\n",
    "\n",
    "   Function Details:\n",
    "   1. **Data Cleaning**:\n",
    "      - Drops specified columns (`cols_to_drop`) from the DataFrame.\n",
    "      - Removes the first row, which may contain null values due to lag features.\n",
    "\n",
    "   2. **Train/Test Split**:\n",
    "      - Splits the dataset into training and testing sets based on `test_size`.\n",
    "\n",
    "   3. **Scaling**:\n",
    "      - Normalizes the target variable using `MinMax`.\n",
    "      - Normalizes exogenous variables separately using another `MinMax`.\n",
    "\n",
    "   4. **Sequence Creation**:\n",
    "      - Creates input sequences of length `time_step` for both the target variable and exogenous variables.\n",
    "      - Aligns each sequence with the corresponding target value.\n",
    "\n",
    "   5. **Dataset Splitting**:\n",
    "      - Splits the sequences back into training and testing sets after sequence creation.\n",
    "\n",
    "   6. **Reshaping for LSTM**:\n",
    "      - Reshapes the target input sequences to be compatible with LSTM input format: `[samples, time steps, features]`.\n",
    "   \"\"\"\n",
    "   # Assume df_with_preds is your DataFrame with target as target and other columns as exogenous variables.\n",
    "   df = df_with_preds.copy()\n",
    "\n",
    "   if cols_to_drop:\n",
    "      cols_to_drop = [col for col in cols_to_drop if col != \"NIV_Outturn\"]\n",
    "      cols_to_drop = [col for col in cols_to_drop if col != \"System_Price\"]\n",
    "      df = df.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "\n",
    "   df = df.iloc[2:]  # Drop the first 2 rows (too many nulls)\n",
    "\n",
    "   # Split the dataset into training and test sets BEFORE scaling\n",
    "   train_data = df.iloc[:-test_size]\n",
    "   test_data = df.iloc[-test_size:]\n",
    "\n",
    "   # Separate target and features for train and test\n",
    "   y_train_raw = train_data[[target]].values\n",
    "   y_test_raw = test_data[[target]].values\n",
    "\n",
    "   X_train_raw = train_data.drop(columns=[target]).values\n",
    "   X_test_raw = test_data.drop(columns=[target]).values\n",
    "\n",
    "   # Normalize the target (target) and the exogenous features using StandardScaler\n",
    "   target_scaler = MinMaxScaler()\n",
    "   # Fit and transform the target variable (y_train_raw)\n",
    "   y_train_scaled = target_scaler.fit_transform(y_train_raw)\n",
    "   # Transform the test target variable (y_test_raw)\n",
    "   y_test_scaled = target_scaler.transform(y_test_raw)\n",
    "\n",
    "\n",
    "   exog_scaler = MinMaxScaler()\n",
    "   # Fit and transform the exogenous variables (X_train_raw)\n",
    "   X_train_scaled = exog_scaler.fit_transform(X_train_raw)\n",
    "   # Transform the test exogenous variables (X_test_raw)\n",
    "   X_test_scaled = exog_scaler.transform(X_test_raw)\n",
    "\n",
    "   # Concatenate train and test scaled data\n",
    "   X_full_scaled = np.vstack((X_train_scaled, X_test_scaled))\n",
    "   y_full_scaled = np.vstack((y_train_scaled, y_test_scaled))\n",
    "\n",
    "   # Generate the dataset\n",
    "   X_full, exog_X_full, y_full = create_dataset_with_exog(y_full_scaled, X_full_scaled, time_step)\n",
    "\n",
    "   # Determine split index (total samples in the train set after sequence creation)\n",
    "   split_index = len(X_train_scaled) - time_step\n",
    "\n",
    "   # Split the dataset back into train and test sets\n",
    "   X_train, X_test = X_full[:split_index], X_full[split_index:]\n",
    "   exog_X_train, exog_X_test = exog_X_full[:split_index], exog_X_full[split_index:]\n",
    "   y_train, y_test = y_full[:split_index], y_full[split_index:]\n",
    "\n",
    "\n",
    "   # Reshape LSTM input to be [samples, time steps, features]\n",
    "   X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "   X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "   \n",
    "   X_train_combined = np.concatenate([X_train, exog_X_train], axis=2)\n",
    "   X_test_combined = np.concatenate([X_test, exog_X_test], axis=2)\n",
    "   \n",
    "   index = df.index\n",
    "\n",
    "\n",
    "   # Step 3: Build and Train the LSTM Model\n",
    "   # Ensure the model runs on GPU\n",
    "   lstm_model = Sequential()\n",
    "   # Add the Input layer\n",
    "   lstm_model.add(Input(shape=(time_step, 1 + len(X_train.columns))))\n",
    "   # Add LSTM layer\n",
    "   lstm_model.add(LSTM(96, return_sequences=True))\n",
    "   lstm_model.add(LSTM(96, return_sequences=False))\n",
    "   # Add Dense layer\n",
    "   lstm_model.add(Dense(18, activation=\"relu\"))\n",
    "   lstm_model.add(Dense(1, activation=\"linear\"))\n",
    "\n",
    "   # Compile the LSTM model\n",
    "   lstm_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "   # Train the LSTM model\n",
    "   lstm_model.fit(X_train_combined, y_train, epochs=3, batch_size=96, verbose=2)\n",
    "\n",
    "   # Step 4: Make Predictions with LSTM and Calculate Residuals\n",
    "   # Predict on training and test sets\n",
    "   y_train_pred = lstm_model.predict(X_train_combined)   \n",
    "\n",
    "   # Make future predictions\n",
    "   n_future_steps = len(X_test_scaled) + 1  # Predict for all test steps\n",
    "   future_preds = make_future_predictions_multivariate(\n",
    "      model=lstm_model,\n",
    "      data=y_train_scaled,\n",
    "      exog_future=X_test_scaled,\n",
    "      n_future_steps=n_future_steps,\n",
    "      time_step=time_step\n",
    "   )\n",
    "\n",
    "   # Inverse transform the predictions to original scale\n",
    "   final_train_preds = inverse_transform_preds(y_train_pred, target_scaler)\n",
    "   final_test_preds = inverse_transform_preds(future_preds, target_scaler)\n",
    "   y_train_original = target_scaler.inverse_transform(y_train.reshape(-1, 1)).flatten()\n",
    "   \n",
    "   # Calculate residuals\n",
    "   train_residuals = y_train_original[1:] - final_train_preds\n",
    "\n",
    "   # Step 5: Evaluate the Model\n",
    "   # Calculate RMSE\n",
    "   rmse = np.sqrt(mean_squared_error(y_train_original[1:], final_train_preds))\n",
    "   print(f\"Test RMSE: {rmse:.2f}\")\n",
    "\n",
    "   # Plot the results\n",
    "   plt.figure(figsize=(20, 10))\n",
    "   plt.plot(index[:-49][time_step+17470:], y_train_original[17469:-1], label=f'Actual {target}')\n",
    "   plt.plot(index[:-49][time_step+17470:], final_train_preds[17469:], label='LSTM', linestyle='dashed')\n",
    "   plt.title(f'{target} Forecasting (LSTM)')\n",
    "   plt.xlabel('Date')\n",
    "   plt.ylabel(target)\n",
    "   plt.legend()\n",
    "   plt.show()\n",
    "   \n",
    "   return final_train_preds, final_test_preds, train_residuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Intraday_Price\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 52.2 GiB for an array with shape (118320, 118320) and data type int32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_with_preds, Day_Ahead_Price_lag_48, not_predicted_columns \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_engineering\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerged_df\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Fill in t+1 to t+48 and add lagged values for the cols we can't predict\u001b[39;00m\n\u001b[0;32m      2\u001b[0m df_with_preds \u001b[38;5;241m=\u001b[39m reduce_mem_usage(df_with_preds, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# reduce memory usage so there is more memory for computing\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[75], line 158\u001b[0m, in \u001b[0;36mfeature_engineering\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# Sum all columns except 'GMT Time', ignoring NaNs\u001b[39;00m\n\u001b[0;32m    156\u001b[0m df\u001b[38;5;241m.\u001b[39mloc[:,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal_Generation\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, skipna\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 158\u001b[0m df_with_preds, not_predicted_columns \u001b[38;5;241m=\u001b[39m \u001b[43mextend_df_MFLES_univariate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m49\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# takes in the df and appends new rows, fills with LSTM for select columns, 49 becaus of how we set up LSTM\u001b[39;00m\n\u001b[0;32m    160\u001b[0m df_with_preds\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# make sure the placeholder had been deleted\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;66;03m# Variable that tracks the difference between total load and demand\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[75], line 117\u001b[0m, in \u001b[0;36mextend_df_MFLES_univariate\u001b[1;34m(df, num_rows)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m to_predict:\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWorking on\u001b[39m\u001b[38;5;124m\"\u001b[39m, col)\n\u001b[1;32m--> 117\u001b[0m     df_with_preds\u001b[38;5;241m.\u001b[39mloc[df_with_preds\u001b[38;5;241m.\u001b[39mindex[\u001b[38;5;241m-\u001b[39mnum_rows:], col] \u001b[38;5;241m=\u001b[39m \u001b[43munivariate_MFLES\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_rows\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m-\u001b[39mnum_rows:]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# df_with_preds[to_derive[0]] = df_with_preds[\"BSAD_Turn_Up\"] + df_with_preds[\"BSAD_Turn_Down\"]  # calculate the BSAD toal wiht the two columns that make it up\u001b[39;00m\n\u001b[0;32m    121\u001b[0m not_predicted_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(df_with_preds\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(to_predict)  \u001b[38;5;66;03m# Columns for which predictions are basically just their respective mean\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[75], line 29\u001b[0m, in \u001b[0;36munivariate_MFLES\u001b[1;34m(series, num_rows)\u001b[0m\n\u001b[0;32m     14\u001b[0m optimal_params \u001b[38;5;241m=\u001b[39m mfles_model\u001b[38;5;241m.\u001b[39moptimize(\n\u001b[0;32m     15\u001b[0m     y\u001b[38;5;241m=\u001b[39mseries\u001b[38;5;241m.\u001b[39mvalues,  \u001b[38;5;66;03m# Time series as a NumPy array\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m48\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m,  \u001b[38;5;66;03m# Test set size\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m     },\n\u001b[0;32m     26\u001b[0m )\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Fit the model using optimal parameters\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m fitted_values \u001b[38;5;241m=\u001b[39m \u001b[43mmfles_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseasonal_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimal_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseasonal_period\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseasonal_periods\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Adjust this if needed\u001b[39;49;00m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimal_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43msmoother\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimal_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msmoother\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseasonality_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimal_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseasonality_weights\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchangepoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimal_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchangepoints\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Predict out-of-sample (next 48 steps)\u001b[39;00m\n\u001b[0;32m     41\u001b[0m forecast_horizon \u001b[38;5;241m=\u001b[39m num_rows\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\statsforecast\\mfles.py:538\u001b[0m, in \u001b[0;36mMFLES.fit\u001b[1;34m(self, y, seasonal_period, X, fourier_order, ma, alpha, decay, n_changepoints, seasonal_lr, rs_lr, exogenous_lr, exogenous_estimator, exogenous_params, linear_lr, cov_threshold, moving_medians, max_rounds, min_alpha, max_alpha, round_penalty, trend_penalty, multiplicative, changepoints, smoother, seasonality_weights)\u001b[0m\n\u001b[0;32m    536\u001b[0m seasonal_period_cycle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(seasons_cycle)\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seasonality_weights:\n\u001b[1;32m--> 538\u001b[0m     seas \u001b[38;5;241m=\u001b[39m \u001b[43mwls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfourier_series\u001b[49m\u001b[43m[\u001b[49m\u001b[43mseasonal_period_cycle\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcycle_weights\u001b[49m\u001b[43m[\u001b[49m\u001b[43mseasonal_period_cycle\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    544\u001b[0m     seas \u001b[38;5;241m=\u001b[39m ols(fourier_series[seasonal_period_cycle], resids)\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\statsforecast\\mfles.py:327\u001b[0m, in \u001b[0;36mwls\u001b[1;34m(X, y, weights)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwls\u001b[39m(X, y, weights):\n\u001b[1;32m--> 327\u001b[0m     weighted_X_T \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m     coefs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mpinv(weighted_X_T\u001b[38;5;241m.\u001b[39mdot(X))\u001b[38;5;241m.\u001b[39mdot(weighted_X_T\u001b[38;5;241m.\u001b[39mdot(y))\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X \u001b[38;5;241m@\u001b[39m coefs\n",
      "File \u001b[1;32mc:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\numpy\\lib\\twodim_base.py:293\u001b[0m, in \u001b[0;36mdiag\u001b[1;34m(v, k)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(s) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    292\u001b[0m     n \u001b[38;5;241m=\u001b[39m s[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mabs\u001b[39m(k)\n\u001b[1;32m--> 293\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    295\u001b[0m         i \u001b[38;5;241m=\u001b[39m k\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 52.2 GiB for an array with shape (118320, 118320) and data type int32"
     ]
    }
   ],
   "source": [
    "df_with_preds, Day_Ahead_Price_lag_48, not_predicted_columns = feature_engineering(merged_df_2024)  # Fill in t+1 to t+48 and add lagged values for the cols we can't predict\n",
    "df_with_preds = reduce_mem_usage(df_with_preds, verbose=True)  # reduce memory usage so there is more memory for computing\n",
    "temp_df = df_with_preds.copy()\n",
    "Day_Ahead_Price_lag_48.index = pd.to_datetime(Day_Ahead_Price_lag_48.index )\n",
    "dap48 = df_with_preds.loc[\"2024-09-30\"][\"Day_Ahead_Price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv1D,\n",
    "    MaxPooling1D,\n",
    "    LSTM,\n",
    "    Dense,\n",
    "    Input,\n",
    "    Dropout,\n",
    "    BatchNormalization,\n",
    "    MultiHeadAttention,\n",
    "    GlobalAveragePooling1D,\n",
    "    LayerNormalization,\n",
    "    Layer,\n",
    "    Bidirectional\n",
    ")\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "@register_keras_serializable()\n",
    "class TransformerEncoder(Layer):\n",
    "    def __init__(self, num_heads, key_dim, dense_dim, dropout_rate=0.1, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.input_proj = Dense(num_heads * key_dim)  # Project inputs to match MHA dimension\n",
    "        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
    "        self.dense_proj = tf.keras.Sequential([\n",
    "            Dense(dense_dim, activation=\"relu\"),\n",
    "            Dense(num_heads * key_dim)  # matches MHA output dimension\n",
    "        ])\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        self.layernorm1 = LayerNormalization()\n",
    "        self.layernorm2 = LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # Project inputs to (None, seq_len, num_heads*key_dim)\n",
    "        inputs_proj = self.input_proj(inputs)\n",
    "        # MultiHeadAttention output: (None, seq_len, num_heads*key_dim)\n",
    "        attn_output = self.attention(inputs_proj, inputs_proj)\n",
    "        # Residual connection: both inputs_proj and attn_output are (None, seq_len, num_heads*key_dim)\n",
    "        attn_output = self.layernorm1(inputs_proj + attn_output)\n",
    "        proj_output = self.dense_proj(attn_output)\n",
    "        proj_output = self.dropout(proj_output, training=training)\n",
    "        # Another residual: attn_output and proj_output now have the same shape\n",
    "        return self.layernorm2(attn_output + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(TransformerEncoder, self).get_config()\n",
    "        config.update({\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"key_dim\": self.key_dim,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "            \"dropout_rate\": self.dropout_rate,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "\n",
    "@register_keras_serializable()\n",
    "class Attention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(\n",
    "            name=\"W\",\n",
    "            shape=(input_shape[-1], input_shape[-1]),\n",
    "            initializer=tf.keras.initializers.GlorotUniform(),\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            name=\"b\",\n",
    "            shape=(input_shape[-1],),\n",
    "            initializer=tf.keras.initializers.Zeros(),\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.u = self.add_weight(\n",
    "            name=\"u\",\n",
    "            shape=(input_shape[-1],),\n",
    "            initializer=tf.keras.initializers.GlorotUniform(),\n",
    "            trainable=True,\n",
    "        )\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Compute the score\n",
    "        score = tf.tanh(tf.matmul(inputs, self.W) + self.b)\n",
    "        # Compute attention weights\n",
    "        attention_weights = tf.nn.softmax(tf.tensordot(score, self.u, axes=1), axis=1)\n",
    "        # Compute the context vector\n",
    "        context_vector = tf.reduce_sum(attention_weights[:, :, tf.newaxis] * inputs, axis=1)\n",
    "        return context_vector\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # Output shape is (batch_size, input_dim)\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super(Attention, self).get_config()\n",
    "        return base_config\n",
    "\n",
    "\n",
    "@register_keras_serializable()\n",
    "class PositionalEncoding(Layer):\n",
    "    def __init__(self, sequence_length, model_dim, **kwargs):\n",
    "        super(PositionalEncoding, self).__init__(**kwargs)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.model_dim = model_dim if model_dim % 2 == 0 else model_dim + 1  # Ensure even dimension\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        position = np.arange(self.sequence_length)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, self.model_dim, 2) * -(np.log(10000.0) / self.model_dim))\n",
    "        pos_encoding = np.zeros((self.sequence_length, self.model_dim))\n",
    "        pos_encoding[:, 0::2] = np.sin(position * div_term)\n",
    "        pos_encoding[:, 1::2] = np.cos(position * div_term)\n",
    "\n",
    "        self.pos_encoding = tf.constant(pos_encoding, dtype=tf.float32)\n",
    "        super(PositionalEncoding, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Inputs: (batch_size, sequence_length, model_dim)\n",
    "        # Add positional encoding to input\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        feature_dim = tf.shape(inputs)[2]\n",
    "        # If feature_dim < self.model_dim, slice pos_encoding\n",
    "        pos_enc = self.pos_encoding[:, :feature_dim]\n",
    "        pos_enc = tf.expand_dims(pos_enc, 0)  # (1, sequence_length, feature_dim)\n",
    "        pos_enc = pos_enc[:, :seq_len, :]  # in case input seq length < self.sequence_length\n",
    "        return inputs + pos_enc\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(PositionalEncoding, self).get_config()\n",
    "        config.update({\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"model_dim\": self.model_dim,\n",
    "        })\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# User-defined parameters\n",
    "target = \"System_Price\"\n",
    "time_step = 48\n",
    "val_size = 48 * 30\n",
    "test_size = 49\n",
    "cols_to_drop = [\n",
    "    'Hydro_Run-of-River_and_Poundage_missing',\n",
    "    'Total_Load_missing',\n",
    "    'Fossil_Oil',\n",
    "    'Day_Ahead_Price',\n",
    "    'BM_Offer_Acceptances',\n",
    "    'Wind_Offshore_missing',\n",
    "    'Fossil_Oil_missing',\n",
    "    'Day_Ahead_Price_missing',\n",
    "    'Fossil_Hard_Coal_missing',\n",
    "    'System_Price_missing',\n",
    "    'Fossil_Gas_missing',\n",
    "    'NIV_Outturn_missing',\n",
    "    'BM_Bid_Acceptances_missing',\n",
    "    'Solar_missing',\n",
    "    'Loss_of_Load_Prob',\n",
    "    'Intraday_Price_missing',\n",
    "    'Hydro_Pumped_Storage_missing',\n",
    "    'BSAD_Turn_Down_missing',\n",
    "    'BM_Offer_Acceptances_missing',\n",
    "    'BSAD_Turn_Up_missing',\n",
    "    'Loss_of_Load_Prob_missing',\n",
    "    'BSAD_Total_missing',\n",
    "    'EPEX_Intraday_Volume_missing',\n",
    "    'BM_Bid_Acceptances',\n",
    "    'Wind_Onshore_missing',\n",
    "    'value',\n",
    "    'Demand_Outturn_missing',\n",
    "    'Biomass_missing',\n",
    "    'Nuclear_missing',\n",
    "]\n",
    "\n",
    "not_predicted_columns = ['Hydro_Pumped_Storage',\n",
    " 'Wind_Offshore',\n",
    " 'BSAD_Turn_Up',\n",
    " 'BSAD_Total',\n",
    " 'Biomass',\n",
    " 'Fossil_Gas',\n",
    " 'Wind_Onshore',\n",
    " 'BM_Offer_Acceptances',\n",
    " 'Loss_of_Load_Prob',\n",
    " 'Nuclear',\n",
    " 'Solar',\n",
    " 'Day_Ahead_Price',\n",
    " 'Fossil_Hard_Coal',\n",
    " 'Hydro_Run-of-River_and_Poundage',\n",
    " 'value',\n",
    " 'BM_Bid_Acceptances',\n",
    " 'Fossil_Oil',\n",
    " 'BSAD_Turn_Down']\n",
    "\n",
    "cols_to_drop = cols_to_drop + not_predicted_columns\n",
    "\n",
    "# Load DataFrame\n",
    "df = temp_df # pd.read_csv(\"trying333.csv\")\n",
    "df.drop(columns=[\"Unnamed: 0\", \"\\tUnnamed: 0\"], errors=\"ignore\", inplace=True)\n",
    "# df.set_index(\"Datetime\", inplace=True)\n",
    "df.index = pd.to_datetime(df.index)\n",
    "# df.loc[\"2024-10-01\", \"System_Price\"] = df.loc[\"2024-09-30\"][\"Day_Ahead_Price\"].values  # placeholder set with DAP lag 48\n",
    "\n",
    "# Drop unwanted columns\n",
    "if cols_to_drop:\n",
    "    cols_to_drop = [col for col in cols_to_drop if col not in [\"NIV_Outturn\", \"System_Price\"]]\n",
    "    df = df.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "\n",
    "df = df.iloc[2:]  # Drop first 2 rows\n",
    "df = df.drop(columns=[\"hr_17664.96h_cos\", \"hr_17664.96h_sin\", \"open_price\", \"close_price\", \"Wind_Solar\", \"Wind_Solar_diff\"] + [col for col in df.columns if col.endswith(\"diff\")])\n",
    "# Define the targets\n",
    "targets = [\"System_Price\", \"NIV_Outturn\"]\n",
    "\n",
    "# Split data indices\n",
    "split_index_val = len(df) - test_size - val_size\n",
    "split_index_test = len(df) - test_size\n",
    "\n",
    "if split_index_val <= time_step or split_index_val <= 0:\n",
    "    raise ValueError(\"Not enough data for training and validation sets with the given time_step and val_size.\")\n",
    "\n",
    "# Split the data\n",
    "train_data = df.iloc[:split_index_val]\n",
    "val_data = df.iloc[split_index_val:split_index_test]\n",
    "test_data = df.iloc[split_index_test:]\n",
    "\n",
    "# Separate targets and features\n",
    "y_train_raw = train_data[targets].values\n",
    "y_val_raw = val_data[targets].values\n",
    "\n",
    "X_train_raw = train_data.drop(columns=targets).values\n",
    "X_val_raw = val_data.drop(columns=targets).values\n",
    "X_test_raw = test_data.drop(columns=targets).values\n",
    "\n",
    "# Define separate scalers for targets and features\n",
    "scaler_y = StandardScaler()\n",
    "scaler_X = StandardScaler()\n",
    "\n",
    "y_train_scaled = scaler_y.fit_transform(y_train_raw)\n",
    "y_val_scaled = scaler_y.transform(y_val_raw)\n",
    "\n",
    "X_train_scaled = scaler_X.fit_transform(X_train_raw)\n",
    "X_val_scaled = scaler_X.transform(X_val_raw)\n",
    "X_test_scaled = scaler_X.transform(X_test_raw)\n",
    "\n",
    "# Update dataset creation function to handle multivariate targets\n",
    "def create_dataset_with_exog(Y, X_exog, time_step):\n",
    "    X, exog_X, Y_multi = [], [], []\n",
    "    for i in range(len(Y) - time_step):\n",
    "        X.append(Y[i:(i + time_step), :])  # Multivariate input sequence for targets\n",
    "        exog_X.append(X_exog[i:(i + time_step), :])  # Exogenous features\n",
    "        Y_multi.append(Y[i + time_step, :])  # Multivariate target (next step)\n",
    "    return np.array(X), np.array(exog_X), np.array(Y_multi)\n",
    "\n",
    "\n",
    "# Create single-step dataset\n",
    "X_train, exog_X_train, y_train = create_dataset_with_exog(y_train_scaled, X_train_scaled, time_step)\n",
    "X_val, exog_X_val, y_val = create_dataset_with_exog(y_val_scaled, X_val_scaled, time_step)\n",
    "\n",
    "# Combine target and exogenous features\n",
    "X_train_combined = np.concatenate([X_train, exog_X_train], axis=2)  # Combine target sequence and exogenous features\n",
    "X_val_combined = np.concatenate([X_val, exog_X_val], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.83870, saving model to best_model.keras\n",
      "168/168 - 45s - 265ms/step - loss: 1.0399 - val_loss: 0.8387 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\n",
      "Epoch 2: val_loss improved from 0.83870 to 0.72778, saving model to best_model.keras\n",
      "168/168 - 15s - 90ms/step - loss: 0.8632 - val_loss: 0.7278 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.72778\n",
      "168/168 - 15s - 87ms/step - loss: 0.7835 - val_loss: 0.7524 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\n",
      "Epoch 4: val_loss improved from 0.72778 to 0.70200, saving model to best_model.keras\n",
      "168/168 - 15s - 89ms/step - loss: 0.7470 - val_loss: 0.7020 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\n",
      "Epoch 5: val_loss improved from 0.70200 to 0.69913, saving model to best_model.keras\n",
      "168/168 - 15s - 91ms/step - loss: 0.7066 - val_loss: 0.6991 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\n",
      "Epoch 6: val_loss improved from 0.69913 to 0.65300, saving model to best_model.keras\n",
      "168/168 - 15s - 91ms/step - loss: 0.6782 - val_loss: 0.6530 - learning_rate: 0.0010\n",
      "Epoch 7/20\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.65300\n",
      "168/168 - 15s - 89ms/step - loss: 0.6549 - val_loss: 0.6555 - learning_rate: 0.0010\n",
      "Epoch 8/20\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.65300\n",
      "168/168 - 15s - 91ms/step - loss: 0.6406 - val_loss: 0.7011 - learning_rate: 0.0010\n",
      "Epoch 9/20\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.65300\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "168/168 - 16s - 93ms/step - loss: 0.6277 - val_loss: 0.7371 - learning_rate: 0.0010\n",
      "Epoch 10/20\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.65300\n",
      "168/168 - 15s - 92ms/step - loss: 0.5598 - val_loss: 0.7668 - learning_rate: 5.0000e-04\n",
      "Epoch 11/20\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.65300\n",
      "168/168 - 17s - 104ms/step - loss: 0.5286 - val_loss: 0.7421 - learning_rate: 5.0000e-04\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ruben\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\layer.py:391: UserWarning: `build()` was called on layer 'transformer_encoder_3', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Future predictions: [[  70.030815    71.44727  ]\n",
      " [  66.09799    143.57872  ]\n",
      " [  69.29823     56.79961  ]\n",
      " [  60.910717   137.15723  ]\n",
      " [  57.599117   207.4843   ]\n",
      " [  58.328514   168.89569  ]\n",
      " [  51.931633   246.61555  ]\n",
      " [  55.972588   193.10384  ]\n",
      " [  63.90398    133.88945  ]\n",
      " [  65.00756    127.71368  ]\n",
      " [  65.97365    141.7727   ]\n",
      " [  66.11592     85.726135 ]\n",
      " [  69.90997     94.63802  ]\n",
      " [  74.66951     57.8639   ]\n",
      " [  80.876274   -29.288733 ]\n",
      " [  79.55052     14.488961 ]\n",
      " [  78.20144     31.634426 ]\n",
      " [  76.413506    58.811577 ]\n",
      " [  71.917244   103.4092   ]\n",
      " [  70.07827    106.55579  ]\n",
      " [  68.94353    118.28267  ]\n",
      " [  67.07964    134.35764  ]\n",
      " [  66.2452     164.91531  ]\n",
      " [  66.3921     152.8478   ]\n",
      " [  68.36144    119.70413  ]\n",
      " [  68.43518    119.46371  ]\n",
      " [  70.254845    95.75823  ]\n",
      " [  70.754486    88.88231  ]\n",
      " [  72.65156     67.77291  ]\n",
      " [  74.10291     41.341526 ]\n",
      " [  76.94395      5.6208124]\n",
      " [  78.12613    -11.221683 ]\n",
      " [  80.146774   -24.71676  ]\n",
      " [  84.44823    -73.456085 ]\n",
      " [  88.55862   -128.19131  ]\n",
      " [  91.92383   -160.36957  ]\n",
      " [  93.67971   -171.08423  ]\n",
      " [  93.77153   -164.46022  ]\n",
      " [  93.74386   -165.92473  ]\n",
      " [  93.38007   -154.33585  ]\n",
      " [  87.91597    -93.17874  ]\n",
      " [  83.90565    -50.26103  ]\n",
      " [  81.060135   -27.319368 ]\n",
      " [  75.72669     31.990005 ]\n",
      " [  70.537796    91.38341  ]\n",
      " [  64.97433    164.12418  ]\n",
      " [  63.769196   162.04993  ]\n",
      " [  62.070526   147.1985   ]]\n"
     ]
    }
   ],
   "source": [
    "# Build the model (one-step output)\n",
    "input_layer = Input(shape=(time_step, X_train_combined.shape[2]))\n",
    "input_with_positional = PositionalEncoding(sequence_length=time_step, model_dim=X_train_combined.shape[2])(input_layer)\n",
    "\n",
    "cnn_layer = Conv1D(filters=64, kernel_size=3, activation='relu')(input_with_positional)\n",
    "cnn_layer = MaxPooling1D(pool_size=4)(cnn_layer)\n",
    "cnn_layer = BatchNormalization()(cnn_layer)\n",
    "\n",
    "lstm_layer = Bidirectional(LSTM(64, return_sequences=True, activation='tanh', recurrent_activation='sigmoid',\n",
    "                                dropout=0.1, kernel_regularizer=l2(0.001)))(cnn_layer)\n",
    "lstm_layer = LayerNormalization()(lstm_layer)\n",
    "\n",
    "transformer_out = TransformerEncoder(num_heads=4, key_dim=128, dense_dim=256)(lstm_layer)\n",
    "attention_out = MultiHeadAttention(num_heads=4, key_dim=64)(transformer_out, transformer_out)\n",
    "pooled_output = GlobalAveragePooling1D()(attention_out)\n",
    "\n",
    "dense_out = Dense(64, activation='relu', kernel_regularizer=l2(0.001))(pooled_output)\n",
    "dense_out = Dropout(0.1)(dense_out)\n",
    "\n",
    "final_output = Dense(2, activation='linear')(dense_out)\n",
    "\n",
    "lstm_model = Model(inputs=input_layer, outputs=final_output)\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "lstm_model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='best_model.keras',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
    "\n",
    "# Train the model on single-step prediction\n",
    "lstm_model.fit(\n",
    "    X_train_combined, y_train,\n",
    "    validation_data=(X_val_combined, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=96,\n",
    "    verbose=2,\n",
    "    callbacks=[checkpoint_callback, reduce_lr, early_stopping]\n",
    ")\n",
    "\n",
    "# Load the best model\n",
    "lstm_model_best_val = load_model('best_model.keras', custom_objects={'Attention': Attention,\n",
    "        \"TransformerEncoder\": TransformerEncoder,\n",
    "        \"PositionalEncoding\": PositionalEncoding,\n",
    "    })\n",
    "\n",
    "###############################################################################\n",
    "# Iterative Forecasting Code (Real Implementation)\n",
    "###############################################################################\n",
    "# Let's assume we want to predict the next 48 steps into the future.\n",
    "n_future_steps = 48\n",
    "\n",
    "# Start from the last known sequence in the validation set as our base.\n",
    "# This represents the last window we have real data for.\n",
    "last_known_sequence = X_val_combined[-1:].copy()  # Shape: (1, time_step, features)\n",
    "\n",
    "predictions = []\n",
    "for i in range(n_future_steps):\n",
    "    # Predict the next step (scaled)\n",
    "    next_pred_scaled = lstm_model_best_val.predict(last_known_sequence, verbose=0)  # Shape: (1,1)\n",
    "\n",
    "    # Inverse transform to original scale\n",
    "    next_pred = scaler_y.inverse_transform(next_pred_scaled.reshape(-1,2))\n",
    "    predictions.append(next_pred[0])\n",
    "\n",
    "    # Get the exogenous features for the next step from test (or future) data\n",
    "    # Make sure you have at least `n_future_steps` rows in X_test_exog_scaled\n",
    "    exog_next_step = X_test_scaled[i].reshape(1,1,-1) \n",
    "\n",
    "    # Re-scale the predicted value to append back into the model's input domain\n",
    "    next_pred_scaled_value = scaler_y.transform(next_pred)  # Shape: (1, 2)\n",
    "\n",
    "    # Construct the next input step (predicted target + future exogenous)\n",
    "    next_input = np.concatenate([next_pred_scaled_value.reshape(1,1,-1), exog_next_step], axis=2)\n",
    "\n",
    "    # Update the sequence: remove the oldest time step and add the new predicted time step\n",
    "    last_known_sequence = np.append(last_known_sequence[:,1:,:], next_input, axis=1)\n",
    "\n",
    "# 'predictions' now contains the iteratively predicted values for the next 48 steps.\n",
    "###############################################################################\n",
    "\n",
    "# Evaluate predictions as needed (e.g., plotting).\n",
    "print(\"Future predictions:\", np.array(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.array(predictions)\n",
    "predictions_sep = pd.DataFrame({\"System_Price\": temp[:, 0], \"NIV_Outturn\": temp[:, 1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error System Price (RMSE): 15.080361514332411\n",
      "Root Mean Squared Error NIV Outturn (RMSE): 414.514687344532\n"
     ]
    }
   ],
   "source": [
    "SP_preds =  np.array(predictions_sep[\"System_Price\"]) * 0.74 + np.array(dap48)*0.46\n",
    "\n",
    "NIV_preds = -220 - 3.5*np.array(predictions_sep[\"System_Price\"]).reshape(-1,1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
